[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "We’ll use data from several real world situations in class."
  },
  {
    "objectID": "data.html#data-1",
    "href": "data.html#data-1",
    "title": "Data",
    "section": "Data 1",
    "text": "Data 1"
  },
  {
    "objectID": "data.html#data-2",
    "href": "data.html#data-2",
    "title": "Data",
    "section": "Data 2",
    "text": "Data 2"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Who can / should take this course?\nThis is a fast-paced and demanding course designed to prepare PhD students for research careers in statistics. Undergraduates and PhD students in other fields are very welcome to take the class, and many have succeeded in the past. But you should be prepared to work hard.\nCan I take this course if I have a conflict with the lecture time?\nAttendance is not taken at lectures but attending is an important part of taking the course. If you want to take another course at the same time, you can do it as long as the other professor is OK with your not attending their lectures.\nHow can I prepare for the course?\nThe course prerequisites are undergraduate-level linear algebra, real analysis, and a year of upper-division probability and statistics. If you are unsure of your background in one or more of these, the texts below can be useful review materials. All books linked below, except Gelman & Hill, should be available for free with a Berkeley library subscription. Email me if you have trouble accessing them.\n\nLinear algebra: Fluency with undergrad-level abstract linear algebra is essential to understanding the course content (numerical tools like LU or Cholesky decompositions are not essential). Chapters 1-3 and 5-6 of Linear Algebra Done Right (Axler) are good review materials.\nReal analysis: Familiarity with and intuition for ideas of infinite sequences, convergence, Taylor approximations, etc. should be enough. Chapters 1-3 of Understanding Analysis (Abbott) are good review materials.\nProbability: The whole course involves probability. If Chapters 1-6, 8-9, 13-17, and 23 of Probability for Data Science (Adhikari and Pitman) aren’t mostly review for you, I strongly recommend studying them before the course begins.\nStatistics: We will derive all statistical results from first principles, but at a fairly technical level. If you have never seen them in an applied context, Chapters 1-6 of Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman & Hill) should give you some frame of reference."
  },
  {
    "objectID": "units/unit3.html",
    "href": "units/unit3.html",
    "title": "Unit 3: More",
    "section": "",
    "text": "This is an example of using qmd as the source document with pdf as one target. I’ve taken out the qmd stuff that doesn’t seem to render to pdf."
  },
  {
    "objectID": "units/unit3.html#evaluated-python-code-chunk-with-a-plot",
    "href": "units/unit3.html#evaluated-python-code-chunk-with-a-plot",
    "title": "Unit 3: More",
    "section": "Evaluated Python code chunk, with a plot",
    "text": "Evaluated Python code chunk, with a plot\n\n\nCode\nimport numpy as np\nx = np.random.normal(size=100)\nimport matplotlib.pyplot as plt\nplt.hist(x)\nplt.show()\nnp.mean(x)\n\n\n\n\n\n0.02842647826381281"
  },
  {
    "objectID": "units/unit3.html#latex",
    "href": "units/unit3.html#latex",
    "title": "Unit 3: More",
    "section": "LaTeX",
    "text": "LaTeX\n\\[\n\\theta = \\int_0^\\infty f(x,\\theta)d\\theta\n\\]"
  },
  {
    "objectID": "units/unit3.html#latex-macro",
    "href": "units/unit3.html#latex-macro",
    "title": "Unit 3: More",
    "section": "LaTeX macro",
    "text": "LaTeX macro\n\nWarning: need to look back at this as having include-before-body in the yaml causes extra space at top of page.\n\n\\[\nA = X \\trans Y\n\\]"
  },
  {
    "objectID": "units/macros.html",
    "href": "units/macros.html",
    "title": "",
    "section": "",
    "text": "\\[\n\\newcommand{\\trans}{^\\mathsf{T}}\n\\newcommand{\\eps}{\\epsilon}\n\\]"
  },
  {
    "objectID": "handwritten-notes.html",
    "href": "handwritten-notes.html",
    "title": "Handwritten notes",
    "section": "",
    "text": "Lecture 1\nLecture 2"
  },
  {
    "objectID": "handwritten-notes.html#handwritten-lecture-notes",
    "href": "handwritten-notes.html#handwritten-lecture-notes",
    "title": "Handwritten notes",
    "section": "",
    "text": "Lecture 1\nLecture 2"
  },
  {
    "objectID": "reader/measure-theory-basics.html",
    "href": "reader/measure-theory-basics.html",
    "title": "Measure Theory Basics",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/measure-theory-basics.html#measure-theory-a-rigorous-grounding-for-probability",
    "href": "reader/measure-theory-basics.html#measure-theory-a-rigorous-grounding-for-probability",
    "title": "Measure Theory Basics",
    "section": "Measure theory: a rigorous grounding for probability",
    "text": "Measure theory: a rigorous grounding for probability\nMeasure theory is an area of mathematics concerned with measuring the “size” of subsets of a certain set. Soon after it was developed in the the early twentieth century, the great Soviet mathematician Kolmogorov realized it could be applied to give a rigorous grounding to probability theory, it was a major advance in understanding and resolving certain paradoxes in probability theory. David Aldous gives a nice discussion of this history.\nThis is not a course on measure-theoretic probability and we will not rigorously develop the subject. However, it will be useful to draw on some of the basics of measure theory, to simplify our notation throughout the course and to clarify certain concepts around integration and conditioning. Homework 0 illustrates some of the interesting aspects of measure theory and why it is useful."
  },
  {
    "objectID": "reader/measure-theory-basics.html#measures",
    "href": "reader/measure-theory-basics.html#measures",
    "title": "Measure Theory Basics",
    "section": "Measures",
    "text": "Measures\nGiven a set \\(\\cX\\), a measure\\(\\mu\\) is a certain kind of function mapping “nice enough” subsets \\(A \\subseteq \\cX\\) to non-negative numbers \\(\\mu(A) \\in [0,\\infty]\\).\nExample 1 (Counting measure): If \\(\\cX\\) is countable, e.g. \\(\\cX = \\mathbb{Z}\\), then a natural measure is the counting measure \\(\\#(A)\\), which simply counts the number of points in a subset \\(A\\). That is, \\(\\#(\\{0,1\\}) = 2\\), and \\(\\#(\\{2,4,6,8,\\ldots\\}) = \\infty\\) .\nExample 2 (Lebesgue measure): If \\(\\cX = \\RR^n\\) for some integer \\(n\\), a natural measure is the Lebesgue measure \\(\\lambda(A)\\), which returns the volume of a subset \\(A\\). Roughly speaking, we can write\n\\[\n\\lambda(A) = \\int \\cdots \\int_A \\td x_1\\td x_2\\cdots \\td x_n.\n\\]\nExample 3 (Gaussian measure): Now taking \\(\\cX = \\RR\\), we might instead want to define the “size” of a set as the probability that a standard Gaussian random variable \\(Z \\sim \\cN(0,1)\\) is observed to be in the set \\(A\\). That is, we can define the measure:\n\\[\nP_Z(A) = \\PP(Z \\in A) = \\int_A \\phi(x)\\td x, \\quad \\text{ where } \\;\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\n\\]\nis the probability density function of \\(Z\\).\nAs it turns out, it is not so obvious how to define what exactly we mean by the right-hand side of the previous two equations; in fact, it is not even possible to define the volume of every subset \\(A \\in \\mathbb{R}\\). In Homework 0, Problem 3, you will use the axiom of choice to construct pathological subsets (so-called non-measurable sets) to which we cannot sensibly assign any volume.\nOne of the original motivations for measure theory was to provide a framework for excluding these pathological sets and rigorously defining integrals over the other, nicer sets. In general, the domain of a measure is not all subsets of \\(\\cX\\) (called the power set and notated \\(2^{\\cX}\\)), but rather a collection of nice subsets \\(\\cF \\subseteq 2^{\\cX}\\).\nFormally, the collection \\(\\cF\\) must be a \\(\\sigma\\)-field, meaning that it satisfies certain closure properties. We say \\(\\cF\\) is a \\(\\sigma\\)-field (or \\(\\sigma\\)-algebra) if\n\nThe full set \\(\\cX\\) is in \\(\\cF\\).\nIf \\(A\\) is in \\(\\cF\\) then its complement \\(\\cX \\setminus A\\) is also in \\(\\cF\\) (i.e., \\(\\cF\\) is closed under complementation)\nIf \\(A_1,A_2,\\ldots \\in \\cF\\) then \\(\\bigcup_{i=1}^\\infty A_i\\) is also in \\(\\cF\\) (i.e. \\(\\cF\\) is closed under countable unions)\n\nNote: The details of this definition are not important for purposes of this course.\nExample: If \\(\\cX\\) is countable we can take \\(\\cF\\) to be the entire power set.\nExample: If \\(\\cX = \\mathbb{R}^n\\) we will typically use the Borel \\(\\sigma\\)-field \\(\\cB\\), defined as the smallest \\(\\sigma\\)-field that includes all open rectangles \\((a_1,b_1)\\times (a_2,b_2) \\times \\cdots \\times (a_n, b_n)\\), where \\(a_i &lt; b_i\\) for all \\(i\\). That is, we start with the open rectangles and recursively apply the closure properties to obtain a very large collection of sets, which informally we can think of as containing all non-pathological subsets of \\(\\mathbb{R}^n\\).\nWe are now ready to define a measure. We call a pair of a set \\(\\cX\\) and an associated \\(\\sigma\\)-field \\(\\cF \\subseteq 2^{\\cX}\\) a measurable space. Given a measurable space \\((\\cX, \\cF)\\), a measure is a function \\(\\mu: \\cF \\to \\mathbb{R}\\) satisfying three properties:\n\nNon-negativity: \\(\\mu(A) \\geq 0\\) for all \\(A\\in \\cF\\).\nEmpty set maps to zero: \\(\\mu(\\emptyset) = 0\\)\nCountable additivity: If \\(A_1,A_2,\\ldots\\in \\cF\\) are all disjoint, then\n\n\\[\n\\mu\\left(\\bigcup_{i=1}^\\infty A_i\\right) = \\sum_{i=1}^\\infty \\mu(A_i)\n\\]\nIf \\(\\mu\\) is a measure on \\((\\cX, \\cF)\\) we call \\((\\cX, \\cF, \\mu)\\) a measure space.\nIn the special case \\(\\mu(\\cX) = 1\\), we call \\(\\mu\\) a probability measure and \\((\\cX, \\cF, \\mu)\\) is called a probability space."
  },
  {
    "objectID": "reader/measure-theory-basics.html#integrals",
    "href": "reader/measure-theory-basics.html#integrals",
    "title": "Measure Theory Basics",
    "section": "Integrals",
    "text": "Integrals\nOne very nice thing about measures is that they let us define integrals of (nice enough) real-valued functions on \\(\\cX\\) with respect to the measure \\(\\mu\\), meaning the integral is “weighted” in a way that assigns total weight \\(\\mu(A)\\) to each set \\(A\\). We will use the notation \\(\\int f(x)\\,\\td\\mu(x)\\), or just \\(\\int f \\td\\mu\\).\nTo construct this integral, we begin by defining it for indicator functions, and then extend to more general functions by linearity and limits, in a few steps:\nFirst, for an indicator function \\(1_A(x) = 1\\{x \\in A\\}\\) of a set \\(A \\in \\cF\\), it is straightforward to define the integral as \\(\\int 1_A \\td\\mu = \\mu(A)\\) (note if \\(A \\notin \\cF\\) this does not work, but we are only defining the integral for a class of “nice” functions determined by our \\(\\sigma\\)-field)\nNext, consider a simple function \\(f(x) = \\sum_{i=1}^\\infty c_i 1_{A_i}(x)\\), with all \\(c_i \\geq 0\\) and \\(A_i \\in \\cF\\). Because the integral should be linear, we should have\n\\[\n\\int f\\td\\mu = \\sum_{i=1}^\\infty c_i \\int 1_{A_i}\\td\\mu = \\sum_{i=1}^\\infty c_i \\mu(A_i)\n\\]\nThird, we can extend to all sufficiently nice non-negative functions by approximating them from below with a series of simple functions:\n\\[\n\\int f\\td\\mu = \\lim_{i=1}^\\infty \\int f_i\\td\\mu.\n\\]\nThis idea is illustrated in the picture below:\n\n\n\nApproximating a non-negative function from below by a series of simple (piecewise constant) functions. The red function is an indicator, the blue function is a simple function, and the dashed orange function is a simple function that approximates the orange curve.\n\n\nFinally, we can write any real-valued function as the sum of its positive and negative parts, \\(f(x) = f^+(x) - f^-(x)\\), where \\(f^+(x) = \\max\\{f(x), 0\\}\\) and \\(f^-(x) = \\max\\{-f(x), 0\\}\\). Then both \\(f^+\\) and \\(f^-\\) have non-negative (possibly infinite) integrals. Then we simply take\n\\[\n\\int f\\td\\mu = \\int f^+\\td\\mu - \\int f^-\\td\\mu \\in [-\\infty, \\infty],\n\\]\ncalling the difference undefined if the integrals of both \\(f^+\\) and \\(f^-\\) are infinite.\nAs a result, we have \\(\\int f\\td\\mu\\) for any function \\(f\\) whose positive and negative parts can both be approximated from below by simple functions. Note that we have left out some important details in this presentation (for example we have not characterized which functions \\(f\\) are nice enough to be approximated well by simple functions) but these details are unimportant for this class. The important thing to know is that to any measure \\(\\mu\\) there corresponds a well-defined integral \\(\\int \\cdot \\td\\mu\\), which behaves as we would expect it to.\nWe can now return to our previous examples of measures and ask what the corresponding integrals are:\nExample 1, continued (Counting measure): An integral with respect to \\(\\#\\) just adds up all the values of \\(f(x)\\):\n\\[\n\\int f\\td\\# = \\sum_{x\\in \\cX} f(x)\n\\]\nExample 2, continued (Lebesgue measure): An integral with respect to the Lebesgue measure is called a Lebesgue integral, which is essentially just the usual integral you are used to from calculus class:\n\\[\n\\int f\\td\\lambda = \\int\\cdots \\int f(x) \\td x_1 \\cdots \\td x_n.\n\\]\nThe Lebesgue integral extends the Riemann integral to a more general class of functions, in the sense that if the Riemann integral of \\(f\\) is defined then the Lebesgue integral is also well defined and the two integrals coincide. But the Lebesgue integral is also well-defined for functions like \\(f(x) = 1\\{x \\in \\mathbb{Q}\\}\\), for which the Riemann integral is not well-defined (Exercise: what is the Lebesgue integral of \\(1\\{x \\in \\mathbb{Q}\\}\\)?)\nExample 3, continued (Gaussian measure): Note that \\(P_Z(A)\\) is defined as the (Lebesgue) integral of \\(1_A(x)\\phi(x)\\). By extension, the integral of \\(f\\) with respect to \\(P_Z\\) is the Lebesgue integral of \\(f(x) \\phi(x)\\), which is nothing more than the expectation of \\(f(Z)\\):\n\\[\n\\int f\\td P_Z = \\int_{-\\infty}^\\infty f(x) \\phi(x) \\td x = \\EE[f(Z)].\n\\]"
  },
  {
    "objectID": "reader/measure-theory-basics.html#densities",
    "href": "reader/measure-theory-basics.html#densities",
    "title": "Measure Theory Basics",
    "section": "Densities",
    "text": "Densities\nWe have just seen in the last two examples that there is a special relationship between the Lebesgue measure \\(\\lambda\\) on \\(\\RR\\) and the Gaussian measure \\(P_Z\\), allowing us to evaluate integrals with respect to \\(P\\) by turning them into integrals with respect to \\(\\lambda\\), namely \\(\\int f(x)\\td P_Z(x) = \\int f(x)\\phi(x) \\td\\lambda(x)\\).\nThis is a happy fact, since mathematicians have gone to a lot of trouble figuring out how to calculate integrals with respect to the usual (Lebesgue) measure. Most of the expectations we want to calculate in statistics are integrals with respect to some joint probability measure over random variables, and we certainly wouldn’t want to have to reinvent the wheel of integration every time we want to do calculations with respect to a new random variable.\nNote that we can’t turn integrals for every random variable into Lebesgue integrals. If \\(Y\\) follows a binomial distribution, for example, we can just as well define \\(P_Y(A) = \\PP(Y \\in A)\\), but there is no counterpart to \\(\\phi\\) that would let us turn \\(P_Y\\) integrals into Lebesgue integrals in the same way.\nFormally, consider a measurable space \\((\\cX, \\cF)\\), with two measures \\(P\\) and \\(\\mu\\). We say \\(P\\) is absolutely continuous with respect to \\(\\mu\\) if \\(P(A) = 0\\) whenever \\(\\mu(A) = 0\\). In notation, we write \\(P \\ll \\mu\\).\nIf \\(P \\ll \\mu\\) then, under mild conditions, we can always define a density function \\(p:\\cX \\mapsto [0,\\infty)\\) such that\n\\[\nP(A) = \\int 1_A(x) p(x)\\td\\mu(x), \\quad \\text{ for all } A \\in \\cF,\n\\]\nand by extension \\(\\int f(x)\\td P(x) = \\int f(x) p(x) \\td\\mu(x)\\).\nThe function \\(p\\) is called the density function or Radon-Nikodym derivative of \\(P\\) with respect to \\(\\mu\\). It is sometimes written using the suggestive notation \\(\\frac{\\td P}{\\td\\mu}(x)\\). Whenever we have a density function we can turn integrals with respect to \\(P\\) into integrals with respect to \\(\\mu\\) simply by multiplying the integrand by \\(p\\).\nIf we do not specify what \\(\\mu\\) is, it is assumed to be the Lebesgue measure; that is, if we say \\(P\\) is absolutely continuous with no further elaboration, we mean \\(P \\ll \\lambda\\).\nIf \\(P\\) is a probability measure, we call \\(p(x)\\) its probability density function (with respect to \\(\\mu\\)). If \\(\\mu\\) is a counting measure, we call \\(p(x)\\) its probability mass function. These are abbreviated pdf and pmf, respectively."
  },
  {
    "objectID": "reader/measure-theory-basics.html#probability-spaces-and-random-variables",
    "href": "reader/measure-theory-basics.html#probability-spaces-and-random-variables",
    "title": "Measure Theory Basics",
    "section": "Probability spaces and random variables",
    "text": "Probability spaces and random variables\nA typical statistics problem involves many, random variables of various types (e.g. some discrete and some continuous random variables), some of which may be functions of others. The overall joint distribution is defined implicitly by specifying the variables’ relationships to one another, or giving some sequence of rules for how they are all generated. We will want to ask about probabilities of events that involve functions of multiple random variables, e.g. a natural question to ask in a simple variance estimation problem with i.i.d. random variables \\(X_1,\\ldots,X_n\\) might be “what is the probability that \\(\\left|\\frac{1}{n-1}\\sum_{i=1}^n \\left(X_i - \\overline X\\right)^2 - \\sigma^2\\right| &lt; \\delta\\) ?”\nThe notation in the previous section doesn’t allow us to ask questions like this without, e.g., massaging the above event into the set of \\((X_1,\\ldots,X_n)\\) vectors for which the event would hold. This would become even more difficult in more complicated setups.\nInstead of trying to work directly with the measure corresponding to the joint distribution of all of the random variables involved, it can be convenient to instead think of the variables as all being functions of some abstract “outcome” \\(\\omega\\) that encompasses all of the randomness in the problem. We introduce an abstract probability space \\((\\Omega, \\cF, \\PP)\\) where\n\n\\(\\omega \\in \\Omega\\) is called an outcome,\n\\(A \\in \\cF\\) is called an event,\n\\(\\PP(A)\\) is called the probability of \\(A\\).\n\nThen a random variable is any (nice enough) function \\(X:\\; \\Omega \\to \\cX\\). We say \\(X\\) has distribution \\(P\\), and write \\(X \\sim P\\), if\n\\[\n\\PP(X \\in B) = \\PP(\\{\\omega:\\; X(\\omega) \\in B\\}) = P(B).\n\\]We say the real-valued random variable \\(X\\) is continuous if its distribution is absolutely continuous (with respect to the Lebesgue meaure). If \\(X\\) is a random variable, then \\(f(X)\\) is also a random variable for any (nice enough) function \\(f\\).\nLikewise, the expectation of a random variable is defined as an integral with respect to \\(\\PP\\):\n\\[\n\\EE[X] = \\int X(\\omega) \\td\\PP(\\omega), \\quad \\text{ and } \\quad \\EE[f(X,Y)] = \\int f(X(\\omega), Y(\\omega)) \\td\\PP(\\omega).\n\\]\nUsually, to do real calculations we will eventually boil \\(\\PP\\) or \\(\\EE\\) into a composition of integrals and/or sums."
  },
  {
    "objectID": "reader/measure-theory-basics.html#conditional-probability",
    "href": "reader/measure-theory-basics.html#conditional-probability",
    "title": "Measure Theory Basics",
    "section": "Conditional probability",
    "text": "Conditional probability\nWhile it is beyond the scope of this course, measure theory also allows us to patch the definition of conditional probability and conditional expectation. Given two events \\(A\\) and \\(B\\), if \\(\\PP(B) &gt; 0\\), we can unproblematically define the conditional probability of \\(A\\) given \\(B\\) as \\(\\PP(A \\mid B) = \\PP(A \\cap B) / \\PP(B)\\), but this definition obviously fails when \\(\\PP(B) = 0\\).\nGenerally speaking, we cannot necessarily define \\(\\PP(A \\mid B)\\) for measure zero events \\(B\\) (Homework 0 includes a problem illustrating the inherent ambiguity of this definition). But, for example, if \\(X\\) and \\(Y\\) are both continuous random variables with some dependence between them we would like to be able to discuss, e.g., the distribution or expectation of \\(Y\\) given that \\(X\\) takes on some specific value \\(x\\). We can do this by defining the conditional expectation \\(\\EE(Y \\mid X)\\) as a random variable \\(g(X)\\), which has the property \\(\\EE[(Y - g(X)) 1_A(X)] = 0\\) for all (nice) subsets \\(A\\). By evaluating this function \\(g\\) at \\(x\\) we can answer the question we asked earlier. However, note this explanation is informal and brushes many important points under the rug; for a more complete explanation, take Stat 205A.\nHaving defined the conditional expectation, we can also ask about the conditional distribution of \\(Y\\) by evaluating the conditional expectation on new random variables defined with indicator functions: \\(\\PP(Y \\in A \\mid X) = \\EE[1_A(Y) \\mid X]\\) ."
  },
  {
    "objectID": "reader/measure-theory-basics.html#more-definitions",
    "href": "reader/measure-theory-basics.html#more-definitions",
    "title": "Measure Theory Basics",
    "section": "More definitions",
    "text": "More definitions\nFinally, this section includes some scattered definitions to remind you of a few more useful definitions which you likely have already seen in your previous probability courses.\nIf \\(\\PP(A) = 1\\), we say \\(A\\) occurs almost surely.\nThe variance of a random variable \\(X\\) is \\(\\textrm{Var}(X) = \\EE[X^2] - \\EE [X]\\)\nSee Keener Ch. 1 for more on probability."
  },
  {
    "objectID": "reader/minimax-estimation.html",
    "href": "reader/minimax-estimation.html",
    "title": "Minimax Estimation",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/minimax-estimation.html#minimax-risk-estimator",
    "href": "reader/minimax-estimation.html#minimax-risk-estimator",
    "title": "Minimax Estimation",
    "section": "1 Minimax Risk Estimator",
    "text": "1 Minimax Risk Estimator\n\n1.1 Definition of Minimax Risk\nThe last idea for choosing an estimator: worst-case risk.\nMinimize \\(\\sup_\\theta R(\\theta, \\delta)\\)\nThe minimum achievable sup risk is called the minimax risk of the estimation problem:\n\\[r = \\inf_\\delta \\sup_\\theta R(\\theta, \\delta)\\]\nAn estimator \\(\\delta\\) is called minimax if it achieves the minimax risk, i.e.,\n\\[\\sup_\\theta R(\\theta, \\delta) = r\\]\n\n\n1.2 Game Theory Interpretation\n\nAnalyst chooses estimator \\(\\delta\\)\nNature chooses parameter \\(\\theta\\) to maximize risk\n\nNote: Nature chooses \\(\\theta\\) adversarially, not \\(X\\).\nCompare to Bayes where Nature chooses prior from a known distribution (Nature plays a specific mixed strategy).\nWe will look for Nature’s Nash equilibrium strategy."
  },
  {
    "objectID": "reader/minimax-estimation.html#least-favorable-priors",
    "href": "reader/minimax-estimation.html#least-favorable-priors",
    "title": "Minimax Estimation",
    "section": "2 Least Favorable Priors",
    "text": "2 Least Favorable Priors\nMinimax is closely related to Bayes.\nKey observation: average case risk ≤ worst case risk\nFor proper prior \\(\\pi\\), the Bayes risk is:\n\\[r(\\pi) = \\int_\\Theta R(\\theta, \\delta_\\pi) d\\pi(\\theta)\\]\n\\[\\leq \\int_\\Theta \\sup_\\theta R(\\theta, \\delta) d\\pi(\\theta) = \\sup_\\theta R(\\theta, \\delta)\\]\nIf \\(\\delta_\\pi\\) is Bayes, then \\(r(\\pi) = \\inf_\\delta \\int_\\Theta R(\\theta, \\delta) d\\pi(\\theta)\\)\nBayes risk of any Bayes estimator lower bounds \\(r\\).\nLeast favorable prior \\(\\pi\\) gives best lower bound: \\(r(\\pi) = \\sup_\\pi r(\\pi)\\)\nSup risk of any estimator upper bounds \\(r\\):\n\\[\\sup_\\theta R(\\theta, \\delta) \\geq r \\geq \\sup_\\pi r(\\pi)\\]\nCan exhibit minimax est. & LF prior by finding \\(\\pi\\) and \\(\\delta\\) that collapse these inequalities.\n\n2.1 Theorem\nIf \\(R(\\delta_\\pi) = \\sup_\\theta R(\\theta, \\delta_\\pi)\\) with Bayes estimator \\(\\delta_\\pi\\), then:\n\n\\(\\delta_\\pi\\) is minimax\nIf \\(\\delta_\\pi\\) is unique Bayes (up to \\(\\pi\\)-a.e.), it is unique minimax\n\\(\\pi\\) is least favorable\n\nProof:\n\nAny other \\(\\delta\\): \\[\\sup_\\theta R(\\theta, \\delta) \\geq \\int R(\\theta, \\delta) d\\pi(\\theta) \\geq\\] \\[\\int R(\\theta, \\delta_\\pi) d\\pi(\\theta) = r(\\pi) = \\sup_\\theta R(\\theta, \\delta_\\pi)\\] \\(r\\) is minimax risk, \\(\\delta_\\pi\\) is minimax\nReplace \\(\\geq\\) with \\(=\\) in 2nd inequality ⟹ \\(\\delta = \\delta_\\pi\\) \\(\\pi\\)-a.e.\nAny other prior \\(\\pi'\\): \\[\\inf_\\delta r(\\pi') \\leq \\int R(\\theta, \\delta_\\pi) d\\pi'(\\theta)\\] \\[\\leq \\sup_\\theta R(\\theta, \\delta_\\pi) = r(\\pi)\\]\n\nThe above theorem gives a checkable condition: does avg risk = sup risk?\nNote: If \\(R(\\theta, \\delta_\\pi)\\) is constant, it doesn’t prove anything.\n\n\\(R(\\theta, \\delta_\\pi)\\) is constant\nAlso \\(R(\\theta, \\delta_\\pi) = \\sup_\\theta R(\\theta, \\delta_\\pi) = r(\\pi)\\)\n\n\n\n2.2 Example: Binomial\n\\(X \\sim \\text{Binom}(n, \\theta)\\), estimate \\(\\theta\\) with squared error\nTry Beta(\\(\\alpha, \\beta\\)), hope to get one with constant risk\n\\[\\delta_\\pi(X) = \\frac{X + \\alpha}{n + \\alpha + \\beta}\\]\n\\[R(\\theta, \\delta_\\pi) = \\mathbb{E}[\\theta^2] - \\mathbb{E}[\\delta_\\pi(X)^2] + \\text{Var}(\\delta_\\pi(X))\\]\n\\[= \\theta - \\frac{(\\alpha + \\beta + n + 1)(\\alpha + n\\theta)^2}{(\\alpha + \\beta + n)^2(n + \\alpha + \\beta + 1)} + \\frac{(\\alpha + n\\theta)(\\beta + n(1-\\theta))}{(\\alpha + \\beta + n)^2(n + \\alpha + \\beta + 1)}\\]\nSet \\(\\alpha + \\beta = n + 2\\), \\(\\alpha + \\beta = \\frac{n}{2}\\)\n\\[\\beta = \\frac{n+2}{2}, \\alpha = \\frac{n+2}{2}\\]\nBeta(\\(\\frac{n+2}{2}, \\frac{n+2}{2}\\)) is LF, \\(\\delta_\\pi\\) is minimax\nWe got lucky.\nQuestion: Why so much prior weight on \\(\\theta \\approx \\frac{1}{2}\\)?"
  },
  {
    "objectID": "reader/minimax-estimation.html#least-favorable-sequence",
    "href": "reader/minimax-estimation.html#least-favorable-sequence",
    "title": "Minimax Estimation",
    "section": "3 Least Favorable Sequence",
    "text": "3 Least Favorable Sequence\nSometimes there is no least favorable prior, e.g., if parameter space isn’t compact.\n\\(X \\sim N(\\theta, 1)\\): LF prior should spread mass everywhere, but that is not a proper prior.\nDefinition: A sequence \\(\\{\\pi_n\\}\\) is LF if \\(r(\\pi_n) \\to \\sup_\\pi r(\\pi)\\)\n\n3.1 Theorem\nSuppose \\(\\{\\pi_n\\}\\) is a prior sequence and \\(\\delta\\) satisfies \\(\\sup_\\theta R(\\theta, \\delta) = \\lim_n r(\\pi_n)\\). Then:\n\n\\(\\delta\\) is minimax\n\\(\\{\\pi_n\\}\\) is LF\n\nProof:\n\nOther est. \\(\\delta'\\). Then \\(\\forall n\\): \\[\\sup_\\theta R(\\theta, \\delta') \\geq \\int R(\\theta, \\delta') d\\pi_n(\\theta) \\geq r(\\pi_n)\\] \\[\\geq \\lim_n r(\\pi_n) = \\sup_\\theta R(\\theta, \\delta)\\]\nPrior \\(\\pi\\): \\[r(\\pi) = \\inf_\\delta \\int R(\\theta, \\delta) d\\pi(\\theta) \\leq \\int R(\\theta, \\delta) d\\pi(\\theta)\\] \\[\\leq \\sup_\\theta R(\\theta, \\delta) = \\lim_n r(\\pi_n)\\]\n\n\n\n3.2 Basic Picture\n\n\\(\\sup_\\theta R(\\theta, \\delta)\\) (generic \\(\\delta\\))\n\\(\\inf_\\delta \\sup_\\theta R(\\theta, \\delta)\\) (minimax risk)\n\\(\\sup_\\pi r(\\pi)\\) (if LF prior exists)\n\\(r(\\pi)\\) (generic \\(\\pi\\))\n\nIf minimax est. exists: \\(\\inf_\\delta \\sup_\\theta R(\\theta, \\delta) = \\sup_\\theta R(\\theta, \\delta^*)\\)\nIf LF prior exists: \\(\\sup_\\pi r(\\pi) = r(\\pi^*)\\)"
  },
  {
    "objectID": "reader/minimax-estimation.html#practical-applications",
    "href": "reader/minimax-estimation.html#practical-applications",
    "title": "Minimax Estimation",
    "section": "4 Practical Applications",
    "text": "4 Practical Applications\nMinimax estimators are very hard to find, but minimax bounds are often used in statistical theory to characterize hardness, especially lower bounds.\n\n4.1 Approach 1: Near-optimal Estimators\n\nPropose practical estimator \\(\\delta\\)\nFind \\(\\pi\\) for which \\(r(\\pi)\\) close to \\(\\sup_\\theta R(\\theta, \\delta)\\) (or same rate, or asymptotically)\nConclude \\(\\delta\\) can’t be improved much\n\n\n\n4.2 Approach 2: Problem Hardness\nQuantify hardness of a problem by its minimax rate in some asymptotic regime.\nCaveat: A problem might be easy throughout most of parameter space but very hard in some bizarre corner we never encounter in practice."
  },
  {
    "objectID": "reader/hypothesis-testing.html",
    "href": "reader/hypothesis-testing.html",
    "title": "Hypothesis Testing and the Neyman-Pearson Lemma",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/hypothesis-testing.html#hypothesis-testing",
    "href": "reader/hypothesis-testing.html#hypothesis-testing",
    "title": "Hypothesis Testing and the Neyman-Pearson Lemma",
    "section": "1 Hypothesis Testing",
    "text": "1 Hypothesis Testing\nIn hypothesis testing, we use data \\(X\\) to infer which of two sub-models generated the data.\nModel: \\(\\cP = \\{P_\\theta: \\theta \\in \\Theta\\}\\)\n\nNull hypothesis: \\(H_0: \\theta \\in \\Theta_0\\)\nAlternative hypothesis: \\(H_1: \\theta \\in \\Theta_1\\)\n\nWhenever \\(H_1\\) is unspecified, assume \\(\\Theta_1 = \\Theta_0^c\\).\n\\(H_0\\) is the default choice. We either: 1. Accept \\(H_0\\) (fail to reject, no definite conclusion) 2. Reject \\(H_0\\) (conclude false / \\(H_1\\) true)\n\n1.1 Examples\n\n\\(X \\sim N(\\theta, 1)\\), \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta \\neq \\theta_0\\)\n\\(H_0: \\theta = 0\\) vs \\(H_1: \\theta &gt; 0\\)\n\\(X_1, \\ldots, X_n \\sim P\\), \\(Y_1, \\ldots, Y_m \\sim Q\\), \\(H_0: P = Q\\) vs \\(H_1: P \\neq Q\\)\n\nCommon conceptual objection: We know \\(\\theta \\neq \\theta_0\\) or \\(P \\neq Q\\) already, why bother? (We will return to this.)\n\n\n1.2 Critical Function\nCan describe a test by its critical function (a.k.a. test function):\n\\[\n\\phi(x) = \\begin{cases}\n0 & \\text{accept } H_0 \\\\\n\\gamma \\in (0,1) & \\text{reject w.p. } \\gamma \\\\\n1 & \\text{reject } H_0\n\\end{cases}\n\\]\nIn practice, randomization is rarely used (\\(\\phi(x) \\in \\{0,1\\}\\)). In theory, it simplifies discussions.\nA non-randomized test partitions \\(\\cX\\) into: - \\(R = \\{x: \\phi(x) = 1\\}\\) - \\(A = \\{x: \\phi(x) = 0\\}\\)\n\n\n1.3 Power Function\nPower function: \\(\\beta(\\theta) = \\mathbb{E}_\\theta[\\phi(X)]\\) (rejection probability) \\(\\beta(\\theta) = P_\\theta(\\text{Reject } H_0)\\)\nFully summarizes test’s behavior.\n\\(\\phi\\) is a level \\(\\alpha\\) test if \\(\\forall \\theta \\in \\Theta_0\\), \\(\\beta(\\theta) \\leq \\alpha\\).\nUbiquitous choice is \\(\\alpha = 0.05\\) (most influential offhand remark in history of science).\nGoal: Maximize \\(\\beta(\\theta)\\) on \\(\\Theta_1\\) subject to level \\(\\alpha\\) constraint.\n\n\n1.4 Example: Normal Distribution Test\n\\(X \\sim N(\\theta, 1)\\), \\(\\theta_0 = 0\\), \\(H_1: \\theta \\neq 0\\)\nLet \\(\\Phi(z)\\) be standard normal CDF.\n\\(\\phi(x) = 1\\{|x| &gt; z_{\\alpha/2}\\}\\) (2-sided test)\n\\(\\beta(\\theta) = 1 - \\Phi(z_{\\alpha/2} - \\theta) + \\Phi(-z_{\\alpha/2} - \\theta)\\)\nSometimes a unique best test exists.\nExample: \\(X \\sim N(\\theta, 1)\\), \\(H_0: \\theta = 0\\), \\(H_1: \\theta = \\theta_1\\)\n\\(\\phi(x) = 1\\{x &gt; c\\}\\) is best possible level \\(\\alpha\\) test.\n\\(\\beta(\\theta) = \\begin{cases} \\alpha & \\text{if } \\theta = 0 \\\\ 1 - \\Phi(c - \\theta_1) & \\text{if } \\theta = \\theta_1 \\end{cases}\\)"
  },
  {
    "objectID": "reader/hypothesis-testing.html#likelihood-ratio-test",
    "href": "reader/hypothesis-testing.html#likelihood-ratio-test",
    "title": "Hypothesis Testing and the Neyman-Pearson Lemma",
    "section": "2 Likelihood Ratio Test",
    "text": "2 Likelihood Ratio Test\nWhen null and alternative are both simple, there exists a unique best test, which rejects for large values of the likelihood ratio.\nLet \\(LR(x) = \\frac{p_1(x)}{p_0(x)}\\) where \\(p_0, p_1\\) are densities (note: dominating measure always exists, e.g., \\(P_0 + P_1\\))\nLikelihood Ratio Test (LRT): \\[\n\\phi(x) = \\begin{cases}\n1 & \\text{if } LR(x) &gt; C \\\\\n\\gamma & \\text{if } LR(x) = C \\\\\n0 & \\text{if } LR(x) &lt; C\n\\end{cases}\n\\]\n\\(C, \\gamma\\) chosen to make \\(\\mathbb{E}_0[\\phi(X)] = \\alpha\\)\nIntuitions (discrete case): - Power under \\(H_1\\): \\(\\sum_x \\phi(x) p_1(x) = \\mathbb{E}_1[\\phi(X)]\\) - Significance level: \\(\\sum_x \\phi(x) p_0(x) = \\mathbb{E}_0[\\phi(X)]\\)\nAnalogy: $$100 to buy as much flour as possible"
  },
  {
    "objectID": "reader/hypothesis-testing.html#neyman-pearson-lemma",
    "href": "reader/hypothesis-testing.html#neyman-pearson-lemma",
    "title": "Hypothesis Testing and the Neyman-Pearson Lemma",
    "section": "3 Neyman-Pearson Lemma",
    "text": "3 Neyman-Pearson Lemma\n\n3.1 Theorem (Neyman-Pearson Lemma)\nLRT with significance level \\(\\alpha\\) is optimal for testing \\(H_0: X \\sim p_0\\) vs \\(H_1: X \\sim p_1\\)\nProof: We are interested in maximization problem:\n\\[\n\\begin{aligned}\n&\\text{maximize } \\mathbb{E}_1[\\phi(X)] \\\\\n&\\text{subject to } \\mathbb{E}_0[\\phi(X)] \\leq \\alpha, \\quad \\phi(x) \\in [0,1]\n\\end{aligned}\n\\]\nLagrange form:\n\\[\n\\text{maximize } \\mathbb{E}_1[\\phi(X)] - \\lambda(\\mathbb{E}_0[\\phi(X)] - \\alpha)\n\\]\n\\[\n= \\int \\phi(x)[p_1(x) - \\lambda p_0(x)]d\\mu(x) + \\lambda\\alpha\n\\]\nSolution: \\[\n\\phi^*(x) = \\begin{cases}\n1 & \\text{if } LR(x) &gt; C \\\\\n\\gamma & \\text{if } LR(x) = C \\\\\n0 & \\text{if } LR(x) &lt; C\n\\end{cases}\n\\]\n\\(\\phi^*\\) maximizes Lagrangian for \\(\\lambda = C\\)\nConsider any other test \\(\\phi(x) \\in [0,1]\\):\n\\[\n\\begin{aligned}\n\\mathbb{E}_1[\\phi^*] - \\lambda\\mathbb{E}_0[\\phi^*] &= \\int \\phi^*(x)[p_1(x) - \\lambda p_0(x)]d\\mu(x) \\\\\n&\\geq \\int \\phi(x)[p_1(x) - \\lambda p_0(x)]d\\mu(x) \\\\\n&= \\mathbb{E}_1[\\phi] - \\lambda\\mathbb{E}_0[\\phi]\n\\end{aligned}\n\\]\nTherefore, \\(\\mathbb{E}_1[\\phi^*] - \\mathbb{E}_1[\\phi] \\geq \\lambda(\\mathbb{E}_0[\\phi^*] - \\mathbb{E}_0[\\phi])\\)\nWe can choose two free parameters. We need both to solve one equation:\n\\[\n\\alpha = \\mathbb{E}_0[\\phi^*(X)] = P_0(LR &gt; C) + \\gamma P_0(LR = C)\n\\]\nCase 1: \\(C\\) is upper \\(\\alpha\\)-quantile of \\(LR\\) Case 2: \\(C\\) and \\(\\gamma\\) discrete\n\\(P_0(LR &gt; c)\\) jumps down at discrete values \\(c_1 &lt; c_2 &lt; \\cdots\\) Set \\(i = \\max\\{i: P_0(LR &gt; c_i) \\geq \\alpha\\}\\) Then \\(P_0(LR &gt; c_i) \\geq \\alpha &gt; P_0(LR \\geq c_{i+1})\\)\n\\[\n\\gamma = \\frac{\\alpha - P_0(LR &gt; c_i)}{P_0(LR = c_i)}\n\\]\n\n\n3.2 Example 1: Exponential Family\n\\(p_\\eta(x) = e^{\\eta^T T(x) - A(\\eta)}h(x)\\)\n\\(H_0: \\eta = \\eta_0\\) vs \\(H_1: \\eta = \\eta_1 &gt; \\eta_0\\)\n\\[\nLR(x) = e^{(\\eta_1 - \\eta_0)^T T(x) - [A(\\eta_1) - A(\\eta_0)]}\n\\]\n\\(\\phi^*\\) rejects for large \\(T(X)\\):\n\\[\n\\phi^*(x) = \\begin{cases}\n1 & \\text{if } T(X) &gt; c \\\\\n\\gamma & \\text{if } T(X) = c \\\\\n0 & \\text{if } T(X) &lt; c\n\\end{cases}\n\\]\nChoose \\(c, \\gamma\\) to make:\n\\[\n\\beta(\\eta_0) = P_{\\eta_0}(T(X) &gt; c) + \\gamma P_{\\eta_0}(T(X) = c) = \\alpha\n\\]\n\n\n3.3 Example 2: \\(X_1, \\ldots, X_n \\sim p(x; \\theta)\\) (same \\(H_0, H_1\\))\nReject for large \\(\\sum_i T(X_i)\\)\nImportant: \\(\\phi^*\\) depends only on \\(\\eta_0\\) and sufficient statistic \\(T(\\cdot)\\), not on \\(\\eta_1\\).\nNext topic: Uniformly most powerful (UMP) tests."
  },
  {
    "objectID": "reader/hierarchical-bayes.html",
    "href": "reader/hierarchical-bayes.html",
    "title": "Hierarchical Bayes, Markov Chain Monte Carlo, and Empirical Bayes",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/hierarchical-bayes.html#hierarchical-bayes",
    "href": "reader/hierarchical-bayes.html#hierarchical-bayes",
    "title": "Hierarchical Bayes, Markov Chain Monte Carlo, and Empirical Bayes",
    "section": "1 Hierarchical Bayes",
    "text": "1 Hierarchical Bayes\nThe full power of Bayes is realized in large, complex problems with repeat structure, allowing us to pool information across many observations.\n\n1.1 Example: Predicting Batting Averages\nPredict a batter’s true batting average from \\(n_i\\) at-bats, \\(X_i\\) hits: \\(X_i|\\theta_i \\sim \\text{Binom}(n_i, \\theta_i)\\)\nPool info across players \\(i=1,\\ldots,m\\) via hierarchical model:\n\\[\n\\begin{aligned}\n\\alpha, \\beta &\\sim \\pi_0(\\alpha, \\beta) \\quad \\text{(hyperprior)} \\\\\n\\theta_i|\\alpha, \\beta &\\sim \\text{Beta}(\\alpha, \\beta), \\quad i=1,\\ldots,m \\\\\nX_i|\\theta_i, n_i &\\sim \\text{Binom}(n_i, \\theta_i), \\quad i=1,\\ldots,m\n\\end{aligned}\n\\]\n\\[\n\\mathbb{E}[\\theta_i|X] = \\mathbb{E}[\\mathbb{E}[\\theta_i|X, \\alpha, \\beta]|X] = \\mathbb{E}\\left[\\frac{\\alpha + X_i}{\\alpha + \\beta + n_i}|X\\right]\n\\]\nUse all \\(X_1,\\ldots,X_m\\) to learn a good prior on \\(\\theta_i\\).\nNote: There is always an equivalent model where we marginalize over \\(\\alpha, \\beta\\) and just write a more complicated prior on \\(\\theta\\). The hierarchical version may give better intuition or computational strategies.\n\n\n1.2 Gaussian Hierarchical Model\n\\[\n\\begin{aligned}\n\\theta_i &\\sim N(\\mu, \\tau^2), \\quad i=1,\\ldots,d \\\\\nX_i|\\theta_i &\\sim N(\\theta_i, \\sigma^2), \\quad i=1,\\ldots,d\n\\end{aligned}\n\\]\nPosterior mean:\n\\[\n\\mathbb{E}[\\theta_i|X] = \\mathbb{E}[\\mathbb{E}[\\theta_i|X, \\mu, \\tau^2]|X] = \\mathbb{E}\\left[\\frac{\\tau^2}{\\tau^2 + \\sigma^2}X_i + \\frac{\\sigma^2}{\\tau^2 + \\sigma^2}\\mu|X\\right]\n\\]\nLinear shrinkage estimator: Bayes optimal shrinkage estimated from data.\nLikelihood for \\(\\mu, \\tau^2\\) (marginalizing over \\(\\theta_i\\)):\n\\[\n\\begin{aligned}\nX_i|\\mu, \\tau^2 &\\sim N(\\mu, \\tau^2 + \\sigma^2) \\\\\n\\bar{X} &\\sim N(\\mu, \\frac{\\tau^2 + \\sigma^2}{n}) \\\\\nS^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 &\\sim \\frac{\\tau^2 + \\sigma^2}{n-1}\\chi^2_{n-1}\n\\end{aligned}\n\\]\nDefine \\(B = \\tau^2 + \\sigma^2\\) (amount of shrinkage):\n\\[\n\\delta(x) = \\mathbb{E}[\\mathbb{E}[\\theta_i|X, B]|X] = \\mathbb{E}\\left[\\frac{B - \\sigma^2}{B}X_i + \\frac{\\sigma^2}{B}\\bar{X}|X\\right]\n\\]\nEstimated from entire data set:\n\\[\n\\begin{aligned}\n\\bar{X} &\\sim N(\\mu, \\frac{B}{n}) \\\\\n(n-1)S^2 &\\sim B\\chi^2_{n-1}\n\\end{aligned}\n\\]\nConjugate prior (scale mixture):\n\\[\n\\pi(B|\\lambda, \\nu) \\propto B^{-\\nu/2-2}\\exp(-\\frac{\\lambda}{2B})\n\\]\n\\[\nB|X \\sim \\text{InvGamma}\\left(\\frac{n+\\nu}{2}, \\frac{\\lambda + (n-1)S^2}{2}\\right)\n\\]\n\\[\n\\mathbb{E}\\left[\\frac{1}{B}|X\\right] = \\frac{n+\\nu}{\\lambda + (n-1)S^2}\n\\]\n\\[\n\\delta_i(x) = \\frac{(n-3)S^2}{(n-1)S^2 + \\lambda}X_i + \\frac{\\lambda + 2S^2}{(n-1)S^2 + \\lambda}\\bar{X}\n\\]\nPseudo-data: \\(\\nu, \\lambda\\) with \\(\\nu \\approx 2, \\lambda \\approx \\nu\\sigma^2\\)\nMight want to truncate prior to \\([\\sigma^2, \\infty)\\) if \\(\\lambda\\) small.\n\n\n1.3 Graphical Form\nFor hyperparameters \\(\\alpha, \\beta\\):\n     α, β\n    /  |  \\\n   θ₁  θ₂  θ₃\n   |   |   |\n   X₁  X₂  X₃\nThese are the distributions associated with a factor for each vertex in a DAG \\((V,E)\\):\n\\[\np(z) = \\prod_{i=1}^{|V|} p(z_i|z_{\\text{pa}(i)})\n\\]\nFor this model:\n\\[\np(\\alpha, \\beta, \\theta_1,\\ldots,\\theta_m, X_1,\\ldots,X_m) = p(\\alpha, \\beta)\\prod_{i=1}^m p(\\theta_i|\\alpha, \\beta)p(X_i|\\theta_i)\n\\]"
  },
  {
    "objectID": "reader/hierarchical-bayes.html#markov-chain-monte-carlo-mcmc",
    "href": "reader/hierarchical-bayes.html#markov-chain-monte-carlo-mcmc",
    "title": "Hierarchical Bayes, Markov Chain Monte Carlo, and Empirical Bayes",
    "section": "2 Markov Chain Monte Carlo (MCMC)",
    "text": "2 Markov Chain Monte Carlo (MCMC)\nHierarchical models are very flexible, but often create big computational headaches:\n\\[\n\\pi(\\theta|x) = \\frac{p(x|\\theta)\\pi(\\theta)}{\\int p(x|\\theta)\\pi(\\theta)d\\theta}\n\\]\nNumerator is usually nice, denominator often intractable.\nComputational strategy: Set up a Markov chain with stationary distribution \\(\\pi(\\theta|x)\\), run it to get approximate samples from \\(\\pi(\\theta|x)\\).\n\n2.1 Definition of Markov Chain\nA stationary Markov chain with transition kernel \\(Q(y|x)\\) and initial distribution \\(\\pi_0(x)\\) is a sequence of r.v.’s \\(X_0, X_1, \\ldots\\) where \\(X_0 \\sim \\pi_0\\) and:\n\\[\nP(X_{t+1} \\in A | X_t, \\ldots, X_0) = P(X_{t+1} \\in A | X_t) = \\int_A Q(y|X_t) dy\n\\]\nMarginal distribution of \\(X_t\\):\n\\[\n\\pi_t(y) = P(X_t \\in A) = \\int Q(y|x)\\pi_{t-1}(x) dx\n\\]\nThis is a directed graphical model: \\(X_0 \\to X_1 \\to X_2 \\to \\cdots\\)\nIf \\(\\pi(y) = \\int Q(y|x)\\pi(x) dx\\), we say \\(\\pi\\) is a stationary distribution for \\(Q\\).\nA sufficient condition is detailed balance:\n\\[\n\\pi(x)Q(y|x) = \\pi(y)Q(x|y)\n\\]\n\\[\n\\int_y Q(y|x)\\pi(x) dx = \\int_y \\pi(y)Q(x|y) dy = \\pi(y)\n\\]\nA Markov chain with detailed balance is called reversible: \\(X_{t-1} | X_t \\sim X_{t+1} | X_t\\) if \\(\\pi_t = \\pi\\).\n\n\n2.2 Theory\nIf a Markov chain with stationary distribution \\(\\pi\\) is:\n\nIrreducible: \\(\\forall x,y, \\exists n: P(X_n = y | X_0 = x) &gt; 0\\)\nAperiodic: \\(\\forall x, \\gcd\\{n &gt; 0: P(X_n = x | X_0 = x) &gt; 0\\} = 1\\)\n\nThen \\(\\pi_t \\to \\pi\\) in TV distance, regardless of \\(\\pi_0\\) (chain “forgets” \\(\\pi_0\\)).\nStrategy: Find \\(Q\\) with stationary distribution \\(\\pi(\\theta|x)\\), start at any \\(X_0\\), run chain for a long time, then \\(X_t\\) is approximately a sample from posterior for large \\(t\\)."
  },
  {
    "objectID": "reader/hierarchical-bayes.html#gibbs-sampler",
    "href": "reader/hierarchical-bayes.html#gibbs-sampler",
    "title": "Hierarchical Bayes, Markov Chain Monte Carlo, and Empirical Bayes",
    "section": "3 Gibbs Sampler",
    "text": "3 Gibbs Sampler\nFor parameter vector \\(\\theta = (\\theta_1, \\ldots, \\theta_d)\\):\nAlgorithm: 1. Initialize \\(\\theta^{(0)}\\) 2. For \\(t = 1, \\ldots, T\\): For \\(j = 1, \\ldots, d\\): Sample \\(\\theta_j^{(t)} \\sim p(\\theta_j | \\theta_{-j}^{(t-1)}, X)\\) Record \\(\\theta^{(t)}\\)\nVariations: - Update one random coordinate \\(J \\sim \\text{Unif}(1,\\ldots,d)\\) - Update coordinates in random order\nAdvantage for hierarchical priors: only need to sample low-dimensional conditional distributions:\n\\[\np(\\theta_i | \\theta_{-i}, X, \\alpha) \\propto p(\\theta_i | \\theta_{\\text{pa}(i)}) \\prod_{j \\in \\text{ch}(i)} p(\\theta_j | \\theta_i)\n\\]\nEspecially easy if using conjugate priors at all levels; often can be parallelized.\n\n3.1 Gibbs Stationarity\nClaim: If \\(\\pi_t = \\pi(\\theta|X)\\), then \\(\\pi_{t+1} = \\pi(\\theta|X)\\).\nProof (sketch): Consider updating only one fixed coordinate \\(j\\): \\(\\theta_j^{(t+1)} \\sim p(\\theta_j | \\theta_{-j}^{(t)}, X)\\) If \\(\\theta^{(t)} \\sim \\pi(\\theta|X)\\), then \\(\\theta^{(t+1)} \\sim \\pi(\\theta|X)\\) Updating any coordinate preserves posterior distribution Updating coordinates in any order also does\nIn theory: Pick initialization and valid kernel \\(Q\\), sample long enough, get \\(\\theta^{(t)} \\sim \\pi(\\theta|X)\\) Do it again \\(N\\) more times, get \\(N\\) samples from \\(\\pi(\\theta|X)\\)\nIn practice: How do we know we’ve sampled long enough? - Plot \\(\\theta_i^{(t)}\\) vs \\(t\\), show how fast the MC mixes - Look for \\(\\hat{R} = \\frac{\\text{between-chain variance}}{\\text{within-chain variance}} \\approx 1\\)\nThese methods are GOOD, NOT GREAT. Can be deceived, especially for bimodal posteriors.\nBetter approach: Burn-in then convergence. Estimate posterior based on \\(\\theta^{(B+1)}, \\ldots, \\theta^{(B+N)}\\)\nFor function \\(f(\\theta)\\): \\(\\hat{\\mu}_f = \\frac{1}{N} \\sum_{t=B+1}^{B+N} f(\\theta^{(t)})\\)\n\n\n3.2 Implementation Details Matter\nExample: \\[\n\\begin{aligned}\n\\theta &\\sim N(0, 100) \\\\\nX_i | \\theta &\\sim N(\\theta, 0.1^2), \\quad i=1,\\ldots,n\n\\end{aligned}\n\\]\nPosterior: \\(\\theta | X \\sim N\\left(\\frac{\\bar{X}/0.01^2}{n/0.01^2 + 1/100}, \\frac{1}{n/0.01^2 + 1/100}\\right)\\)\nGibbs sampler: 1. \\(\\mathbb{E}[\\theta|X] = 8.9, \\text{SD}(\\theta|X) = 0.1\\) 2. \\(\\theta^{(t+1)} | X, \\theta^{(t)} \\sim N(8.9, 0.1^2)\\)\nGibbs takes a long time to mix.\nBetter parameterization: \\[\n\\begin{aligned}\nB &= \\theta - \\bar{X} \\\\\n\\theta &= B + \\bar{X}\n\\end{aligned}\n\\]\n\\(B|X \\sim N(0, 0.1^2)\\)\nGibbs: Directly sampling from posterior"
  },
  {
    "objectID": "reader/hierarchical-bayes.html#empirical-bayes",
    "href": "reader/hierarchical-bayes.html#empirical-bayes",
    "title": "Hierarchical Bayes, Markov Chain Monte Carlo, and Empirical Bayes",
    "section": "4 Empirical Bayes",
    "text": "4 Empirical Bayes\nBack to Gaussian hierarchical model:\n\\[\n\\begin{aligned}\n\\theta_i &\\sim N(\\mu, \\tau^2) \\\\\nX_i | \\theta_i &\\sim N(\\theta_i, \\sigma^2)\n\\end{aligned}\n\\]\n\\[\n\\mathbb{E}[\\theta_i | X, B] = \\frac{B - \\sigma^2}{B}X_i + \\frac{\\sigma^2}{B}\\bar{X}, \\quad B = \\tau^2 + \\sigma^2\n\\]\nFor any reasonable prior: \\(B | X \\approx \\frac{1}{N}\\sum_{i=1}^N (X_i - \\bar{X})^2\\)\nIf prior doesn’t matter much, why use one? Could just estimate \\(B\\) from data, however we want.\nA minimax estimator is \\(B = \\sigma^2 + \\frac{1}{N}\\sum_{i=1}^N (X_i - \\bar{X})^2\\)\nCalled Empirical Bayes: a hybrid approach in which hyperparameters are treated as fixed, others treated as random."
  },
  {
    "objectID": "reader/score-fisher.html",
    "href": "reader/score-fisher.html",
    "title": "Score Function and Fisher Information",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/score-fisher.html#outline",
    "href": "reader/score-fisher.html#outline",
    "title": "Score Function and Fisher Information",
    "section": "1 Outline",
    "text": "1 Outline\n\nScore function\nFisher information\nCramér-Rao Lower Bound\nExamples"
  },
  {
    "objectID": "reader/score-fisher.html#motivation-tangent-family",
    "href": "reader/score-fisher.html#motivation-tangent-family",
    "title": "Score Function and Fisher Information",
    "section": "2 Motivation: Tangent Family",
    "text": "2 Motivation: Tangent Family\nConsider a family of densities:\n\\[p(x; \\theta) = e^{\\theta'T(x) - A(\\theta)}h(x)\\]\nwhere \\(\\theta \\in \\RR^d\\) and \\(A(\\theta) = \\log \\int e^{\\theta'T(x)}h(x)dx\\).\nFor this family:\n\n\\(T(X)\\) is complete sufficient\n\\(T(X)\\) is minimal\n\\(\\PP_\\theta(T(X) = t) = e^{\\theta't - A(\\theta)}\\)\n\\(\\EE_\\theta[T(X)] = A'(\\theta)\\)\n\nLet \\(\\theta_0 \\in \\RR^d\\) be fixed. Define the tangent family\n\\[q(x; t) = e^{t'\\nabla l_{\\theta_0}(x) - k(t)}p_{\\theta_0}(x)\\]\nwhere \\(k(t) = \\log \\int e^{t'\\nabla l_{\\theta_0}(x)}p_{\\theta_0}(x)dx\\).\nThen \\(\\nabla l_{\\theta_0}(X)\\) is complete sufficient for the tangent family at \\(\\theta_0\\).\nThis is called the Score function."
  },
  {
    "objectID": "reader/score-fisher.html#score-function",
    "href": "reader/score-fisher.html#score-function",
    "title": "Score Function and Fisher Information",
    "section": "3 Score Function",
    "text": "3 Score Function\nAssume a family \\(\\cP\\) has densities \\(p_\\theta\\) with respect to a measure \\(\\mu\\), for \\(\\theta \\in \\Theta \\subseteq \\RR^d\\). Assume additionally that these densities have common support: that \\(\\{x: p_\\theta(x) &gt; 0\\}\\) is the same for all \\(\\theta\\).\nRecall the log-likelihood is \\(l(\\theta;X) = \\log p_\\theta(X)\\) (thought of as a random function of \\(\\theta\\))\nDefinition: The Score function is \\(\\nabla l_\\theta(X)\\).\nIt plays a key role in many areas of statistics, especially in asymptotics. We can think of it as a “local complete sufficient statistic.” For \\(\\eta \\approx 0\\), and \\(\\theta_0 \\in \\Theta^\\circ\\), we have\n\\[p_{\\theta_0+\\eta}(x) = e^{\\ell(\\theta_0 + \\eta; x)} \\approx e^{\\eta'\\nabla \\ell(\\theta_0;x)}p_{\\theta_0}(x).\\]"
  },
  {
    "objectID": "reader/score-fisher.html#differential-identities-and-the-fisher-information",
    "href": "reader/score-fisher.html#differential-identities-and-the-fisher-information",
    "title": "Score Function and Fisher Information",
    "section": "4 Differential Identities and the Fisher Information",
    "text": "4 Differential Identities and the Fisher Information\nAssuming enough regularity, we can arrive at some important differential identities by differentiating both sides of the equation\n\\[1 = \\int_\\cX e^{\\ell(\\theta;x)}\\,d\\mu(x).\\]\nDifferentiating both sides with respect to \\(\\theta_j\\), we obtain \\[0 = \\int_\\cX \\frac{\\partial}{\\partial \\theta_j} \\ell(\\theta; x) e^{\\ell(\\theta; x)}\\,d\\mu(x) = \\EE_\\theta \\left[\\frac{\\partial}{\\partial\\theta_j}\\ell(\\theta;X)\\right].\\] Collecting these identities into a vector, we obtain \\[\\EE_\\theta [\\nabla \\ell(\\theta; X)] = 0.\\] Importantly, note that this identity only holds if the \\(\\theta\\) in the subscript (defining the distribution with respect to which the expectation is taken) matches the \\(\\theta\\) at which the gradient is being evaluated.\nIf we differentiate the identity a second time with respect to \\(\\theta_k\\), we obtain \\[0 = \\int_\\cX \\left(\\frac{\\partial^2\\ell}{\\partial \\theta_j\\partial\\theta_k} + \\frac{\\partial \\ell}{\\partial \\theta_j}\\frac{\\partial \\ell}{\\partial\\theta_k}\\right) e^{\\ell}\\,d\\mu = \\EE_\\theta\\left[\\frac{\\partial^2\\ell}{\\partial \\theta_j\\partial\\theta_k}\\right] + \\EE_\\theta\\left[\\frac{\\partial \\ell}{\\partial \\theta_j}\\frac{\\partial \\ell}{\\partial \\theta_k}\\right]\n%= \\EE_\\theta\\left[\\frac{\\partial^2\\ell}{\\partial \\theta_j\\partial\\theta_k}\\right] + \\Cov_\\theta\\left(\\frac{\\partial \\ell}{\\partial \\theta_j},\\frac{\\partial \\ell}{\\partial \\theta_k}\\right).\n\\] Again collecting these identities into a matrix, and noting that \\[\\EE_\\theta\\left[\\frac{\\partial \\ell}{\\partial \\theta_j}\\frac{\\partial \\ell}{\\partial \\theta_k}\\right] = \\Cov_\\theta\\left(\\frac{\\partial \\ell}{\\partial \\theta_j},\\frac{\\partial \\ell}{\\partial \\theta_k}\\right),\\] we obtain \\[\\Var_\\theta\\left(\\nabla\\ell(\\theta;X)\\right) = \\EE_\\theta\\left[-\\nabla^2\\ell(\\theta;X)\\right],\\] again with the important observation that the \\(\\theta\\) in both subscripts must match the \\(\\theta\\) where the first and second derivatives are evaluated.\nThe left-hand side of the last equation, the variance of the score, is called the Fisher Information matrix \\[ J(\\theta) := \\Var_\\theta(\\nabla\\ell(\\theta;X)). \\] Note \\(J(\\theta)\\) is always positive semidefinite. It is possible to extend this definition to certain models where \\(\\ell(\\theta;x)\\) is not differentiable with respect to \\(\\theta\\), such as the Laplace location family. However we will not explore these generalizations."
  },
  {
    "objectID": "reader/score-fisher.html#cramér-rao-lower-bound",
    "href": "reader/score-fisher.html#cramér-rao-lower-bound",
    "title": "Score Function and Fisher Information",
    "section": "5 Cramér-Rao Lower Bound",
    "text": "5 Cramér-Rao Lower Bound\nLet \\(\\delta(X)\\) be any real-valued statistic. Let \\(g(\\theta) = \\EE_\\theta[\\delta]\\), so \\(\\delta\\) is an unbiased estimator for \\(g(\\theta)\\). If we repeat the idea of differentiating \\(g(\\theta) = \\int \\delta(x) e^{\\ell(\\theta;x)}\\,d\\mu(x)\\) with respect to \\(\\theta_j\\) for each \\(j\\), and collect the resulting partial derivatives into a vector, we obtain\n\\[\\nabla g(\\theta) = \\int \\delta(x) \\nabla \\ell(\\theta;x) e^{\\ell(\\theta;x)}\\,d\\mu(x) = \\EE_\\theta\\left[\\delta(X) \\nabla\\ell(\\theta;X)\\right] = \\Cov_\\theta\\left(\\delta(X), \\nabla\\ell(\\theta;X)\\right).\\] Combining these results with the Cauchy-Schwarz inequality gives us the Cramér-Rao Lower Bound, also known as the Information lower bound. For a single parameter (\\(d=1\\)), we have \\[\\Var_\\theta(\\delta(X)) \\cdot \\Var_\\theta(\\dot{\\ell}(\\theta;X)) \\geq \\Cov_\\theta(\\delta(X), \\dot{\\ell}(\\theta; X))^2, \\] so after rearranging terms and applying identities, \\[\\Var_\\theta(\\delta(X)) \\geq \\frac{\\dot{g}(\\theta)^2}{J(\\theta)}.\\]\nFor the multivariate case (\\(d&gt;1\\)), we have more generally \\[ \\Var_\\theta(\\delta(X) \\geq \\nabla g(\\theta)'J(\\theta)^{-1}\\nabla g(\\theta).\\] The interpretation of this identity is that no unbiased estimator for \\(g(\\theta)\\) can have variance smaller than \\(\\nabla g(\\theta)'J(\\theta)^{-1}\\nabla g(\\theta)\\). In particular, if \\(g(\\theta) = \\theta_j\\), no estimator can have variance smaller than \\((J(\\theta)^{-1})_{jj}\\).\n\\[\\Var_\\theta(\\delta) \\geq \\Var_\\theta(\\delta(X)) \\Cov_\\theta(\\delta, \\nabla l_\\theta(X))I(\\theta)^{-1}\\Cov_\\theta(\\delta, \\nabla l_\\theta(X))' = g'(\\theta)I(\\theta)^{-1}g'(\\theta)'\\]\n\n\n\n\n\n\nExpand to see proof\n\n\n\n\n\nFor any \\(a \\in \\RR^d\\), we can write \\[\\begin{aligned}\n\\Var_\\theta(\\delta(X)) \\cdot a'J(\\theta)a &= \\Var_\\theta(\\delta)\\Var_\\theta(a'\\nabla\\ell(\\theta;X))\\\\\n&\\geq \\Cov_\\theta(\\delta(X), a'\\nabla\\ell(\\theta;X))^2\\\\\n&= \\left(a'\\nabla \\Cov_\\theta(\\delta, \\nabla\\ell(\\theta))\\right)^2\\\\\n&= (a'\\nabla g(\\theta))^2.\n\\end{aligned}\\]\nThus we obtain for all nonzero \\(a \\in \\RR^d\\),\n\\[\\Var_\\theta(\\delta(X)) \\geq \\frac{(a'\\nabla g(\\theta))^2}{a'J(\\theta)a}.\\]\nWe obtain the result by optimizing the bound, with \\(a = J(\\theta)^{-1}\\nabla g(\\theta)\\) (show this as an exercise)."
  },
  {
    "objectID": "reader/score-fisher.html#examples",
    "href": "reader/score-fisher.html#examples",
    "title": "Score Function and Fisher Information",
    "section": "6 Examples",
    "text": "6 Examples\nExample: i.i.d. sample\nAssume \\(X_1, \\ldots, X_n \\simiid p_\\theta^{(1)}(x)\\), for \\(\\theta \\in \\Theta \\subseteq \\RR^d\\).\nAssume additionally that \\(p_\\theta^{(1)}\\) is “regular:” it has common support, and finite derivative w.r.t. \\(\\theta\\).\nThen the full data density is \\(p_\\theta(x) = \\prod_i p_\\theta^{(1)}(x_i)\\).\nDefine the single-sample log-likelihood \\(\\ell_1(\\theta;x_i) = \\log p_\\theta^{(1)}(x_i)\\); then we have \\(\\ell(\\theta;x) = \\sum_i \\ell_1(\\theta;x_i)\\).\nThen the Fisher information for the full sample is \\[J(\\theta) = \\Var_\\theta(\\nabla \\ell(\\theta; X)) = \\sum_{i=1}^n \\Var_\\theta(\\nabla \\ell_1(\\theta; X_i)) = n J_1(\\theta),\\] where \\(J_1(\\theta) = \\Var_\\theta(\\nabla\\ell(\\theta; X_1))\\) is the Fisher information for a single sample.\nAs a result, we see that the Information bound scales like \\(n^{-1}\\) for regular families; in other words, the standard deviation of an estimator should scale roughly like \\(1/\\sqrt{n}\\).\nExample: exponential family\nSuppose we have an exponential family of the form \\[ p_\\eta(x) = e^{\\eta'T(x) - A(\\eta)} h(x).\\]\nThe log-likelihood is \\(\\ell(\\eta;X) = \\eta'T(X) - A(\\eta) + \\log h(X)\\), and its gradient (the score) is \\[\\nabla \\ell(\\eta;X) = T(X) - \\nabla A(\\eta) = T(X) - \\EE_\\eta T(X).\\] Since \\(\\EE_\\eta T(X)\\) is nonrandom, the variance is \\[ J(\\eta) = \\Var_\\eta (T(X)) = \\nabla^2 A(\\eta).\\]\nWe could alternatively derive the Fisher information from taking a second derivative with respect to \\(\\eta\\), giving \\[ \\nabla^2\\ell(\\eta;X) = -\\nabla^2 A(\\eta),\\] which is deterministically equal to \\(-\\Var_\\eta(T(X))\\), so we have confirmed the identity \\(J(\\eta) = -\\EE_\\eta[\\nabla^2 \\ell(\\eta;X)]\\).\nExample: Curved exponential family\nNext, consider a curved version of the previous family, parameterized by \\(\\theta \\in \\RR\\): \\[p_\\theta(x) = e^{\\eta(\\theta)'T(x) - B(\\theta)}h(x),\\quad \\text{ with } B(\\theta) = A(\\eta(\\theta))\\] Again, the log-likelihood is \\[\\ell(\\theta;X) = \\eta(\\theta)'T(x) - B(\\theta)  + \\log h(x),\\] and its first derivative is \\[\\begin{aligned}\n\\dot{\\ell}(\\theta;X) &= \\dot{\\eta}(\\theta)'T(X) - \\dot{\\eta}(\\theta)'\\nabla_\\eta A(\\eta(\\theta))\\\\\n&= \\dot{\\eta}(\\theta) '\\left(T(X) - \\nabla_\\eta A(\\eta(\\theta))\\right)\\\\\n&= \\dot{\\eta}(\\theta)'(T(X) - \\EE_\\theta T(X)).\\end{aligned}\\]\nAs a result, the Fisher information is \\[J(\\theta) = \\Var_\\theta(\\dot{\\eta}(\\theta)'T(X)) =  \\dot{\\eta}(\\theta)'\\Var_\\theta(T(X))\\dot{\\eta}(\\theta).\\] Note in this model \\(\\dot{\\eta}'T(X)\\) is a “local complete sufficient statistic” for the model near \\(\\theta\\)."
  },
  {
    "objectID": "reader/score-fisher.html#efficiency",
    "href": "reader/score-fisher.html#efficiency",
    "title": "Score Function and Fisher Information",
    "section": "7 Efficiency",
    "text": "7 Efficiency\nThe CRLB is not necessarily attainable.\nWe define the efficiency of an unbiased estimator as:\n\\[\\text{eff}_\\delta(\\theta) = \\frac{\\text{CRLB}(\\theta)}{\\Var_\\theta(\\delta)} \\leq 1,\\]\nWe say \\(\\delta(X)\\) is efficient if \\(\\text{eff}_\\delta(\\theta) = 1\\) for all \\(\\theta\\).\nFor \\(g(\\theta)=\\theta\\in \\RR\\), the efficiency depends on how correlated \\(\\delta(X)\\) is with the score: \\[\\begin{aligned}\\text{eff}_\\delta(\\theta) &= \\frac{\\Cov_\\theta(\\delta(X), \\dot{\\ell}(\\theta;X))^2}{\\Var_\\theta(\\delta(X)) \\cdot \\Var_\\theta(\\dot{\\ell}(\\theta;X))}\\\\\n&= \\Corr_\\theta(\\delta,\\dot{\\ell}(\\theta))^2\n\\end{aligned}\\]\nThus, an efficient estimator for \\(\\theta\\) is one that is perfectly correlated with the score. This is rarely achieved in finite samples, but we can often approach it asymptotically as \\(n \\to \\infty\\)."
  },
  {
    "objectID": "reader/completeness.html",
    "href": "reader/completeness.html",
    "title": "Completeness, Ancillarity, and Basu’s Theorem",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/completeness.html#completeness",
    "href": "reader/completeness.html#completeness",
    "title": "Completeness, Ancillarity, and Basu’s Theorem",
    "section": "1 Completeness",
    "text": "1 Completeness\nAs we have seen, for a given statistical problem we may have many different sufficient statistics, some of which reduce the data more than others. We usually want to look for one that is minimal sufficient, meaning that it strips away as much irrelevant information as possible and only retains the information that is relevant to estimating the parameter.\nIn some cases the minimal sufficient statistic has an additional property called completeness. The definition of completeness is initially counterintuitive, but it has a number of useful implications we will explore throughout the semester.\n\n1.1 Definition of completeness\nA statistic \\(T(X)\\) is complete for a family of distributions \\(\\cP = \\{P_\\theta: \\theta \\in \\Theta\\}\\) if no nontrivial function of \\(T\\) can have expectation zero for every distribution in the family:\n\\[\\EE_\\theta \\,f(T(X)) = 0 \\quad \\forall \\theta \\in \\Theta \\implies f(T) \\eqPas 0\\] ::: callout-note The name for complete statistics comes from a prior notion that \\(\\cP^T = \\{P_\\theta^T:\\; \\theta \\in \\Theta\\}\\) is ``complete’’ as a model if its linear span includes all possible distributions on \\(T(X)\\); see Homework 3. :::\nIf \\(X\\) itself is complete, then the definition immediately implies that there can be at most one unbiased estimator for any estimand: if \\(\\EE_\\theta \\delta_1(X) = \\EE_\\theta \\delta_2(X) = g(\\theta)\\) for all \\(\\theta \\in \\Theta\\), then \\(f(X) = \\delta_1(X) - \\delta_2(X) = 0\\) almost surely. More generally, if \\(T(X)\\) is a complete statistic then there can be at most one unbiased estimator that runs through \\(T\\). We will return to this fact when we discuss unbiased estimation.\nWe will be especially interested in statistics that are both complete and sufficient. If \\(T(X)\\) is complete and sufficient we call it a complete sufficient statistic.\n\n\n\n\n\n\nWarning\n\n\n\nA complete statistic need not be sufficient: the constant “statistic” \\(T(X) \\equiv 0\\) is complete in any model. In general, to show that \\(T(X)\\) is complete sufficient we must establish both properties.\n\n\n\n\n1.2 Examples\nExample 1 (Laplace location family): Let \\(X_1,\\ldots,X_n \\simiid \\text{Lap}(\\theta)\\) for \\(\\theta \\in \\RR\\), and recall that the vector of order statistics \\(S(X) = (X_{(1)},\\ldots,X_{(n)})\\) is a minimal sufficient statistic. Is \\(S(X)\\) complete?\n\n\n\n\n\n\nExpand to see answer\n\n\n\n\n\nNo, \\(S(X)\\) is not complete.\nBoth the sample median \\(\\text{Med}(S)\\) (as defined in Homework 2 Problem XX) and sample mean \\(\\overline{X}(S)\\) can be calculated using \\(S(X)\\) alone, and both are unbiased estimators for \\(\\theta\\). Hence \\(f(S) = \\text{Med}(S) - \\overline{X}(S)\\) has expectation zero, but is not almost surely equal to zero because the median and mean are not equal to each other.\n\n\n\nExample 1 (Uniform scale family): Let \\(X_1, \\ldots, X_n \\simiid U[0, \\theta]\\), for \\(\\theta &gt; 0\\). We showed previously that the maximum \\(T(X) = X_{(n)}\\) is minimal sufficient. Is it complete?\n\n\n\n\n\n\nExpand to see answer\n\n\n\n\n\nYes, \\(T(X)\\) is complete.\nAs we showed previously, the density of \\(T(X)\\) for \\(t &gt; 0\\) is \\[\np_\\theta(t) =  \\frac{nt^{n-1}}{\\theta^n}\\,\\cdot \\;1\\{t \\leq \\theta\\}.\n\\]\nSuppose we could find \\(f(t)\\) such that \\[\n0 = \\EE_\\theta f(T) = \\frac{n}{\\theta^n}\\int_0^\\theta f(t) t^{n-1} \\,dt, \\quad \\text{ for all } \\theta &gt; 0.\n\\] Dividing the last expression by \\(n/\\theta^n\\) and then differentiating with respect to \\(\\theta\\), we obtain \\[\n0 = f(\\theta) \\theta^{n-1}, \\quad \\text{ for all } \\theta &gt; 0,\n\\] hence \\(f \\equiv 0\\).\n\n\n\n\n\n1.3 Full-rank exponential families\nIn the general case where \\(T(X)\\) can take on infinitely many values, it is hard to show completeness because the space of possible counterexample functions \\(f\\) is infinite-dimensional. But there is an important class of examples where we can quickly verify complete sufficiency, as we see next.\nDefinition: Let \\(\\cP = \\{P_\\eta:\\; \\eta \\in \\Xi\\}\\) be an \\(s\\)-parameter exponential family with densities \\[\np_\\eta(x) = e^{\\eta'T(x) - A(\\eta)} h(x),\n\\] with respect to some carrier measure \\(\\mu\\). Assume further that the sufficient statistic \\(T(X)\\) satisfies no affine constraint: that is, there is no \\(\\alpha \\in \\RR\\) and nonzero \\(\\beta \\in \\RR^s\\) with \\(\\beta'T(x) \\eqPas \\alpha\\).\nIf \\(\\Xi\\) contains an open set we say \\(\\cP\\) is full-rank; otherwise we say it is curved.\n\n\n\n\n\n\nNote\n\n\n\nIf \\(T(X)\\) does satisfy a linear constraint, that means \\(\\cP\\) can be defined equivalently as an \\(r\\)-parameter exponential family for some \\(r &lt; s\\). It may be full-rank or curved depending on the parameter space in a lower-dimensional parameterization.\n\n\nTheorem (Complete sufficiency in full-rank exponential families): If \\(\\cP\\) is a full-rank \\(s\\)-parameter exponential family, then \\(T(X)\\) is complete sufficient.\nThe proof is somewhat technical and uses the uniqueness of moment-generating functions.\n\n\n\n\n\n\nExpand for proof\n\n\n\n\n\n\\(T(X)\\) is sufficient by the factorization theorem, so it remains only to prove completeness.\nAssume without loss of generality that \\(0\\) is in the interior of \\(\\Xi\\); otherwise we can reparameterize. Assume also that \\(\\cP\\) is in canonical form, i.e. \\(T(X) = X\\) and \\(p_\\eta(x) = e^{\\eta'x - A(\\eta)}\\); in general we can always reduce to this case by making a sufficiency reduction and taking \\(P_0^T\\) as the carrier measure.\nAny measurable function \\(f\\) can be decomposed as \\(f(x) = f^+(x) - f^-(x)\\) where \\(f^+\\) and \\(f^-\\) are non-negative measurable functions. If \\(\\EE_\\eta f(X) = \\int (f^+-f^-)p_\\eta \\,d\\mu = 0\\) for all \\(\\eta\\in\\Xi\\), then we have \\[\n\\int e^{\\eta'x} f^+(x) \\,d\\mu(x) = \\int e^{\\eta'x} f^-(x) \\,d\\mu(x), \\quad \\text{ for all } \\eta \\in \\Xi.\n\\tag{1}\\] By assumption, \\(\\Xi\\) contains an open neighborhood that includes \\(0\\) on which both integrals are finite. Let \\(c = \\int f^+(x)\\,d\\mu(x) \\geq 0\\).\nIf \\(c&gt;0\\) then we can define the random variables \\(Y^+\\) and \\(Y^-\\) with probability densities \\(f^+(x)/c\\) and \\(f^-(x)/c\\) respectively; then Equation 1 implies that \\(Y^+\\) and \\(Y^-\\) have equal MGFs in a neighborhood of \\(0\\); hence they have the same distribution and their densities must be a.s. equal to each other. But \\(f^+(x)=f^-(x)\\) only when both are zero, so we have \\(f^+, f^- \\eqmuas 0\\).\n\n\n\nThe next figure shows three cases for exponential families with the same sufficient statistic. The set \\(\\Xi_1\\) indicates the full natural parameter space for a generic 2-parameter exponential family, and (A), (B), and (C) denote parameter spaces for three different subfamilies. The subfamily described by the shaded circle (A) is a full-rank exponential family, because it contains an open set. The subfamily described by the curve (B) is a typical example of a curved family, because it does not contain an open set. The subfamily described by the line segment (C) is technically curved according to the definition above, but we could view it as a full-rank \\(1\\)-parameter exponential family after reparameterizing it.\n\n\n\nThis figure shows three cases:\n\n\n\n\n1.4 Complete sufficient statistics are minimal\nA second convenient property of completeness is that complete sufficient statistics are always minimal sufficient.\nTheorem: If \\(T(X)\\) is complete sufficient for the family \\(\\mathcal{P}\\), then \\(T(X)\\) is minimal sufficient for \\(\\cP\\).\nThe way that we usually use completeness in proofs is to show that two quantities are almost surely equal by showing that they have the same expectation. The next proof is an example.\nProof: Let \\(S(X)\\) represent any minimal sufficient statistic, and define the conditional expectation of \\(T\\) given \\(S\\): \\[\n\\overline{T}(S(X)) = \\EE[T(X) \\mid S(X)].\n\\] Note that this conditional expectation does not depend on the parameter \\(\\theta\\), because \\(S(X)\\) is sufficient, so \\(\\overline{T}\\) is a valid statistic. If we can show that \\(\\overline{T} \\eqas T(X)\\), that means we can calculate \\(T(X)\\) from \\(S(X)\\), so \\(T(X)\\) is also minimal sufficient.\nBecause \\(S(X)\\) is minimal sufficient, we can write it as \\(f(T(X))\\) for some function \\(f\\), and use \\(f\\) to define a function \\(g\\) giving the difference between \\(T\\) and \\(\\overline{T}\\): \\[\ng(t) = t - \\overline{T}(f(t)).\n\\] The expectation of \\(g(T)\\) is always zero, because \\[\n\\begin{aligned}\n\\EE_\\theta\\; g(T(X)) &= \\EE_\\theta\\; T(X) - \\EE_\\theta\\; \\overline{T}(S(X)) \\\\[5pt]\n&= \\EE_\\theta\\; T(X) - \\EE_\\theta\\left[\\EE[T(X) \\mid S(X)]\\right]\\\\\n&= 0.\n\\end{aligned}\n\\] As a result, \\(g(T) \\eqas 0\\) and hence \\(T \\eqas \\overline{T}\\), as desired."
  },
  {
    "objectID": "reader/completeness.html#ancillarity",
    "href": "reader/completeness.html#ancillarity",
    "title": "Completeness, Ancillarity, and Basu’s Theorem",
    "section": "2 Ancillarity",
    "text": "2 Ancillarity\nWe’ve spent a lot of time discussing sufficient statistics, which are statistics that carry all of the information about \\(\\theta\\). Our next definition describes a type of statistic that carry no information about the parameter.\nDefinition: We say \\(V(X)\\) is ancillary for the model \\(\\cP = \\{P_\\theta: \\theta \\in \\Theta\\}\\) if its distribution does not depend on \\(\\theta\\).\nJust as the sufficiency principle tells us that our inference procedures should depend only on information from sufficient statistics, an analogous principle suggests in effect that procedures should depend as little as possible on ancillary statistics. Specifically, it recommends treating \\(V(X)\\) as a fixed value and evaluating the rest of the data set according to its distribution conditional on \\(V(X)\\):\nConditionality Principle: If \\(V(X)\\) is ancillary, then all inference should be conditional on \\(V(X)\\).\nIt may not be immediately clear why conditioning on \\(V(X)\\) “removes” it from the problem, but we will return to the idea of conditional inference during our unit on hypothesis testing and interval estimation, where it will play an important role."
  },
  {
    "objectID": "reader/completeness.html#basus-theorem",
    "href": "reader/completeness.html#basus-theorem",
    "title": "Completeness, Ancillarity, and Basu’s Theorem",
    "section": "3 Basu’s Theorem",
    "text": "3 Basu’s Theorem\nBasu’s Theorem gives us a simple way to prove that statistics are independent of one another using the definitions introduced above.\nTheorem (Basu): If \\(T(X)\\) is complete sufficient and \\(V(X)\\) ancillary for the model \\(\\cP\\), then \\(V(X) \\indep T(X)\\) for all \\(\\theta \\in \\Theta\\).\nAgain, for this proof our strategy will be to show that two quantities are almost surely equal to each other, by showing that they have the same expectation for all \\(\\theta\\).\nProof: Define the following two quantities representing the marginal and conditional probabilities that \\(V\\) falls into a generic set \\(A\\). \\[\n\\begin{aligned}\np_A &= \\PP(V \\in A)\\\\[5pt]\nq_A(T(X)) &= \\PP(V \\in A \\mid T(X))\n\\end{aligned}\n\\] Note that \\(p_A\\) does not depend on \\(\\theta\\) by ancillarity of \\(V\\), while \\(q_A\\) does not depend on \\(\\theta\\) by sufficiency of \\(T\\).\nThe expectation of their difference is \\[\n\\EE_\\theta\\left[q_A(T) - p_A\\right] = p_A - p_A = 0, \\quad \\text{ for all } \\theta.\n\\] By completeness of \\(T\\), this implies that \\(q_A(T) \\eqas p_A\\): the conditional probability equals the marginal probability. Hence, for any \\(B\\), we have \\[\n\\begin{aligned}\n\\PP_\\theta(V \\in A, T \\in B) &= \\int q_A(t) 1\\{t \\in B\\}\\,dP_\\theta^T(t)\\\\\n&= \\int p_A 1\\{t \\in B\\}\\,dP_\\theta^T(t)\\\\\n&= \\PP_\\theta(V \\in A) \\PP_\\theta(T \\in B).\n\\end{aligned}\n\\]\n\n3.1 Using Basu’s Theorem\nBasu’s Theorem can be helpful in proving independence. To use it, remember that the hypotheses of the theorem (sufficiency, completeness, and ancillarity) are all defined with respect to a family \\(\\cP\\). The conclusion, however, is defined with respect to individual distributions. As a result, when we apply the theorem we can often benefit from being a little clever about how to define \\(\\cP\\). The following example should make this clear:\nExample (Independence of sample mean and sample variance for Gaussian): Assume \\(X_1,\\ldots,X_n \\simiid \\cN(\\mu, \\sigma^2)\\) for \\(\\mu \\in \\RR\\) and \\(\\sigma^2 &gt; 0\\). Define the sample mean and sample variance as \\[\n\\begin{aligned}\n\\overline{X} &= \\frac{1}{n}\\sum_{i=1}^n X_i\\\\[5pt]\nS^2 &= \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\overline{X})^2\n\\end{aligned}\n\\] We would like to show \\(\\overline{X} \\indep S^2\\).\nInitially the approach of applying Basu’s Theorem appears hopeless because, in the model with \\(\\mu\\) and \\(\\sigma^2\\) unknown, neither of these two statistics is ancillary or sufficient. However, we can nevertheless apply Basu’s Theorem if we are just a bit more clever:\n\n\n\n\n\n\nExpand for answer\n\n\n\n\n\nConsider the model \\(\\cP\\) with known \\(\\sigma^2 &gt; 0\\) and unknown \\(\\mu\\in \\RR\\). This \\(\\cP\\) is a one-parameter full-rank exponential family with complete sufficient statistic \\(\\overline{X}\\). Moreover, \\(S^2\\) is ancillary, since we can write \\[\nS^2 = \\sum_{i=1}^n (Z_i - \\overline{Z})^2, \\quad \\text{ for } Z_i = X_i - \\mu.\n\\] Because the distribution of \\(Z_1,\\ldots,Z_n \\simiid N(0,\\sigma^2)\\) is known, it follows that the distribution of \\(S^2\\) is known as well (specifically, \\(S^2/\\sigma^2\\) is a \\(\\chi^2\\) random variable with \\(n-1\\) degrees of freedom). Since \\(\\mu\\) is the only unknown parameter, \\(S^2\\) is therefore ancillary in \\(\\cP\\). Applying Basu’s theorem, we have \\(\\overline{X} \\indep S^2\\) for any \\(\\mu \\in \\RR\\). But \\(\\sigma^2\\) was arbitrary, so we have the result for all \\(\\mu\\) and \\(\\sigma^2\\)."
  },
  {
    "objectID": "reader/sufficiency.html",
    "href": "reader/sufficiency.html",
    "title": "Sufficiency",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/sufficiency.html#sufficiency",
    "href": "reader/sufficiency.html#sufficiency",
    "title": "Sufficiency",
    "section": "Sufficiency",
    "text": "Sufficiency\nSufficiency is a central concept in statistics that allows us to focus on the essential aspects of the data set while ignoring details that are irrelevant to the inference problem. If \\(X\\sim P_\\theta\\) represents the entire data set, drawn from a model \\(\\cP = \\{P_\\theta:\\; \\theta \\in \\Theta\\}\\), then this lecture will concern the idea of a sufficient statistic \\(T(X)\\) that carries all of the information in the data that can help us learn about \\(\\theta\\).\nA statistic \\(T(X)\\) is any random variable which is a function of the data \\(X\\), and which does not depend on the unknown parameter \\(\\theta\\). We say the statistic \\(T(X)\\) is sufficient for the model \\(\\cP\\) if \\(P_\\theta(X \\mid T)\\) does not depend on \\(\\theta\\). This lecture will be devoted to interpreting this definition and giving examples.\nExample (Independent Bernoulli sequence): We introduced the binomial example from Lecture 2 by telling a story about an investigator who flips a biased coin \\(n\\) times and records the total number of heads, which has a binomial distribution. All of the estimators we considered were functions only of the (binomially-distributed) count of heads.\nBut if the investigator had actually performed this experiment, they would have observed more than just the total number of heads: they would have observed the entire sequence of \\(n\\) heads and tails. If we let \\(X_i\\) denote a binary indicator of whether the \\(i\\)th throw is heads, for \\(i=1,\\ldots,n\\), then we have assumed that these indicators are i.i.d. Bernoulli random variables:\n\\[\nX_1,\\ldots,X_n \\simiid \\text{Bern}(\\theta).\n\\]\nLet \\(T(X) = \\sum_i X_i \\sim \\text{Binom}(n,\\theta)\\) denote the summary statistic that we previously used to represent the entire data set. It is undeniable that we have lost some information by only recording \\(T(X)\\) instead of the entire sequence \\(X = (X_1,\\ldots,X_n)\\). As a result, we might wonder whether we could have improved the estimator by considering all functions of \\(X\\), not just functions of \\(T(X)\\).\nThe answer is that, no, we did not really lose anything by summarizing the data by \\(T(X)\\) because \\(T(X)\\) is sufficient. The joint pmf of the data set \\(X \\in \\{0,1\\}^n\\) (i.e., the density wrt the counting measure on \\(\\{0,1\\}^n\\)) is\n\\[\np_\\theta(x) = \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i} = \\theta^{\\sum_i x_i}(1-\\theta)^{n-\\sum_i x_i}.\n\\]\nNote that this pmf depends only on \\(T(x)\\): it assigns probability \\(\\theta^t (1-\\theta)^{n-t}\\) to every sequence with \\(T(X)=t\\) total heads. As a result, the conditional distribution given \\(T(X)=t\\) should be uniform on all of the \\(\\binom{n}{t}\\) sequences with \\(t\\) heads. We can confirm this by calculating the conditional pmf directly:\n\\[\n\\begin{aligned}\n\\PP_\\theta(X = x \\mid T(X) = t)\n&= \\frac{\\PP_\\theta(X=x, \\sum_i X_i = t)}{\\PP_\\theta(T(X) = t)} \\\\[7pt]\n&= \\frac{\\theta^t (1-\\theta)^{n-t}1\\{\\sum_i x_i = t\\}}{\\theta^t(1-\\theta)^{n-t}\\binom{n}{t}}\\\\[5pt]\n&= \\binom{n}{t}^{-1}1\\{T(x) = t\\}.\n\\end{aligned}\n\\]\nSince the conditional distribution does not depend on \\(\\theta\\), \\(T(X)\\) is sufficient for the model \\(\\cP\\).\nAs we will see next, we didn’t really need to go to the trouble of calculating the conditional distribution. Once we noticed that the density depends on \\(x\\) only through \\(T(x)\\), we could have concluded that \\(T(X)\\) was sufficient."
  },
  {
    "objectID": "reader/sufficiency.html#factorization-theorem",
    "href": "reader/sufficiency.html#factorization-theorem",
    "title": "Sufficiency",
    "section": "Factorization theorem",
    "text": "Factorization theorem\nThe easiest way to verify that a statistic is sufficient is to show that the density \\(p_\\theta\\) factorizes into a part that involves only \\(\\theta\\) and \\(T(x)\\), and a part that involves only \\(h(x)\\).\nFactorization Theorem: Let \\(\\cP\\) be a model having densities \\(p_\\theta(x)\\) with respect to a common dominating measure \\(\\mu\\). Then \\(T(X)\\) is sufficient for \\(\\cP\\) if and only if there exist non-negative functions \\(g_\\theta\\) and \\(h\\) for which\n\\[\np_\\theta(x) = g_\\theta(T(x)) h(x),\n\\]\nfor almost every \\(x\\) under \\(\\mu\\).\nThe “almost every \\(x\\)” qualification means that\n\\[\n\\mu\\left(\\{x:\\; p_\\theta(x) \\neq g_\\theta(T(x))h(x)\\}\\right) = 0.\n\\]\nIt is needed to avoid counterexamples with continuous distributions where we could arbitrarily change the value of \\(p_{\\theta_0}(x_0)\\) for a single \\(\\theta_0\\) and \\(x_0\\) without actually changing any of the distributions.\nProof: The proof is easiest in the discrete case, so we don’t have to deal with conditioning on measure-zero events.\nFirst, assume that there exists a factorization \\(p_\\theta(x) = g_\\theta(T(x)) h(x)\\). Then we\n\\[\n\\begin{aligned}\n\\PP_\\theta(X = x \\mid T(X) = t)\n&= \\frac{p_\\theta(x)1\\{T(x) = t\\}}{\\displaystyle\\sum_{z:\\;T(z) = t} p_\\theta(z)}\\\\[7pt]\n&= \\frac{g_\\theta(t) h(x) 1\\{T(x) = t\\}}{g_\\theta(t)\\displaystyle\\sum_{z:\\;T(z) = t} h(z)}\\\\[7pt]\n&= \\frac{h(x) 1\\{T(x) = t\\}}{\\displaystyle\\sum_{z:\\;T(z) = t} h(z)},\n\\end{aligned}\n\\]\nwhich we see does not depend on \\(\\theta\\).\nNext consider the opposite direction. If \\(T(X)\\) is sufficient, then we can construct a factorization by writing\n. First, define\n\\[\ng_\\theta(t) = \\PP_\\theta(T(X)=t) = \\sum_{x:\\;T(x) = t} p_\\theta(x).\n\\]\nThen, let\n\\[\nh(x) = \\PP(X = x \\mid T(X) = T(x)),\n\\]\nwhich does not depend on \\(\\theta\\) by sufficiency. Then we have\n\\[\n\\PP_\\theta(X = x) = \\PP_\\theta(T(X) = T(x)) \\;\\cdot\\;\\PP_\\theta(X = x \\mid T(X) = T(x)) = g_\\theta(T(x)) h(x)\n\\]"
  },
  {
    "objectID": "reader/sufficiency.html#sufficient-statistics-in-exponential-families",
    "href": "reader/sufficiency.html#sufficient-statistics-in-exponential-families",
    "title": "Sufficiency",
    "section": "Sufficient statistics in exponential families",
    "text": "Sufficient statistics in exponential families"
  },
  {
    "objectID": "reader/models.html",
    "href": "reader/models.html",
    "title": "Statistical decision theory",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/models.html#statistical-models",
    "href": "reader/models.html#statistical-models",
    "title": "Statistical decision theory",
    "section": "Statistical models",
    "text": "Statistical models\nUntil now, we have been discussing the topic of probability. Roughly speaking, in probability we fully specify the distribution of some random variables, and then ask what we can say about the distribution. For example, given a complete description of the rules for generating a random walk, we might ask how long, in expectation, it will take to reach a certain threshold. This is an essentially deductive exercise: while the mathematics might be very hard, the questions we ask generally have unambiguous answers.\nIn statistics, we do essentially the opposite: beginning with the data — the Latin word for “given” — we work backwards to draw inferences about the data-generating distribution. This is an inductive exercise, for which the answers will inevitably be more ambiguous.\nWe will generally use the letter \\(X\\) to denote the full data set, which we assume is drawn randomly from some unknown distribution \\(P\\) over the sample space \\(\\cX\\). Let \\(\\cP\\) denote a family of candidate probability distributions, called the statistical model. We assume the analyst knows that one of the elements of \\(\\cP\\) is the true data-generating distribution \\(P\\), but does not know which one. The set \\(\\cX\\) in which \\(X\\) is\nExample (Binomial): As a simple example, we can imagine an analyst who flips a biased coin \\(n\\) times, getting \\(X\\) heads and \\(n-X\\) tails. If we assume the successive flips are independent, and each has a common probability \\(\\theta\\) of landing heads, we can write the model as\n\\[\nX \\sim \\text{Binom}(n, \\theta), \\quad \\text{ for some } \\theta \\in [0,1].\n\\]\nFormally, we could say the family of distributions is \\(\\cP = \\{\\text{Binom}(n, \\theta):\\; \\theta \\in [0,1]\\}\\), a set of distributions indexed by the real parameter \\(\\theta\\).\nNote that in the previous example, the integer \\(n\\) is another important variable in the problem, but we implicitly assumed that it was “known” by the analyst, meaning that it is the same for all \\(P \\in \\cP\\). The parameter \\(\\theta\\), by contrast, is termed “unknown” in the sense that it varies over the family \\(\\cP\\).\n\nParametric vs nonparametric models\nMany of the models we will consider in this class are parametric, typically meaning that they are indexed by finitely many real parameters. That is, we have \\(\\cP = \\{P_\\theta:\\; \\theta \\in \\Theta\\}\\), typically for some parameter space \\(\\Theta \\subseteq \\RR^d\\). Then \\(\\theta\\) is called the parameter or parameter vector.\nIn other models, there is no natural way to index \\(\\cP\\) using \\(d\\) real numbers. We call these nonparametric models. Sometimes excited authors referred to their methods as “assumption-free,” but essentially all nonparametric models still make some assumptions about the data distribution. For example, we might assume independence between multiple observations, or shape constraints such as unimodality.\nExample (Nonparameric model): Suppose we observe an i.i.d. sample of size \\(n\\) from a distribution \\(P\\) on the real line. Even if we do not want to assume anything about \\(P\\), the i.i.d. assumption will play an important role in the analysis. We might write this model as\n\\[\nX_1,\\ldots,X_n \\simiid P, \\quad \\text{ for some distribution } P \\text{ on } \\RR.\n\\]\nFormally, if \\(X = (X_1,\\ldots,X_n)\\), we can write the family as \\(\\cP = \\{P^n:\\; P \\text{ is a distribution on } \\RR\\}\\), where \\(P^n\\) represents the \\(n\\)-fold product of \\(P\\) on \\(\\RR^n\\).\nNotation: Much of what we will learn in this course applies to parametric and nonparametric models alike, and indeed there is no crisp demarcation between parametric and nonparametric models in practice. It will often be convenient to use notation \\(\\cP = \\{P_\\theta :\\; \\theta \\in \\Theta\\}\\), without specifying what kind of set \\(\\Theta\\) is; in particular there is nothing to stop \\(\\theta\\) from being an infinite-dimensional object such as a density function. We can work in this notation without any loss of generality, since we could always take \\(\\theta = P\\) and \\(\\Theta = \\cP\\).\n\n\nBayesian vs Frequentist inference\nThus far we have assumed the data \\(X\\) follows a distribution \\(P_\\theta\\), for some unknown parameter \\(\\theta\\) which can be any arbitrary member of the set \\(\\Theta\\). In some contexts we will introduce an additional assumption we can call the Bayesian assumption: that \\(\\theta\\) is itself random, drawn from some known distribution \\(\\Lambda\\) that we call the prior.\nA major advantage of this assumption is that it reduces the problem of inference about \\(\\theta\\) to simply calculating the conditional distribution of \\(\\theta\\) given \\(X\\).\nThe philosophical ramifications of this assumption, as well as its practical advantages and disadvantages, will be a major theme later in the course, but for now we will simply say it is an assumption we are sometimes, but not always, willing to make. From a mathematical perspective, it makes no more or less sense to assume \\(\\theta\\) is random than it does to assume \\(\\theta\\) is fixed and unknown.\nFor the remainder of this lecture, and until our unit on Bayesian inference, we will refrain from making this assumption, instead regarding \\(\\theta\\) as taking an arbitrary fixed value in \\(\\Theta\\)."
  },
  {
    "objectID": "reader/models.html#estimation-in-statistical-models",
    "href": "reader/models.html#estimation-in-statistical-models",
    "title": "Statistical decision theory",
    "section": "Estimation in statistical models",
    "text": "Estimation in statistical models\nHaving observed \\(X \\sim P_\\theta\\), an unknown distribution in the model \\(\\cP = \\{P_\\theta:\\; \\theta \\in \\Theta\\}\\), we will be interested in learning something about \\(\\theta\\). In estimation, we guess the value of some quantity of interest \\(g(\\theta)\\), called the estimand. Our guess is called the estimate \\(\\delta(X)\\), calculated based on the data. The method \\(\\delta(\\cdot)\\) that we use to calculate the estimate is called the estimator.\nExample (Binomial, continued): Returning to our binomial example from above, we may want to estimate \\(g(\\theta) = \\theta\\), the probability of the coin landing heads. A natural estimator is \\(\\delta_0(X) = X/n\\), the fraction of coins landing heads in any given trial. One favorable property of this estimator is that it is unbiased, meaning that \\(\\EE_\\theta \\delta_0(X) = g(\\theta)\\), for all \\(\\theta \\in \\Theta\\).\nThere are many potential estimators for any given problem, so our goal will generally be to find a good estimator. To evaluate and compare estimators, we must have a way of evaluating how successful an estimator is in any given realization of the data. To this end we introduce the loss function \\(L(\\theta, d)\\), which measures how bad it is to guess that \\(g(\\theta) = d\\) when \\(\\theta\\) is the true parameter value. Typically loss functions are non-negative, with \\(L(\\theta, d) = 0\\) if and only if \\(g(\\theta) = d\\) (no loss from a perfect guess) but this is not required.\nIn any given problem, we should ideally choose the loss that best measures our own true (dis)utility function, but in practice people fall back on simple defaults. One loss function that is especially popular for its mathematical convenience is the squared-error loss, defined by \\(L(\\theta, d) = (d-g(\\theta))^2\\).\nWhereas the loss function measures how (un)successful an estimator is in one realization of the data, we would really like to evaluate an estimator’s performance over the whole range of possible data sets \\(X\\) that we might observe. This is measured by the risk function, defined as\n\\[\nR(\\theta; \\delta(\\cdot)) = \\EE_\\theta [\\, L(\\theta, \\delta(X)) \\,] = \\int L(\\theta, \\delta(x)) \\td P_\\theta(x)\n\\]\nRemark on notation: The subscript in the previous expression tells us which of our candidate probability distributions to use in evaluating the expectation. In some other fields, people may use the subscript to indicate “what randomness to integrate over,” with the implication that any random variable that does not appear in the subscript should be held fixed. In our course, it should generally be assumed that any expectation or probability is integrating over the joint distribution of the entire data set; if we want to hold something fixed we will condition on it. Recall that, for now, the parameter \\(\\theta\\) is fixed unless otherwise specified.\nThe semicolon in the risk function is meant to indicate we are viewing it primarily as a function of \\(\\theta\\). That is, we should think of and estimator \\(\\delta\\) as having a risk function \\(R(\\theta)\\), and the second input in \\(R(\\theta; \\delta)\\) is telling us which estimator’s risk function to evaluate at \\(\\theta\\).\nThe risk for the squared-error loss is called the mean squared error (MSE):\n\\[\n\\textrm{MSE}(\\theta; \\delta) = \\EE_\\theta\\left[\\,(\\delta(X) - g(\\theta))^2\\,\\right]\n\\]\nExample (Binomial, continued): To calculate the MSE of our estimator \\(\\delta_0 = X/n\\), note that \\(\\EE_\\theta[X/n] = \\theta\\) (the estimator is unbiased). As a result, we have\n\\[\n\\begin{aligned}\n\\textrm{MSE}(\\theta; \\delta_0) &= \\EE_\\theta\\left[ \\left(\\frac{X}{n} - \\theta\\right)^2\\right] \\\\[7pt]\n&= \\text{Var}_\\theta(X/n)\\\\[3pt]\n&= \\frac{1}{n}\\theta(1-\\theta)\n\\end{aligned}\n\\]\nOne reason why we might consider estimators other than \\(\\delta_0\\) is that, if \\(n\\) is small, our estimate could be quite noisy. As an extreme example, if \\(n=1\\) we will always estimate either \\(\\theta = 0\\) or \\(\\theta = 1\\), both of which would be extreme conclusions to draw after a single trial. One simple way of reducing the variance is to pretend that we flipped the coin an additional \\(m\\) times resulting in \\(a\\) heads and \\(m-a\\) tails. This will tend to shade our estimate toward \\(a/m\\), reducing the risk if \\(\\theta = a/m\\) but possibly increasing the risk for other values of \\(\\theta\\).\n\nWe show the risk function for several alternative estimators of this form below:\n\\[\n\\delta_1(X) = \\frac{X + 1}{n + 2}, \\quad \\delta_2(X) = \\frac{X + 2}{n + 4}, \\quad \\delta_3(X) = \\frac{X + 1}{n}\n\\]\nThe last estimator, \\(\\delta_3\\), is another example where we add something to \\(X\\) in the numerator but nothing \\(n\\) in the denominator.\n\nlibrary(RColorBrewer)\n\nn = 16\n\n## risk function of estimator (X + synth.heads) / (n + synth.flips)\nbinom.mse &lt;- function(theta, n, synth.heads, synth.flips) {\n  binom.var &lt;- theta * (1 - theta) * n / (n + synth.flips)^2\n  binom.bias &lt;- (n * theta + synth.heads) / (n + synth.flips) - theta\n  return(binom.var + binom.bias^2)\n}\n\n\npalette &lt;- c(\"black\",brewer.pal(4, \"Set1\"))\ncurve(binom.mse(x, n, 0, 0), from=0, to=1, ylim=c(0,0.35/n), lwd=2, col=palette[1],\n      main = \"Mean squared error for binomial estimators (n=16)\", \n      ylab=expression(MSE(theta)), \n      xlab=expression(theta))\ncurve(binom.mse(x, n, 1, 2), add=TRUE, col=palette[2], lwd=2) \ncurve(binom.mse(x, n, 2, 4), add=TRUE, col=palette[3], lwd=2) \ncurve(binom.mse(x, n, 1, 0), add=TRUE, col=palette[4], lwd=2) \nlegend(\"topright\", col=palette, lwd=2,bty=\"n\",\n       legend=c(expression(delta[0]), expression(delta[1]), expression(delta[2]), expression(delta[3])))\n\n\n\n\nIf we look at the vertical axis, the MSE may appear to be very small, especially considering we only have 16 flips. But recall that an MSE of \\(0.01\\) means that we are typically missing by about \\(0.1\\), while estimating a parameter that is between \\(0\\) and \\(1\\).\n\nComparing estimators\nIn comparing the risk functions of these estimators, we can notice a few things. As expected, both \\(\\delta_1\\) and \\(\\delta_2\\) outperform \\(\\delta_0\\) for values of \\(\\theta\\) close to \\(1/2\\), but underperform for more extreme values of \\(\\theta\\). The estimator \\(\\delta_3\\), however, performs worse than \\(\\delta_0\\) throughout the entire parameter space; this is because we have added bias without doing anything to reduce the variance. While it is difficult to choose between the other three estimators, we can at least rule out \\(\\delta_3\\) on the grounds that we have no reason to ever prefer it over \\(\\delta_0\\).\nFormally, we say an estimator \\(\\delta\\) is inadmissible if there is some other estimator \\(\\delta^*\\) for which\n\n\\(R(\\theta; \\delta^*) \\leq R(\\theta; \\delta)\\) for all \\(\\theta\\in\\Theta\\), and\n\\(R(\\theta; \\delta^*) &lt; R(\\theta; \\delta)\\) for some \\(\\theta\\in\\Theta\\).\n\nIn this case we say \\(\\delta^*\\) strictly dominates \\(\\delta\\); more generally we can say \\(\\delta^*\\) *dominates* \\(\\delta\\) if we only have (1). An estimator is admissible if it is not inadmissible. We can see from our plot that \\(\\delta_3\\) is inadmissible because \\(\\delta_0\\) strictly dominates it.\nComparing the other three estimators is more difficult, however, because no one of them dominates any other. In most estimation problems, including this one, we can never hope to come up with an estimator that uniformly attains the smallest risk among all estimators. That is because, for example, we can always choose the constant estimator \\(\\delta(X) \\equiv 1/2\\) that simply ignores the data and always guesses that \\(\\theta = 1/2\\). This estimator may perform poorly for other values of \\(\\theta\\), but it is the only estimator that has exactly zero MSE for \\(\\theta = 1/2\\).\nIf we cannot hope to minimize the risk for every value of \\(\\theta\\) simultaneously then we must come up with some other way to resolve the inherent ambiguity in comparing all of the many estimators that we must choose among.\nIn our unit on estimation, we will consider two main strategies for resolving this ambiguity.\n\n\nStrategy 1: Summarizing the risk function by a scalar\nIf we can find a way to summarize the risk function for each estimator by a single real number that we want to minimize, then we can find an estimator that is optimal in this summary sense. The two main ways to summarize the risk are to examine the average-case risk and the worst-case risk.\n\nAverage-case risk (Bayes estimation)\nThe first option is to minimize some (weighted) average of the risk function over the parameter space \\(\\Theta\\) :\n\\[\n\\minz_{\\delta(\\cdot)} \\int_\\theta R(\\theta; \\delta)\\td \\Lambda(\\theta)\n\\]\nThe average is taken with respect to some measure \\(\\Lambda\\) of our choosing. If \\(\\Lambda(\\Theta) &lt; \\infty\\) we can assume without loss of generality that \\(\\Lambda\\) is a probability measure, since we could always normalize it without changing the minimization problem. Then, this average is simply the estimator’s expected risk, called the Bayes risk, or equivalently the expected loss averaging over the joint distribution of \\(\\theta\\) and \\(X\\). An estimator that minimizes the Bayes risk is called a Bayes estimator.\nIn the binomial problem above, \\(\\delta_1(X) = \\frac{X + 1}{n + 2}\\) is a Bayes estimator that minimizes the average-case risk with respect to the Lebesgue measure on \\(\\Theta = [0,1]\\). \\(\\delta_2(X) = \\frac{X+2}{n+4}\\) is also a Bayes estimator with respect to a different prior, specifically the \\(\\textrm{Beta}(2,2)\\) distribution. We will show this later.\nNote that minimizing the average-case risk may be a natural thing to do regardless of whether we “really believe” that \\(\\theta \\sim \\Lambda\\). Hence Bayes estimators are well-motivated even from a purely frequentist perspective; using them does not have to imply one has any specific position on the philosophical interpretation of probability.\nIf \\(\\Lambda(\\Theta) = \\infty\\) then we call \\(\\Lambda\\) an improper prior, and we can no longer interpret the corresponding Bayes risk as an expectation. But, as we will see, working with improper priors can sometimes be convenient and often leads to good estimators in practice.\n\n\nWorst-case risk (Minimax estimation)\nIf we are reluctant to average over the parameter space, we can instead seek to minimize the worst-case risk over the entire parameter space:\n\\[\n\\minz_{\\delta(\\cdot)} \\sup_{\\theta\\in\\Theta} R(\\theta; \\delta)\n\\]\nThis minimization problem has a game-theoretic interpretation if we imagine that, after we choose our estimator, Nature will adversarially choose the least favorable parameter value.\nAs we will see, minimax estimation is closely related to Bayes estimation and the minimax estimator is commonly a Bayes estimator.\nThe minimax perspective pushes us to choose estimators with flat risk functions, and indeed \\(\\delta_2(X) = \\frac{X + 2}{X + 4}\\) is the minimax estimator when \\(n = 16\\).\n\n\n\nStrategy 2: Restricting the choice of estimators\nThe second main strategy for resolving ambiguity is to restrict ourselves to choose an estimator that satisfies some additional side constraint.\n\nUnbiased estimation\nOne property we might want to demand of an estimator is that it be unbiased, meaning that \\(\\EE_\\theta [\\delta_0(X)] = g(\\theta)\\), for all \\(\\theta\\in\\Theta\\). This rules out, for example, estimators that ignore the data and always guess the same value.\nAs we will see, once we requiring unbiasedness there will often be a clear winner among all remaining estimators under consideration, called the uniformly minimum variance unbiased (UMVU) estimator, which uniformly minimizes the risk for any convex loss function.\nOf the four estimators we considered above, only \\(\\delta_0(X) = X/n\\) is unbiased, and it is indeed the UMVU for this problem."
  },
  {
    "objectID": "reader/maximum-likelihood.html",
    "href": "reader/maximum-likelihood.html",
    "title": "Maximum Likelihood Estimation: Theory and Asymptotics",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/maximum-likelihood.html#maximum-likelihood-estimation",
    "href": "reader/maximum-likelihood.html#maximum-likelihood-estimation",
    "title": "Maximum Likelihood Estimation: Theory and Asymptotics",
    "section": "1 Maximum Likelihood Estimation",
    "text": "1 Maximum Likelihood Estimation\nFor a generic dominated family \\(\\cP = \\{P_\\theta: \\theta \\in \\Theta\\}\\) with densities \\(f_\\theta\\), a simple estimator for \\(\\theta\\) is:\n\\[\\hat{\\theta}_{\\text{MLE}}(X) = \\arg\\max_{\\theta \\in \\Theta} p_\\theta(X) = \\arg\\max_{\\theta \\in \\Theta} \\prod_{i=1}^n f_\\theta(X_i) = \\arg\\max_{\\theta \\in \\Theta} \\ell_n(\\theta; X)\\]\nRemarks: 1. \\(\\arg\\max\\) may not exist, be unique, or be computable 2. Doesn’t depend on parameterization or base measure; MLE for \\(g(\\theta)\\) is \\(g(\\hat{\\theta}_{\\text{MLE}})\\)\n\n1.1 Example: Exponential Family\n\\[\\ell(\\eta; X) = \\eta^T T(X) - A(\\eta) + \\log h(X)\\]\n\\(T(\\bar{X}) = \\mathbb{E}_\\eta[T(X)]\\) if such \\(\\eta\\) exists\nBecause \\(\\ell''(\\eta; X) = -\\text{Var}_\\eta[T(X)]\\) is negative definite unless \\(\\eta \\to T(\\eta)\\) constant, in which case param redundant\nAt most 1 solution exists\nLet \\(m(X) = \\mathbb{E}_\\eta[T(X)] = \\nabla A(\\eta)\\)\n\n\n1.2 Example: Normal Distribution\n\\(X_i \\sim \\text{iid } N(\\theta, \\sigma^2)\\), \\(h(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-x^2/(2\\sigma^2)}\\), \\(\\eta \\in \\mathbb{R}\\)\n\\(T(X) = \\frac{X}{\\sigma^2}\\), \\(A(\\eta) = \\frac{\\eta^2}{2\\sigma^2}\\)\nAssume \\(\\eta = \\theta/\\sigma^2\\), \\(\\sigma^2\\) known\n\\(m(\\eta) = \\eta\\sigma^2 = \\theta\\), \\(m^{-1}(\\theta) = \\theta/\\sigma^2\\)\nConsistency: \\(\\bar{X} \\xrightarrow{p} \\theta\\) (LLN)\nCts mapping: \\(\\hat{\\eta} = m^{-1}(\\bar{X}) \\xrightarrow{p} \\theta/\\sigma^2\\)\nSince \\(\\sqrt{n}(\\bar{X} - \\theta) \\xrightarrow{d} N(0, \\text{Var}_\\eta[T(X)])\\):\n\\(N(0, \\sigma^2)\\)\nRecall: \\(J(\\eta) = \\text{Var}_\\eta[T(X)]\\)\nDelta method: \\(\\sqrt{n}(\\hat{\\eta} - \\eta) \\xrightarrow{d} N(0, [m^{-1'}(\\theta)]^2 \\sigma^2)\\)\n\\(N(0, 1/\\sigma^2)\\)\nRecall: \\(J(\\eta) = \\text{Var}_\\eta[T(X)] = \\sigma^2\\)\n\\(N(0, J^{-1})\\)\nAsymptotically unbiased Gaussian, achieves CRLB\n\n\n1.3 Example: Poisson Distribution\n\\(X_i \\sim \\text{iid Poisson}(\\theta)\\), \\(\\eta = \\log \\theta\\)\n\\(T(X) = X\\), \\(\\mathbb{E}[X] = \\theta\\), \\(N(0, \\theta)\\)\n\\(\\hat{\\eta}_n = \\log \\bar{X}\\), \\(\\sqrt{n}(\\log \\bar{X} - \\log \\theta) \\xrightarrow{d} N(0, \\theta^{-1})\\) (Delta method)\n\\(N(0, \\theta^{-1})\\)\nBut for finite \\(n\\), \\(\\mathbb{P}(\\bar{X} = 0) = \\mathbb{P}(X_1 = 0)^n = e^{-n\\theta} &gt; 0\\)\nMLE can have embarrassing finite sample performance despite being asymptotically optimal\n\n\n1.4 Proof: Convergence in Distribution with Probability Approaching 1\nIf \\(\\mathbb{P}(B_n) \\to 1\\), \\(X_n \\xrightarrow{d} X\\), \\(Z_n\\) arbitrary, then \\(X_n 1_{B_n} + Z_n 1_{B_n^c} \\xrightarrow{d} X\\)\nProof: \\(\\mathbb{P}(\\|Z_n 1_{B_n^c}\\| &gt; \\epsilon) \\leq \\mathbb{P}(B_n^c) \\to 0\\), so \\(Z_n 1_{B_n^c} \\xrightarrow{p} 0\\) Also, \\(1_{B_n} \\xrightarrow{p} 1\\), apply Slutsky\nAny zany behavior has no effect on convergence in distribution"
  },
  {
    "objectID": "reader/maximum-likelihood.html#asymptotic-efficiency",
    "href": "reader/maximum-likelihood.html#asymptotic-efficiency",
    "title": "Maximum Likelihood Estimation: Theory and Asymptotics",
    "section": "2 Asymptotic Efficiency",
    "text": "2 Asymptotic Efficiency\nIn the exponential family case, generalizes to a much broader class of models\nSetting: \\(X_1, \\ldots, X_n \\stackrel{\\text{iid}}{\\sim} p_\\theta(x)\\), \\(\\theta \\in \\mathbb{R}^d\\)\n\\(p_\\theta\\) smooth in \\(\\theta\\) (e.g., 2 cts integrable derives, can be relaxed)\nLet \\(\\ell_i(\\theta; X) = \\log p_\\theta(X_i)\\), \\(\\ell_n(\\theta; X) = \\sum_{i=1}^n \\ell_i(\\theta; X)\\)\n\\(S_n(\\theta) = \\nabla_\\theta \\ell_n(\\theta; X)\\), \\(J_n(\\theta) = \\text{Var}_\\theta[\\nabla_\\theta \\ell_n(\\theta; X)] = nJ_1(\\theta)\\)\nWe say an estimator is asymptotically efficient if \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\xrightarrow{d} N(0, J_1^{-1}(\\theta))\\)\nDelta method for differentiable estimand \\(g(\\theta)\\):\n\\(\\sqrt{n}(g(\\hat{\\theta}_n) - g(\\theta)) \\xrightarrow{d} N(0, \\nabla g(\\theta)^T J_1^{-1}(\\theta) \\nabla g(\\theta))\\)\nAlso achieves CRLB if \\(\\hat{\\theta}_n\\) does, \\(g\\) diff"
  },
  {
    "objectID": "reader/maximum-likelihood.html#asymptotic-distribution-of-mle",
    "href": "reader/maximum-likelihood.html#asymptotic-distribution-of-mle",
    "title": "Maximum Likelihood Estimation: Theory and Asymptotics",
    "section": "3 Asymptotic Distribution of MLE",
    "text": "3 Asymptotic Distribution of MLE\nUnder mild conditions, \\(\\hat{\\theta}_n\\) is asymptotically Gaussian efficient\nWe will be interested in \\(\\ell_n(\\theta; X)\\) as a function of \\(\\theta\\) Notate true value as \\(\\theta_0\\): \\(X \\sim P_{\\theta_0}\\)\nDerivatives of \\(\\ell_n\\) at \\(\\theta_0\\): \\(S_0\\), \\(S_1\\)\n\\(S_n(\\theta_0; X) = \\sum_{i=1}^n \\nabla \\ell_i(\\theta_0; X_i) \\sim N(0, J_n(\\theta_0))\\)\n\\(\\mathbb{E}[S_n(\\theta; X)] = n\\mathbb{E}[\\nabla \\ell_i(\\theta_0; X_i)] = 0\\)\n\\(\\mathbb{E}[-\\nabla^2 \\ell_n(\\theta; X)] = \\mathbb{E}[\\sum_{i=1}^n -\\nabla^2 \\ell_i(\\theta_0; X_i)] = J_n(\\theta_0)\\)\n\n3.1 Informal Proof\nTaylor expansion between \\(\\theta_0\\), \\(\\hat{\\theta}_n\\):\n\\(S_n(\\hat{\\theta}_n; X) = S_n(\\theta_0; X) + S_n'(\\theta_0; X)(\\hat{\\theta}_n - \\theta_0)\\)\n\\(\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) = J_n^{-1}(\\theta_0) S_n(\\theta_0; X)/\\sqrt{n}\\)\n\\(\\xrightarrow{d} N(0, J_1^{-1}(\\theta_0))\\)\nMore rigorous proof later, but note we need consistency of \\(\\hat{\\theta}_n\\) first to even justify Taylor expansion\n\n\n3.2 Quadratic Approximation\nQuadratic approximation near \\(\\theta_0\\):\n\\(\\ell_n(\\theta) \\approx \\ell_n(\\theta_0) + (\\theta - \\theta_0)^T S_n(\\theta_0) - \\frac{1}{2}(\\theta - \\theta_0)^T J_n(\\theta_0)(\\theta - \\theta_0)\\)\n\\(N(J_n^{-1}(\\theta_0)S_n(\\theta_0), J_n^{-1}(\\theta_0))\\)\nGaussian linear term + Deterministic curvature\n\\(\\ell_n(\\theta) - \\ell_n(\\theta_0) \\approx -\\frac{n}{2}(\\theta - \\hat{\\theta}_n)^T J_1(\\theta_0)(\\theta - \\hat{\\theta}_n) + \\text{const}\\)"
  },
  {
    "objectID": "reader/maximum-likelihood.html#consistency-of-mle",
    "href": "reader/maximum-likelihood.html#consistency-of-mle",
    "title": "Maximum Likelihood Estimation: Theory and Asymptotics",
    "section": "4 Consistency of MLE",
    "text": "4 Consistency of MLE\n\\(X_1, \\ldots, X_n \\stackrel{\\text{iid}}{\\sim} f_{\\theta_0}\\), \\(\\theta_n \\in \\arg\\max_{\\theta \\in \\Theta} \\ell_n(\\theta; X)\\)\nWill be ok if \\(\\theta_n\\) comes close to maximizing \\(\\ell_n\\)\nQuestion: When does \\(\\ell_n \\to \\ell\\)?\nAssume model identifiable: \\(P_\\theta = P_{\\theta_0}\\) for \\(\\theta \\neq \\theta_0\\)\nRecall KL Divergence:\n\\(D(g\\|f) = \\mathbb{E}_g[\\log \\frac{g(X)}{f(X)}] = \\int g(x) \\log \\frac{g(x)}{f(x)} dx\\) (note switch)\n\\(\\log \\frac{g}{f} \\geq 1 - \\frac{f}{g}\\) (strict ineq unless const, i.e., unless \\(f=g\\))\nLet \\(W_i(\\theta) = \\ell_i(\\theta; X_i) - \\ell_i(\\theta_0; X_i)\\), \\(W(\\theta) = \\mathbb{E}_{\\theta_0}[W_i(\\theta)]\\)\nNote: \\(\\theta_0 = \\arg\\max_{\\theta \\in \\Theta} W(\\theta)\\)\n\\(W(\\theta_0) = 0\\)\n\\(W(\\theta) = -\\mathbb{E}_{\\theta_0}[\\log \\frac{f_{\\theta_0}(X)}{f_\\theta(X)}] = -D(f_{\\theta_0}\\|f_\\theta) \\leq 0\\)\n\\(= 0\\) iff \\(\\theta = \\theta_0\\)\nBut not enough: 1. MLE \\(\\hat{\\theta}_n\\) depends on entire function \\(\\ell_n\\) 2. Need uniform convergence in \\(\\theta\\)\n\n4.1 Definition: Compact Convergence\nFor compact \\(K\\), let \\(C(K) = \\{f: K \\to \\mathbb{R} \\text{ cts}\\}\\)\nFor \\(f \\in C(K)\\), let \\(\\|f\\| = \\sup_{x \\in K} |f(x)|\\)\n\\(f_n \\to f\\) in this norm if \\(\\|f_n - f\\| \\to 0\\)\n\n\n4.2 Theorem: LLN for Random Functions\nAssume \\(K\\) compact, \\(W_i, W_2, \\ldots \\in C(K)\\) iid \\(\\mathbb{E}[\\|W_i\\|] &lt; \\infty\\), \\(\\mathbb{E}[W_i(\\theta)] = W(\\theta)\\)\nThen \\(\\bar{W}_n \\in C(K)\\) and \\(\\mathbb{P}(\\|\\bar{W}_n - W\\| &gt; \\epsilon) \\to 0\\)\ni.e., \\(\\ell_n \\to \\ell\\) in \\(\\|\\cdot\\|_\\infty\\) norm\n\n\n4.3 Theorem (Keener 9.4)\nLet \\(G_n, G\\) random functions in \\(K\\) compact 1. \\(G_n \\to g\\) in \\(\\|\\cdot\\|_\\infty\\), some fixed \\(g \\in C(K)\\). Then: a. If \\(t^* \\in K\\) fixed, then \\(G_n(t^*) \\xrightarrow{p} g(t^*)\\) b. If \\(g\\) maximized at unique value \\(t^*\\) and \\(G_n(t_n) = \\max_t G_n(t)\\), then \\(t_n \\xrightarrow{p} t^*\\) 2. If \\(K \\subset \\mathbb{R}\\), \\(g'(t) = 0\\) has unique sol. \\(t^*\\) and \\(t_n\\) solve \\(G_n'(t) = 0\\), then \\(t_n \\xrightarrow{p} t^*\\)\n(Sketch of proof in purple) If \\(G_n'(t_n) = 0\\), get \\(G_n'(t^*) = g'(t^*) + (g'(t_n) - g'(t^*)) + (G_n'(t^*) - g'(t^*))\\) \\(\\to 0 + 0 + 0\\) by assumptions + by cts mapping\nFix \\(\\epsilon &gt; 0\\), let \\(B_\\epsilon = \\{t: \\|t - t^*\\| &lt; \\epsilon\\}\\) Let \\(K_\\epsilon = K \\setminus B_\\epsilon(t^*)\\), \\(K \\setminus B_\\epsilon\\) compact \\(\\delta = g(t^*) - \\max_{t \\in K_\\epsilon} g(t) &gt; 0\\)\nIf \\(t_n \\notin K_\\epsilon\\), then \\(G_n(t_n) &gt; G_n(t^*) - \\frac{\\delta}{2} &gt; g(t^*) - \\delta = \\max_{t \\in K_\\epsilon} g(t)\\)\n\\(\\mathbb{P}(t_n \\notin K_\\epsilon) \\leq \\mathbb{P}(\\|G_n - g\\|_\\infty &gt; \\frac{\\delta}{2}) \\to 0\\)\nAnalogous to \\(\\bar{X}_n \\xrightarrow{p} \\mu\\)\n\n\n4.4 Theorem: Consistency of MLE for Compact \\(\\Theta\\)\n\\(X_1, \\ldots, X_n \\stackrel{\\text{iid}}{\\sim} f_{\\theta_0}\\), \\(\\cP\\) has densities \\(p_\\theta\\), \\(\\theta \\in \\Theta\\)\nAssume: 1. \\(p_\\theta\\) cts in \\(\\theta\\) 2. \\(\\Theta\\) compact 3. \\(\\mathbb{E}_{\\theta_0}[\\sup_\\theta |f_\\theta(X)|/f_{\\theta_0}(X)] &lt; \\infty\\) 4. \\(\\mathbb{E}_{\\theta_0}[\\sup_\\theta |W_i(\\theta)|] &lt; \\infty\\) 5. Model identifiable\nThen \\(\\hat{\\theta}_n \\xrightarrow{p} \\theta_0\\) if $"
  },
  {
    "objectID": "reader/testing-nuisance.html",
    "href": "reader/testing-nuisance.html",
    "title": "Testing with Nuisance Parameters",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/testing-nuisance.html#nuisance-parameters",
    "href": "reader/testing-nuisance.html#nuisance-parameters",
    "title": "Testing with Nuisance Parameters",
    "section": "1 Nuisance Parameters",
    "text": "1 Nuisance Parameters\n\n1.1 Common Setup\nExtra unknown parameters which are not of direct interest:\n\\(\\cP = \\{P_{\\theta, \\lambda}: \\theta \\in \\Theta, \\lambda \\in \\Lambda\\}\\)\n\\(H_0: \\theta \\in \\Theta_0\\) vs \\(H_1: \\theta \\in \\Theta_1\\)\n\n\\(\\theta\\): parameter of interest\n\\(\\lambda\\): nuisance parameter\n\nIssue: \\(\\lambda\\) unknown but might affect type I error or power of a given test\n\n\n1.2 Examples\n\n\\(X_1, \\ldots, X_n \\sim \\text{iid } N(\\mu, \\sigma^2)\\), \\(Y_1, \\ldots, Y_m \\sim \\text{iid } N(\\nu, \\sigma^2)\\) \\(\\mu, \\nu, \\sigma^2\\) unknown \\(H_0: \\mu = \\nu\\) vs \\(H_1: \\mu \\neq \\nu\\) \\(\\theta = \\mu - \\nu\\), \\(\\lambda = (\\mu + \\nu, \\sigma^2)\\) or \\((\\mu, \\sigma^2)\\)\n\\(X \\sim \\text{Binom}(n_1, \\pi_1)\\), \\(X_2 \\sim \\text{Binom}(n_2, \\pi_2)\\) \\(n_1, n_2\\) known (not nuisance parameters) \\(H_0: \\pi_1 = \\pi_2\\) vs \\(H_1: \\pi_1 \\neq \\pi_2\\)\n\\(X \\sim N(\\mu, \\sigma^2)\\), \\(\\theta \\in \\mathbb{R}\\), \\(\\lambda \\in \\mathbb{R}\\), both unknown How to test \\(H_0: \\theta = 0\\) vs \\(H_1: \\theta \\neq 0\\)?\n\n\n\n1.3 Idea: Condition on Sufficient Statistic for \\(\\lambda\\)\nCondition on \\(U(X)\\) to eliminate dependence on \\(\\lambda\\)\n\\[p_{\\theta, \\lambda}(t|u) = \\frac{p_{\\theta, \\lambda}(t, u)}{p_{\\lambda}(u)} = \\frac{e^{\\theta \\cdot t} g_\\lambda(t, u)}{\\int e^{\\theta \\cdot s} g_\\lambda(s, u) ds}\\]\nEvaluate \\(H_0: \\theta \\in \\Theta_0\\) vs \\(H_1: \\theta \\in \\Theta_1\\) in s-parameter model \\(\\{p_\\theta(\\cdot|u): \\theta \\in \\Theta\\}\\)\nNote: If \\(s=1\\), this family has MLR in \\(T\\). Even if \\(s&gt;1\\), we have still gotten rid of \\(\\lambda\\)."
  },
  {
    "objectID": "reader/testing-nuisance.html#theorem-informal",
    "href": "reader/testing-nuisance.html#theorem-informal",
    "title": "Testing with Nuisance Parameters",
    "section": "2 Theorem (Informal)",
    "text": "2 Theorem (Informal)\nLet \\(\\cP\\) be full rank exp. fam. with densities \\(p_{\\theta, \\lambda}(x) = e^{\\theta \\cdot T(x) + \\lambda \\cdot U(x) - A(\\theta, \\lambda)}h(x)\\)\n\\(\\theta \\in \\mathbb{R}^s\\), \\(\\lambda \\in \\mathbb{R}^r\\), \\((\\theta_0, \\lambda_0)\\) possible\n\nTo test \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta \\neq \\theta_0\\), there is a UMPU test \\(\\phi(x) = \\psi(T(x), U(x))\\) where\n\\[\\psi(t, u) = \\begin{cases}\n1 & \\text{if } t &gt; c_2(u) \\\\\n\\gamma_2(u) & \\text{if } t = c_2(u) \\\\\n0 & \\text{if } c_1(u) &lt; t &lt; c_2(u) \\\\\n\\gamma_1(u) & \\text{if } t = c_1(u) \\\\\n1 & \\text{if } t &lt; c_1(u)\n\\end{cases}\\]\nwith \\(\\gamma_1, \\gamma_2, c_1, c_2\\) chosen to make \\(\\mathbb{E}_{\\theta_0}[\\phi] = \\alpha\\) and \\(\\mathbb{E}_{\\theta_0}[T\\phi] = \\theta_0\\)\nTo test \\(H_0: \\theta \\leq \\theta_0\\) vs \\(H_1: \\theta &gt; \\theta_0\\), there is a UMPU test \\(\\phi(x) = \\psi(T(x), U(x))\\) where\n\\[\\psi(t, u) = \\begin{cases}\n1 & \\text{if } t &gt; c(u) \\\\\n\\gamma(u) & \\text{if } t = c(u) \\\\\n0 & \\text{if } t &lt; c(u)\n\\end{cases}\\]\nwith \\(\\gamma, c\\) chosen to make \\(\\mathbb{E}_{\\theta_0}[\\phi] = \\alpha\\)\n\nNote: \\(h\\) has disappeared from the problem.\n\n2.1 Example: Poisson Ratio\n\\(X_i \\sim \\text{iid Poisson}(\\mu_i)\\), \\(i=1,2\\)\n\\(H_0: \\mu_1 = \\mu_2\\) vs \\(H_1: \\mu_1 \\neq \\mu_2\\)\n\\[p(x) = \\frac{\\mu_1^{x_1} e^{-\\mu_1}}{x_1!} \\cdot \\frac{\\mu_2^{x_2} e^{-\\mu_2}}{x_2!} = e^{x_1 \\log \\mu_1 + x_2 \\log \\mu_2 - \\mu_1 - \\mu_2}\\]\nLet \\(\\theta = \\log \\frac{\\mu_1}{\\mu_2}\\), \\(\\lambda = \\log \\mu_2\\)\n\\(H_0: \\theta = 0\\) vs \\(H_1: \\theta \\neq 0\\)\nReject for conditionally large values of \\(X_1\\) given \\(X_1 + X_2 = u\\):\n\\[P_\\theta(X_1 = x_1 | X_1 + X_2 = u) = \\frac{e^{\\theta x_1}}{\\sum_{i=0}^u e^{\\theta i}} = \\binom{u}{x_1} \\left(\\frac{e^\\theta}{1+e^\\theta}\\right)^{x_1} \\left(\\frac{1}{1+e^\\theta}\\right)^{u-x_1}\\]\n\\(X_1 | X_1 + X_2 \\sim \\text{Binom}(u, \\frac{e^\\theta}{1+e^\\theta})\\)\nSo in the end, we do a Binomial test."
  },
  {
    "objectID": "reader/testing-nuisance.html#proof-sketch",
    "href": "reader/testing-nuisance.html#proof-sketch",
    "title": "Testing with Nuisance Parameters",
    "section": "3 Proof Sketch",
    "text": "3 Proof Sketch\n\nAny unbiased test has \\(\\mathbb{P}(\\phi=1) \\leq h(t,u)\\) (continuity)\nPower = 0 on boundary \\(\\implies \\mathbb{E}_{\\theta_0}[T\\phi] = \\theta_0\\) (UK complete sufficient on boundary sub-model)\n\\(\\phi\\) optimal among all tests with conditional level \\(\\alpha\\) by reduction to univariate model\n\n\n3.1 Detailed Proof\nAssume \\(\\phi\\) any unbiased test:\n\n\\(\\mathbb{E}_\\theta[\\phi] = \\alpha + f(\\theta)\\), \\(f(\\theta_0) = 0\\), \\(f'(\\theta_0) = 0\\) Keener Thm 12.4\n\\(\\mathbb{E}_\\theta[\\phi]\\) infinitely diff on \\(\\mathbb{R}^s\\), can diff under \\(\\int\\)\n\\(\\phi\\) unbiased \\(\\implies \\mathbb{E}_\\theta[T\\phi] = \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_\\theta[\\phi] = \\theta\\)\n\nStep 1: Boundary sub-model \\(\\cP_0 = \\{p_{\\theta_0, \\lambda}: \\lambda \\in \\mathbb{R}^r\\}\\)\n\\(p_{\\theta_0, \\lambda}(x) = e^{\\lambda \\cdot U(x) - A(\\theta_0, \\lambda)}h(x)\\)\n\\(\\cP_0\\) is full rank \\(r\\)-param exp fam, \\(U(X)\\) complete suff\nLet \\(f(\\lambda) = \\mathbb{E}_{\\theta_0, \\lambda}[\\phi(X) | U(X)] = \\alpha\\)\n\\(\\mathbb{E}_{\\theta_0, \\lambda}[\\phi(X) T(X) | U(X)] = \\theta_0\\) a.s.\n\\(\\mathbb{E}_{\\theta_0, \\lambda}[\\phi(X) | U(X)] = \\alpha\\) a.s.\nTwo-sided: \\(\\mathbb{E}_{\\theta_0, \\lambda}[g(U(X))\\phi(X)] = \\mathbb{E}_{\\theta_0, \\lambda}[T(X)\\phi(X)] = \\theta_0\\)\n\\(\\mathbb{E}_{\\theta_0, \\lambda}[g(U)\\mathbb{E}_{\\theta_0, \\lambda}[\\phi|U]] = \\theta_0\\)\n\\(\\mathbb{E}_{\\theta_0, \\lambda}[g(U) \\alpha] = \\theta_0\\)\n\\(\\mathbb{E}_{\\theta_0, \\lambda}[g(U)] = \\frac{\\theta_0}{\\alpha}\\)\nOne-sided: \\(\\mathbb{E}_{\\theta_0, \\lambda}[\\phi] = \\alpha\\) \\(\\forall \\lambda\\)\nSteps 2-3: For any value \\(u\\), the conditional model is:\n\\[p_\\theta(t|u) = e^{\\theta \\cdot t} g(t,u)\\]\n1-param exp fam.\nIn one/two-sided case, we have shown \\(\\psi(t,u)\\) is UMP/UMPU in \\(\\{p_\\theta(\\cdot|u)\\}\\)\nLet \\(g(t,u) = \\mathbb{E}_{\\theta_0}[\\phi(X) | T(X)=t, U(X)=u] \\leq 1\\)\n\\(\\mathbb{E}_{\\theta_0}[\\psi(T,U) | U] = \\mathbb{E}_{\\theta_0}[\\phi(X) | U(X)=u]\\)\n\\(\\psi\\) if \\(\\theta &gt; \\theta_0\\) \\(\\phi(X)\\) is a conditional test of \\(H_0\\) vs \\(H_1\\) in \\(\\{p_\\theta(\\cdot|u)\\}\\) with power \\(\\leq \\alpha\\) at boundary\nOne-sided case: For \\(\\theta &gt; \\theta_0\\) \\(\\psi(t,u)\\) is the UMP test of \\(\\theta=\\theta_0\\) vs \\(\\theta&gt;\\theta_0\\) in \\(\\{p_\\theta(\\cdot|u)\\}\\), which is a 1-param exp fam\nTwo-sided: \\(\\psi(t,u)\\) is the UMP test of \\(\\theta=\\theta_0\\) vs \\(\\theta \\neq \\theta_0\\) among tests with power \\(\\alpha\\) over \\(\\theta=\\theta_0\\) Keener Thm 12.22 (main thm for two-sided tests)\nIn either case, \\(\\psi\\) has higher cond. power than \\(\\phi\\) a.s.\nFor \\(\\theta \\neq \\theta_0\\):\n\\[\\mathbb{E}_\\theta[\\phi] = \\mathbb{E}_\\theta[\\mathbb{E}[\\phi(X) | T(X), U(X)]]\\] \\[\\leq \\mathbb{E}_\\theta[\\mathbb{E}[\\psi(T(X), U(X)) | T(X), U(X)]]\\] \\[= \\mathbb{E}_\\theta[\\psi]\\]\n\n\n3.2 Example: Normal Mean with Unknown Variance\n\\(X \\sim N(\\mu, \\sigma^2)\\), \\(\\sigma^2&gt;0\\) unknown \\(H_0: \\mu=0\\) vs \\(H_1: \\mu \\neq 0\\)\n\\(T = \\frac{\\bar{X}}{\\|X\\|}\\), \\(U = \\|X\\|^2\\)\nOptimal test rejects when \\(\\bar{X}\\) is extreme given \\(\\|X\\|^2\\)\nIf \\(\\mu=0\\), \\(\\frac{X}{\\|X\\|}\\) is rotationally symmetric \\(\\frac{X}{\\|X\\|} \\sim \\text{Unif}(S^{n-1})\\), \\(\\frac{X}{\\|X\\|}\\) indep of \\(\\|X\\|\\)\nOptimal test rejects when \\(\\frac{\\bar{X}}{\\|X\\|}\\) extreme (marginally)\nCould stop here & simulate\n\n3.2.1 Geometric Picture (n=2)\n[Insert geometric picture here]\nAbove test rejects for: - conditionally extreme \\(\\bar{X}\\) given \\(\\|X\\|^2\\) OR - marginally extreme \\(\\frac{\\bar{X}}{\\|X\\|}\\)\nFact: reject for marginally extreme \\(T\\) where\n\\[T^2 = \\frac{(\\sum X_i)^2}{\\sum X_i^2 - \\frac{1}{n}(\\sum X_i)^2} = \\frac{n\\bar{X}^2}{\\|X\\|^2 - n\\bar{X}^2} = \\frac{n\\bar{X}^2}{S^2}\\]\nand \\(S^2 = \\frac{1}{n-1}\\sum (X_i - \\bar{X})^2\\)\n\n\n3.2.2 Geometric Picture\n[Insert second geometric picture here]\n\\(T^2 = \\frac{\\|\\text{Proj}_\\mathbf{1}X\\|^2}{\\|\\text{Proj}_{\\mathbf{1}^\\perp}X\\|^2} \\cdot \\frac{n-1}{n}\\)\nNext major theme: ratios of projections"
  },
  {
    "objectID": "reader/testing-nuisance.html#permutation-tests",
    "href": "reader/testing-nuisance.html#permutation-tests",
    "title": "Testing with Nuisance Parameters",
    "section": "4 Permutation Tests",
    "text": "4 Permutation Tests\nEven if we don’t get a UMPU test at the end, conditioning on null suff. stat. still helps\nExample: \\(X_1, \\ldots, X_n \\sim \\text{iid } P\\), \\(Y_1, \\ldots, Y_m \\sim \\text{iid } Q\\) \\(H_0: P=Q\\) vs \\(H_1: P \\neq Q\\)\nUnder \\(H_0\\): \\(P=Q\\), \\(X_1, \\ldots, X_n, Y_1, \\ldots, Y_m \\sim P\\)\nLet \\(Z = (Z_1, \\ldots, Z_{n+m}) = (X_1, \\ldots, X_n, Y_1, \\ldots, Y_m)\\)\nUnder \\(H_0\\), \\(U = Z\\) is complete sufficient\nLet \\(S_{n+m}\\) = Permutations on \\(n+m\\) elements\n\\((X, Y) = (U_{\\pi(1)}, \\ldots, U_{\\pi(n+m)})\\) for \\(\\pi \\in S_{n+m}\\)\nThus for test stat \\(T\\), if \\(P=Q\\):\n\\[\\mathbb{P}(T \\geq t | U) = \\frac{1}{(n+m)!}\\sum_{\\pi \\in S_{n+m}} 1\\{T(Z_{\\pi(1)}, \\ldots, Z_{\\pi(n+m)}) \\geq t\\}\\]\nMonte Carlo test: In practice, we sample $_1, , _B"
  },
  {
    "objectID": "reader/unbiased-estimation.html",
    "href": "reader/unbiased-estimation.html",
    "title": "Unbiased Estimation",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/unbiased-estimation.html#outline",
    "href": "reader/unbiased-estimation.html#outline",
    "title": "Unbiased Estimation",
    "section": "1 Outline",
    "text": "1 Outline\n\nConvex Loss\nRao-Blackwell Theorem\nUMVU Estimators\nExamples"
  },
  {
    "objectID": "reader/unbiased-estimation.html#unbiased-estimation",
    "href": "reader/unbiased-estimation.html#unbiased-estimation",
    "title": "Unbiased Estimation",
    "section": "2 Unbiased Estimation",
    "text": "2 Unbiased Estimation\nRecall from Lecture 2 that we had two primary strategies to choose an estimator:\n\nSummarize the risk function by a scalar (average or supremum)\nRestrict attention to a smaller class of estimators\n\nToday we’ll discuss unbiased estimation, which is an example of the second strategy. That is, if \\(g(\\theta)\\) is our estimand, we will require that \\(\\EE_\\theta \\delta = g(\\theta)\\) for all \\(\\theta\\)\nUnbiased estimation is especially convenient in models with a complete sufficient statistic \\(T(X)\\). In that case:\n\nThere is at most one unbiased \\(\\delta(T(X))\\) (if \\(\\delta_1, \\delta_2(T)\\) are both unbiased, then \\(\\delta_1 \\eqas \\delta_2\\))\nIf an unbiased estimator exists, it uniformly minimizes risk for any convex loss function"
  },
  {
    "objectID": "reader/unbiased-estimation.html#convex-loss-functions",
    "href": "reader/unbiased-estimation.html#convex-loss-functions",
    "title": "Unbiased Estimation",
    "section": "3 Convex Loss Functions",
    "text": "3 Convex Loss Functions\nRecall \\(f(y)\\) is convex if for all \\(x_1, x_2\\) and all \\(\\gamma \\in [0,1]\\):\n\\[f(\\gamma x_1 + (1-\\gamma)x_2) \\leq \\gamma f(x_1) + (1-\\gamma) f(x_2)\\] \\(f\\) is strictly convex if the inequality is strict unless \\(x_1 = x_2\\).\nAn key fact about convex functions is Jensen’s Inequality: If \\(f\\) is convex, then for any random variable \\(X\\), we have\n\\[f(\\EE[X]) \\leq \\EE[f(X)]\\] If \\(f\\) is strictly convex, then the inequality is strict unless \\(X\\) is constant.\nWe say a loss function \\(L(\\theta, d)\\) is (strictly) convex if is (strictly) convex as a function of the estimate \\(d\\), its second argument, holding the parameter \\(\\theta\\) fixed.\nExample: The best-known example of a convex loss function is the squared error loss. Recall that the corresponding risk, the MSE, can be decomposed as the sum of the bias squared and the variance:\n\\[\n\\begin{aligned}\n\\text{MSE}_\\theta(\\delta) &= \\EE_\\theta[(\\delta(X) - g(\\theta))^2] \\\\[5pt]\n&= \\text{Bias}_\\theta(\\delta)^2 + \\text{Var}_\\theta(\\delta(X))\n\\end{aligned}\n\\] If \\(\\delta(X)\\) is unbiased, then its MSE is exactly its variance, so minimizing the risk among unbiased estimators just amounts to finding one with the least variance."
  },
  {
    "objectID": "reader/unbiased-estimation.html#the-rao-blackwell-theorem",
    "href": "reader/unbiased-estimation.html#the-rao-blackwell-theorem",
    "title": "Unbiased Estimation",
    "section": "4 The Rao-Blackwell Theorem",
    "text": "4 The Rao-Blackwell Theorem\nIntuitively, convex losses punish us for using noisy estimators: we would always improve the risk if we could replace an estimator \\(\\delta(X)\\) with its expectation: \\[L(\\theta, \\EE_\\theta [\\delta(X)]) \\leq \\EE_\\theta \\left[L(\\theta, \\delta(X))\\right].\\]Generally, this is not feasible in real problems because \\(\\EE_\\theta [\\delta(X)]\\) depends on \\(\\theta\\), so it is not an estimator. But for any sufficient statistic \\(T(X)\\), the conditional expectation of \\(\\delta(X)\\) given \\(T(X)\\) really is an estimator, and it is always at least as good as \\(\\delta(X\\)) if the loss is convex.\nThe next result formalizes this fact, and thereby gives decision-theoretic teeth to the sufficiency principle:\nTheorem (Rao-Blackwell): Let \\(T(X)\\) be sufficient for \\(\\cP = \\{P_\\theta:\\;\\theta\\in\\Theta\\}\\), and let \\(\\delta(X)\\) be any estimator for \\(g(\\theta)\\). Define the new estimator:\n\\[\\bar{\\delta}(T(X)) = \\EE[\\delta(X) \\mid T(X)]\\]\nThen for any convex loss \\(L(\\theta, d)\\), we have \\(R(\\theta, \\bar{\\delta}) \\leq R(\\theta, \\delta)\\) for all \\(\\theta\\). If \\(L\\) is strictly convex, \\(\\bar{\\delta}\\) strictly dominates \\(\\delta\\) as an estimator unless \\(\\delta(X) \\eqPas \\bar{\\delta}(T(X))\\).\nProof:\n\\[\\begin{aligned}\nR(\\theta, \\bar{\\delta}) &= \\EE_\\theta\\left[\\,L(\\theta, \\;\\EE[\\delta \\mid T])\\,\\right]\\\\[5pt]\n&\\leq \\EE_\\theta\\left[\\,\\EE[L(\\theta, \\delta) \\mid T]\\,\\right]\\\\[5pt]\n&= \\EE_\\theta[\\,L(\\theta, \\delta)\\,] \\\\[5pt]\n&= R(\\theta, \\bar{\\delta})\n\\end{aligned}\\]\n\\(\\bar{\\delta}\\) is called the Rao-Blackwellization of \\(\\delta\\). Note that the condition $\\delta(X) \\eqPas \\bar{\\delta}(T(X))$ is equivalent to the condition that \\(\\delta\\) depends only on \\(X\\) through \\(T(X)\\).\nWhenever we are dealing with a convex loss, the Rao-Blackwell theorem lets us restrict our attention only to estimators that run through \\(T(X)\\), because any other estimator could be improved (or at least not worsened) by Rao-Blackwellization. The theorem even gives us a recipe for **constructing** the improved estimator."
  },
  {
    "objectID": "reader/unbiased-estimation.html#umvu-estimators",
    "href": "reader/unbiased-estimation.html#umvu-estimators",
    "title": "Unbiased Estimation",
    "section": "5 UMVU estimators",
    "text": "5 UMVU estimators\nIn this section we will combine two key facts from this lecture and last concerning unbiased estimation of any estimand \\(g(\\theta)\\).\n\nIf \\(T(X)\\) is complete sufficient, there can be at most one unbiased estimator based on \\(T(X)\\).\nIf the loss is convex, then we can restrict our attention only to estimators that are based on \\(T(X)\\).\n\nTogether these facts imply that, if any unbiased estimator exists at all, then there is a unique best unbiased estimator.\nNot all estimands have unbiased estimators. We say \\(g(\\theta)\\) is U-estimable if there exists any \\(\\delta(X)\\) with \\(\\EE_\\theta \\delta(X) = g(\\theta)\\) for all \\(\\theta\\). This leads to the following theorem:\nTheorem: For model \\(\\mathcal{P} = \\{P_\\theta : \\theta \\in \\Theta\\}\\), assume \\(T(X)\\) is a complete sufficient statistic. Then\n\nFor any U-estimable \\(g(\\theta)\\) there exists a unique unbiased estimator of the form \\(\\delta(T(X))\\).\nFor a (strictly) convex loss, that estimator (strictly) dominates any other unbiased estimator \\(\\tilde{\\delta}(X)\\) unless \\(\\tilde{\\delta}(X) \\eqas \\delta(T(X))\\).\n\nAs usual the “uniqueness” here is only up to \\(\\eqPas\\).\nProof:\n(1) Since \\(g(\\theta)\\) is U-estimable, there exists some unbiased estimator \\(\\delta_0(X)\\). Then its Rao-Blackwellization \\(\\delta(T(X)) = \\EE[\\delta_0 \\mid T]\\) is also unbiased, since\n\\[\n\\EE_\\theta \\delta(T) = \\EE_\\theta[\\EE[\\delta_0 | T]] = \\EE_\\theta \\delta_0 = g(\\theta).\n\\]\nAny other estimator of the form \\(\\tilde\\delta(T)\\) must be almost surely equal to \\(\\delta(T)\\), by completeness: if \\(f(t) = \\delta(t)-\\tilde\\delta(t)\\), then both estimators being unbiased means \\(\\EE_\\theta f(T) = g(\\theta)-g(\\theta) = 0\\), so \\(f(T(X)) \\eqas 0\\). Thus, \\(\\delta(T)\\) is unique.\n(2) The first result implies that every unbiased estimator has the same Rao-Blackwellization, namely \\(\\delta(T)\\). Thus, by the Rao-Blackwell theorem, \\(\\delta(T)\\) (strictly) dominates every other unbiased estimator for any (strictly) convex loss function, unless the estimator is almost surely identical to \\(\\delta\\). \\(\\blacksquare\\)\nThe estimator from this theorem is usually called the UMVU (Uniformly Minimum Variance Unbiased) Estimator. We say \\(\\delta(X)\\) is UMVU if:\n\n\\(\\delta(X)\\) is unbiased\n\\(\\text{Var}_\\theta \\,\\delta(X) \\leq \\text{Var}_\\theta \\,\\tilde{\\delta}(X)\\) for all \\(\\theta\\) and all unbiased \\(\\tilde{\\delta}(X)\\)\n\nSince \\(\\text{MSE}(\\theta; \\delta) = \\text{Var}_\\theta(\\delta(X))\\) for any unbiased estimator, and the squared error loss is strictly convex, Theorem XXX immediately implies the existence of a unique UMVU estimator for any U-estimable \\(g(\\theta)\\), whenever we have a complete sufficient statistic.\nNote that in problems where no complete sufficient statistic exists, there can be multiple unbiased estimators based on the minimal sufficient statistic; for example, both the mean and the median are unbiased for the Laplace location parameter, but they are not almost surely equal to each other, and they do not have the same risk function."
  },
  {
    "objectID": "reader/unbiased-estimation.html#finding-the-umvue",
    "href": "reader/unbiased-estimation.html#finding-the-umvue",
    "title": "Unbiased Estimation",
    "section": "6 Finding the UMVUE",
    "text": "6 Finding the UMVUE\nTheorem XXX suggests two strategies for finding the UMVUE:\n\nSolve directly for an unbiased estimator based on \\(T\\)\nFind any unbiased estimator at all, then Rao-Blackwellize it\n\nWe give examples of both strategies below:\nExample (Poisson): Let \\(X_1, \\ldots, X_n \\sim \\text{Pois}(\\theta)\\), \\(g(\\theta) = e^{-\\theta}\\) and consider unbiased estimation for \\(g(\\theta) = \\theta^2\\).\nThe complete sufficient statistic for the model is\n\\[T(X) = \\sum X_i \\sim \\text{Pois}(n\\theta),\\] and its probability mass function for \\(t \\geq 0\\) is \\[\np_\\theta(t) = \\frac{e^{-n\\theta} (n\\theta)^t}{t!}\n\\] Strategy 1\nIf there is some unbiased estimator \\(\\delta(t)\\), we can try to solve for it by setting its expectation equal to \\(\\theta^2\\): \\[\n\\theta^2 = \\EE_\\theta \\delta(T) = \\sum_{t=0}^\\infty \\delta(t) \\frac{e^{-n\\theta} (n\\theta)^t}{t!}.\n\\] Rearranging factors, we obtain matching power series: \\[\n\\sum_{t=0}^\\infty \\delta(t) \\frac{n^t \\theta^t}{t!} = e^{n\\theta}\\theta^2 =  \\sum_{k=0}^\\infty \\frac{n^k\\theta^{k+2}}{k!}.\n\\] We will choose the coefficients on the left-hand side to match terms. First, change the index for the left-hand sum to \\(t = k+2\\): \\[\n\\sum_{t=0}^\\infty \\delta(t) \\frac{n^t \\theta^t}{t!} = e^{n\\theta}\\theta^2 =  \\sum_{t=2}^\\infty \\frac{n^{t-2}\\theta^{t}}{(t-2)!}.\n\\] To match the terms, we can set \\(\\delta(0)=\\delta(1)=0\\), and for \\(t\\geq 2\\), set \\(\\delta(t)=\\frac{t!}{n^2(t-2)!}=\\frac{t(t-1)}{n^2}\\). The same expression works for both, so we obtain the estimator \\[\n\\delta(T) = \\frac{T(T-1)}{n^2}\n\\]\nStrategy 2:\nAlternatively, we can find an unbiased estimator and Rao-Blackwellize it. If $n$, we can use the fact that\n\\[\n\\EE_\\theta [X_1 X_2] = \\EE_\\theta [X_1] \\;\\cdot\\; \\EE_\\theta [X_2] = \\theta^2\n\\] to obtain an initial unbiased estimator \\(\\delta_0(X) = X_1X_2\\), which we will Rao-Blackwellize.\nConditional on $\nto match the terms\n\\(\\delta(T) = (1 - 1/n)^T\\) unbiased:\n\\[\\begin{aligned}\n\\EE_\\theta \\delta(T) &= \\sum_{t=0}^\\infty (1-1/n)^t e^{-n\\theta} (n\\theta)^t / t! \\\\\n&= e^{-n\\theta} \\sum_{t=0}^\\infty ((n-1)\\theta)^t / t! \\\\\n&= e^{-n\\theta} e^{(n-1)\\theta} = e^{-\\theta}\n\\end{aligned}\\]\nAlternatively, we could Rao-Blackwellize \\(\\delta_0(X) = I(X_1 = 0)\\):\n\\[\\begin{aligned}\n\\EE[I(X_1 = 0) | T] &= \\PP(X_1 = 0 | T) \\\\\n&= \\frac{\\PP(X_1 = 0, X_2 + \\cdots + X_n = T)}{\\PP(X_2 + \\cdots + X_n = T-1) + \\PP(X_2 + \\cdots + X_n = T)} \\\\\n&= \\frac{\\binom{n-1}{T} (1/n)^0 (1-1/n)^T}{\\binom{n-1}{T-1} (1/n) (1-1/n)^{T-1} + \\binom{n-1}{T} (1-1/n)^T} \\\\\n&= \\frac{(1-1/n)^T}{T/n + (1-1/n)^T} \\\\\n&= (1-1/n)^T\n\\end{aligned}\\]\nExample: \\(X_1, \\ldots, X_n \\sim U[0, \\theta]\\), \\(\\theta &gt; 0\\)\n\\(T = X_{(n)}\\) complete sufficient\n\\(p_\\theta(t) = n t^{n-1} / \\theta^n \\cdot I(0 &lt; t &lt; \\theta)\\)\n\\(\\EE_\\theta[T] = \\frac{n}{n+1} \\theta\\)\n\\(T \\cdot \\frac{n+1}{n}\\) is UMVUE\nAlternatively, \\(2X_1\\) is unbiased:\n\\[\\EE[2X_1 | T] = 2T \\cdot \\frac{n+1}{2n} = T \\cdot \\frac{n+1}{n}\\]\nActually, \\(T\\) is inadmissible too! Keener shows \\(\\frac{n-1}{n} T\\) has better MSE for any estimator \\(c \\cdot T\\).\nThis raises the question: why do we require zero bias?\nThe UMVUE is often inefficient, inadmissible, or just dumb in cases where another approach makes much more sense.\nExample: \\(X \\sim \\text{Bin}(1000, \\theta)\\)\nEstimate \\(g(\\theta) = I(\\theta &gt; 0.5)\\)\nUMVUE is \\(I(X &gt; 500)\\). Why?\n\n\\(X = 500\\): Conclude \\(g(\\theta) = 1\\)\n\\(X = 499\\): Conclude \\(g(\\theta) = 0\\)\n\nThis is not epistemically reasonable. Could do much better with e.g. MLE or a Bayes estimator.\nIn fact, our theorem should make us suspicious of UMVUEs: every idiotic function of \\(T\\) is a UMVUE of its own expectation!\nExample: \\(X_1, \\ldots, X_n \\sim N(\\mu, 1)\\), estimate \\(g(\\mu) = \\|\\mu\\|\\)\n\\(\\bar{X}\\) is complete sufficient\n\\(\\|\\bar{X}\\|\\) is unbiased: \\(\\EE[\\|\\bar{X}\\|] = \\EE[\\|N(\\mu, 1/n)\\|] = \\|\\mu\\|\\)\nSo \\(\\|\\bar{X}\\|\\) is UMVUE\nIf \\(\\mu = 0\\), \\(\\delta(\\bar{X}) = 0\\) about half the time\n\\(\\|\\bar{X}\\| + d \\cdot \\max(0, \\|\\bar{X}\\| - d)\\) strictly dominates UMVUE"
  },
  {
    "objectID": "reader/multiple-testing.html",
    "href": "reader/multiple-testing.html",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/multiple-testing.html#multiple-testing",
    "href": "reader/multiple-testing.html#multiple-testing",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "1 Multiple Testing",
    "text": "1 Multiple Testing\nIn many testing problems, we want to test many hypotheses at a time, e.g.:\n\nTest \\(H_0: \\beta_j = 0\\) for \\(j = 1,\\ldots,d\\) in linear regression\nTest whether each of 20K single nucleotide polymorphisms (SNPs) is associated with a given phenotype (e.g., diabetes, schizophrenia)\nTest whether each of 2000 website tweaks affect user engagement\n\n\n1.1 Setup\n\\(X \\sim P_\\theta \\in \\cP\\), \\(H_{0i}: \\theta \\in \\Theta_i\\), \\(i = 1,\\ldots,m\\)\nCommonly, \\(H_{0i}: \\theta_i = 0\\)\nGoal: Return accept/reject decision for each \\(i\\)\nLet \\(R = \\{i: H_{0i} \\text{ rejected}\\}\\), \\(|R| \\leq m\\)\n\\(H_{0c} = \\{i: H_{0i} \\text{ true}\\}\\), \\(|H_{0c}| = m_0 \\leq m\\)\n\\(R \\cap H_{0c}\\) = false rejections"
  },
  {
    "objectID": "reader/multiple-testing.html#family-wise-error-rate-fwer",
    "href": "reader/multiple-testing.html#family-wise-error-rate-fwer",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "2 Family-wise Error Rate (FWER)",
    "text": "2 Family-wise Error Rate (FWER)\nProblem: Even if all \\(H_{0i}\\) true, might have:\n\\(\\mathbb{P}(\\text{any } H_{0i} \\text{ rejected}) \\leq 1 - (1-\\alpha)^m \\approx m\\alpha\\)\nExample: \\(X_i \\sim N(\\theta_i, 1)\\) iid, \\(i = 1,\\ldots,m\\), \\(H_{0i}: \\theta_i = 0\\)\n\\(\\mathbb{P}_0(\\text{any } H_{0i} \\text{ rejected}) = 1 - (1-\\alpha)^m \\approx m\\alpha\\)\nIs this a problem? Yes, if all attention will be focused on the false rejections and none on the correct non-rejections.\nClassical solution is to control the family-wise error rate (FWER):\nFWER = \\(\\mathbb{P}_\\theta(\\text{any false rejections}) = \\mathbb{P}_\\theta(R \\cap H_{0c} \\neq \\emptyset)\\)\nWant: \\(\\sup_\\theta \\text{FWER}(\\theta) \\leq \\alpha\\)\nTypically achieved by correcting marginal p-values: \\(p_1(X), \\ldots, p_m(X)\\), \\(p_i \\sim U(0,1)\\)\ne.g., \\(\\phi_i = 1\\{\\alpha/(2m)|X_i| &gt; \\Phi^{-1}(1-\\alpha/(2m))\\}\\) for Gaussian"
  },
  {
    "objectID": "reader/multiple-testing.html#bonferroni-correction",
    "href": "reader/multiple-testing.html#bonferroni-correction",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "3 Bonferroni Correction",
    "text": "3 Bonferroni Correction\nAssume \\(p_1,\\ldots,p_m\\) are p-values for \\(H_{01},\\ldots,H_{0m}\\) with \\(p_i \\sim U(0,1)\\) under \\(H_{0i}\\)\nFor general dependence, can guarantee control by rejecting \\(H_{0i}\\) iff \\(p_i \\leq \\alpha/m\\):\n\\(\\mathbb{P}_\\theta(\\text{any false rejections}) \\leq \\mathbb{P}_\\theta(\\text{any } H_{0i} \\text{ rejected}) \\leq \\sum_{i \\in H_{0c}} \\mathbb{P}_\\theta(H_{0i} \\text{ rejected}) \\leq m_0\\alpha/m \\leq \\alpha\\)\nIf p-values independent, can improve to \\(1-(1-\\alpha)^{1/m}\\) (Šidák correction)\nThen \\(\\mathbb{P}_\\theta(\\text{no false rejections}) = \\prod_{i \\in H_{0c}} \\mathbb{P}_\\theta(p_i &gt; (1-(1-\\alpha)^{1/m})) \\geq (1-\\alpha)^{m_0/m} \\geq 1-\\alpha\\)\nFor small \\(\\alpha\\): \\(1-(1-\\alpha)^{1/m} \\approx \\alpha/m\\)\nŠidák doesn’t improve much on Bonferroni"
  },
  {
    "objectID": "reader/multiple-testing.html#testing-with-dependence",
    "href": "reader/multiple-testing.html#testing-with-dependence",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "4 Testing with Dependence",
    "text": "4 Testing with Dependence\nBonferroni isn’t much worse than Šidák e.g., \\(\\alpha = 0.05\\), \\(m = 20\\): \\(0.0025\\) vs \\(0.00256\\)\nBut when tests are highly dependent, can often do much better\n\n4.1 Example: Scheffé’s S-method\n\\(X \\sim N(\\theta, I_d)\\), \\(\\theta \\in \\mathbb{R}^d\\) \\(H_0: a_j^T \\theta = 0\\) for \\(j = 1,\\ldots,m\\), \\(\\|a_j\\| = 1\\)\nReject \\(H_{0j}\\) if \\(|a_j^T X| &gt; \\sqrt{d F_{d,\\infty,1-\\alpha}}\\)\nControls FWER:\n\\(\\mathbb{P}(\\|X - \\theta\\|^2 \\leq dF_{d,\\infty,1-\\alpha}) = 1-\\alpha\\)\nCan view as deduction from confidence region: \\(C(X) = \\{\\theta: \\|X - \\theta\\|^2 \\leq dF_{d,\\infty,1-\\alpha}\\}\\)"
  },
  {
    "objectID": "reader/multiple-testing.html#deduced-inference",
    "href": "reader/multiple-testing.html#deduced-inference",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "5 Deduced Inference",
    "text": "5 Deduced Inference\nGiven any joint confidence region \\(C(X)\\) for \\(\\theta \\in \\Theta\\), we may freely assume \\(\\theta \\in C(X)\\) and deduce any and all implied conclusions without any FWER inflation:\n\\(\\mathbb{P}_\\theta(\\text{any deduced inference is wrong}) \\leq \\mathbb{P}_\\theta(\\theta \\notin C(X)) \\leq \\alpha\\)\nDeduction is often a good paradigm for deriving simultaneous intervals\nWe say \\(C_1(X),\\ldots,C_m(X)\\) are simultaneous \\(1-\\alpha\\) confidence intervals for \\(g_1(\\theta),\\ldots,g_m(\\theta)\\) if:\n\\(\\mathbb{P}_\\theta(g_i(\\theta) \\in C_i(X) \\text{ for all } i = 1,\\ldots,m) \\geq 1-\\alpha\\)\n\n5.1 Example: Simultaneous Intervals for Multivariate Gaussian\nAssume \\(X \\sim N_d(\\theta, \\Sigma)\\), \\(\\Sigma\\) known, \\(\\Sigma_{ii} = 1\\)\nLet \\(t_\\alpha\\) be upper \\(\\alpha\\) quantile of \\(\\|X - \\theta\\|_\\Sigma = \\sqrt{(X-\\theta)^T \\Sigma^{-1}(X-\\theta)}\\)\n\\(C_i(X) = [\\theta_i: |X_i - \\theta_i| \\leq t_\\alpha \\sqrt{\\Sigma_{ii}}]\\) for all \\(i\\)\n\\(\\mathbb{P}(C(X) \\ni \\theta_i \\text{ for any } i) = \\mathbb{P}(\\|X - \\theta\\|_\\Sigma \\leq t_\\alpha) = 1-\\alpha\\)\n\\(t_\\alpha = \\sqrt{\\chi^2_{d,1-\\alpha}}\\) if \\(\\Sigma = I_d\\)\nNote: we could have instead constructed an elliptical conf. region, but then the intervals would be conservative:\n\\(\\mathbb{P}(\\|X - \\theta\\|_\\Sigma^2 \\leq \\chi^2_{d,1-\\alpha}) = 1-\\alpha\\)\n\n\n5.2 Example: Linear Regression (n obs, d variables)\n\\(X \\in \\mathbb{R}^{n \\times d}\\) design, \\(\\beta \\in \\mathbb{R}^d\\), \\(Y \\sim N(X\\beta, \\sigma^2 I_n)\\)\nEstimate \\(\\hat{\\beta} = (X^T X)^{-1} X^T Y\\)\nwhere \\(\\hat{\\beta} \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\)\n\\(S^2 = \\|Y - X\\hat{\\beta}\\|^2/(n-d)\\), \\(V = RS^2\\), \\(R = (X^T X)^{-1}\\)\nDistr. of \\(\\hat{\\beta}_j/\\sqrt{V_{jj}}\\) fully known\nAssume w.l.o.g. \\(X^T X = I_d\\)\nLet \\(t_\\alpha\\) denote upper \\(\\alpha\\) quantile of \\(\\|\\hat{\\beta} - \\beta\\|/\\sqrt{S^2}\\)\nThen \\(C_j = \\hat{\\beta}_j \\pm t_\\alpha \\sqrt{V_{jj}}\\) are simultaneous CIs for \\(\\beta_j\\), \\(j = 1,\\ldots,d\\) (compute \\(t_\\alpha\\) by simulation)\n\\(\\mathbb{P}(|\\hat{\\beta}_j - \\beta_j| \\leq t_\\alpha \\sqrt{V_{jj}} \\text{ for all } j) = 1-\\alpha\\)"
  },
  {
    "objectID": "reader/multiple-testing.html#false-discovery-rate-fdr",
    "href": "reader/multiple-testing.html#false-discovery-rate-fdr",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "6 False Discovery Rate (FDR)",
    "text": "6 False Discovery Rate (FDR)\nProblem: With 10K independent test statistics, all at level \\(\\alpha = 0.001\\), we expect 10 rejections just by chance. What if we get 50? Probably only ~20 of them are false rejections.\nCan we accept 10 false rejections as long as most rejections are valid?\nBenjamini-Hochberg (1995) proposed a more liberal error control criterion called FDR:\n\\(R(X) = |R(X)|\\) = rejections (“discoveries”) \\(V(X) = |R(X) \\cap H_{0c}|\\) = false discoveries\nThe FDP is: \\(\\text{FDP} = \\begin{cases} V(X)/R(X) & \\text{if } R(X) &gt; 0 \\\\ 0 & \\text{if } R(X) = 0 \\end{cases}\\)\nThe FDR is \\(\\mathbb{E}[\\text{FDP}]\\)"
  },
  {
    "objectID": "reader/multiple-testing.html#benjamini-hochberg-procedure",
    "href": "reader/multiple-testing.html#benjamini-hochberg-procedure",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "7 Benjamini-Hochberg Procedure",
    "text": "7 Benjamini-Hochberg Procedure\nB-H also proposed a method to control FDR given ordered p-values \\(p_{(1)} \\leq p_{(2)} \\leq \\cdots \\leq p_{(m)}\\):\n\\(R(X) = \\max\\{r: p_{(r)} \\leq \\alpha r/m\\}\\) (called step-up procedure)\nReject \\(H_{0(1)},\\ldots,H_{0(R)}\\)\nThis is much more liberal than Bonferroni procedure: When \\(\\alpha = 0.05\\), B-H rejects at least \\(r\\) p-values if \\(p_{(r)} \\leq 0.05r/m\\)\n\n7.1 B-H as Empirical Bayes\nEquivalent formulation for \\(R(t) = \\#\\{p_i \\leq t\\}\\): Let \\(\\hat{F}(t) = R(t)/m\\) = estimate of CDF of p-values\nB-H rejects \\(H_i\\) if \\(p_i \\leq T(X) = \\max\\{t: \\hat{F}(t) \\geq t/\\alpha\\}\\)\nWhen \\(\\hat{F}(t)\\) is continuously increasing in \\(t\\) except at jump values where it jumps down:\n\\(\\hat{F}(T(X)) = T(X)/\\alpha\\)\n[Insert graph showing \\(\\hat{F}(t)\\) vs \\(t/\\alpha\\)]\nOnly values of \\(t\\) that matter for the algorithm are \\(t = p_i\\) where \\(\\hat{F}(t) = t/\\alpha\\), i.e., \\(\\alpha i/m = p_{(i)}\\)"
  },
  {
    "objectID": "reader/multiple-testing.html#fdr-control",
    "href": "reader/multiple-testing.html#fdr-control",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "8 FDR Control",
    "text": "8 FDR Control\nElegant but fragile proof due to Storey, Taylor, Siegmund (2002)\nAssume \\(\\#\\{i: H_{0i} \\text{ true}\\} = m_0\\) indep. \\(p_i \\sim U[0,1]\\) under \\(H_{0i}\\)\nLet \\(V(t) = \\#\\{i \\in H_{0c}: p_i \\leq t\\}\\)\n\\(\\text{FDR} = \\mathbb{E}[\\text{FDP}] = \\mathbb{E}\\left[\\frac{V(T)}{R(T)} \\cdot 1\\{R(T) &gt; 0\\}\\right]\\)\nThen FDR \\(= \\mathbb{E}[\\text{FDP}] = \\mathbb{E}[\\mathbb{E}[\\text{FDP} | T]] \\leq \\mathbb{E}[m_0T/(mT)] = \\alpha m_0/m \\leq \\alpha\\)\nNote: \\(Q(t) = V(t)/(mt)\\) is a martingale when \\(t\\) runs backwards from \\(t=1\\) to \\(t=0\\)\n\\(\\mathbb{E}[V(s) | V(t)] = V(t) \\cdot s/t\\)\n\\(\\mathbb{E}[1\\{p_i \\leq s\\} | 1\\{p_i \\leq t\\}, V(t)] = s/t \\cdot 1\\{p_i \\leq t\\}\\)\nAnd \\(T\\) is a stopping time w.r.t. the filtration \\(\\mathcal{F}_t\\) of \\(\\{V(t), R(t)\\}\\) (again, filtration with \\(t=1 \\to t=0\\))\nWhy? For \\(s&lt;t\\), \\(R(s) = \\#\\{i: p_i \\leq s\\}\\)\n\\(\\mathbb{E}[1\\{p_i \\leq s\\} | \\mathcal{F}_t] = s/t \\cdot 1\\{p_i \\leq t\\}\\)\n\\(\\hat{F}(s) = R(s)/m\\)\n[Insert graph showing \\(\\hat{F}(t)\\) vs \\(t/\\alpha\\) and \\(Q(t)\\)]\n\\(\\text{FDR} = \\alpha \\mathbb{E}[V(T)/(mT)] = \\alpha \\mathbb{E}[Q(T)] = \\alpha \\mathbb{E}[Q(1)] = \\alpha m_0/m\\)\n\n8.1 Remarks\n\nProof only works if p-values indep., null ones exactly uniform\nMore robust proof shows FDR controlled when null p-values conservative\nCan be extended to positive dependence\nFDR controlled under general dependence if we use corrected level \\(\\alpha m / (m+1-i)\\)\n\\(\\sum_{i=1}^m 1/i \\approx \\log m + 0.577\\)"
  },
  {
    "objectID": "reader/empirical-bayes.html",
    "href": "reader/empirical-bayes.html",
    "title": "Empirical Bayes and the James-Stein Paradox",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/empirical-bayes.html#empirical-bayes",
    "href": "reader/empirical-bayes.html#empirical-bayes",
    "title": "Empirical Bayes and the James-Stein Paradox",
    "section": "1 Empirical Bayes",
    "text": "1 Empirical Bayes\n\n1.1 Common Situation in Hierarchical Bayes Models\n\n\\(\\theta \\sim G\\), one draw: hard to justify prior\nLots of info: prior doesn’t matter\n\\(\\theta_i \\sim G\\), only \\(X_i\\) informative: prior helps\nMany draws: can check fit\n\n\\[X_i \\sim p_{\\theta_i}(x), \\quad i = 1,\\ldots,d\\]\n\n\n1.2 Hybrid Approach\nTreat \\(G\\) as fixed:\n\nEstimate \\(G\\) based on observed data\nPlug in \\(\\hat{G}\\) as though known\n\n\n\n1.3 Example\n\\(\\theta_i \\sim N(0, \\tau^2)\\), \\(\\tau^2\\) fixed unknown \\(X_i|\\theta_i \\sim N(\\theta_i, 1)\\), \\(i = 1,\\ldots,d\\)\nBayes estimator if we knew \\(\\tau^2\\) is:\n\\[\\delta(X) = \\frac{\\tau^2}{\\tau^2 + 1}X_i\\]\n\\(\\tau^2\\) is sufficient.\nTo estimate \\(\\tau^2\\), use \\(X \\sim N(0, \\tau^2 I_d + I_d)\\):\n\\[\\mathbb{E}\\|X\\|^2 = d(\\tau^2 + 1)\\]\n\\[\\hat{\\tau}^2 = \\max\\{\\frac{1}{d}\\|X\\|^2 - 1, 0\\}\\]\nPlug in: \\(\\hat{\\delta}(X) = (1 - \\frac{d}{\\|X\\|^2})_+ X_i\\)\nIf \\(d\\) large, should be near optimal."
  },
  {
    "objectID": "reader/empirical-bayes.html#james-stein-estimator",
    "href": "reader/empirical-bayes.html#james-stein-estimator",
    "title": "Empirical Bayes and the James-Stein Paradox",
    "section": "2 James-Stein Estimator",
    "text": "2 James-Stein Estimator\n\\[\\delta_{JS}(X) = (1 - \\frac{d-2}{\\|X\\|^2})X\\]\nProof: If \\(Y \\sim \\text{Gamma}(\\frac{d}{2}, \\frac{1}{2})\\), then:\n\\[\\mathbb{P}(Y &gt; \\frac{d-2}{2}) = 1\\]\nNow use \\(f(x) = x - \\frac{d-2}{x}\\), \\(\\frac{1}{2}\\|X\\|^2 \\sim \\text{Gamma}(\\frac{d}{2}, \\frac{1}{2})\\)\n\\[\\mathbb{E}[f(\\frac{1}{2}\\|X\\|^2)] &gt; f(\\mathbb{E}[\\frac{1}{2}\\|X\\|^2]) = \\frac{d}{2} - \\frac{d-2}{\\frac{d}{2}} = 1\\]\n\n2.1 James-Stein Paradox\nFor \\(d \\geq 3\\), the sample mean \\(\\bar{X} = \\frac{1}{n}\\sum X_i\\) is inadmissible as an estimator of \\(\\theta\\) under squared error loss.\nFor \\(\\delta_{JS}(X) = (1 - \\frac{d-2}{\\|X\\|^2})X\\):\n\\[\\text{MSE}(\\theta, \\delta_{JS}) &lt; \\text{MSE}(\\theta, \\bar{X}) \\quad \\forall \\theta \\in \\mathbb{R}^d\\]\nNotes: - \\(\\bar{X}\\) is UMVU, Minimax, objective Bayes - Might as well take \\(n=1\\) (Sufficiency reduction) - This result holds without assumption of Bayes model on \\(\\theta\\): true for \\(\\theta = (50, 10, 94, \\ldots)\\) - Nothing special about 0: for any \\(\\theta_0 \\in \\mathbb{R}^d\\), \\(\\delta(X) = \\theta_0 + (1 - \\frac{d-2}{\\|X-\\theta_0\\|^2})(X-\\theta_0)\\) also dominates \\(X\\)\nDeep implication: shrinkage makes sense even without Bayes justification.\n\n\n2.2 General Form\nLet \\(\\delta(X) = (1 - \\frac{c}{\\|X\\|^2})X\\), \\(c\\) is tuning parameter\n\\[R(\\theta, \\delta) = \\mathbb{E}_\\theta\\|\\delta(X) - \\theta\\|^2 = \\mathbb{E}_\\theta\\|X - \\theta\\|^2 + \\mathbb{E}_\\theta[\\frac{c^2}{\\|X\\|^2} - 2c]\\]\nWhat is optimal \\(c\\)?\n\\[R(\\theta, \\delta) = d + \\mathbb{E}_\\theta[\\frac{c^2}{\\|X\\|^2}] - 2c\\]\n\\[\\frac{\\partial R}{\\partial c} = 2\\mathbb{E}_\\theta[\\frac{c}{\\|X\\|^2}] - 2 = 0\\]\n\\[c = \\frac{\\mathbb{E}_\\theta[\\|X\\|^2]}{\\mathbb{E}_\\theta[\\frac{1}{\\|X\\|^2}]}\\]\n\\(c\\) always &gt; 0, but → 0 as \\(\\|\\theta\\| → \\infty\\)\nWhat if we estimate \\(c\\)? How does adaptivity of \\(c(X)\\) affect MSE?"
  },
  {
    "objectID": "reader/empirical-bayes.html#steins-lemma",
    "href": "reader/empirical-bayes.html#steins-lemma",
    "title": "Empirical Bayes and the James-Stein Paradox",
    "section": "3 Stein’s Lemma",
    "text": "3 Stein’s Lemma\nUseful tool for computing/estimating risk in Gaussian estimation problems.\n\n3.1 Theorem (Stein’s Lemma - Univariate)\nSuppose \\(X \\sim N(\\theta, \\sigma^2)\\) \\(h: \\mathbb{R} → \\mathbb{R}\\) differentiable, \\(\\mathbb{E}|h'(X)| &lt; \\infty\\)\nThen \\(\\mathbb{E}[(X-\\theta)h(X)] = \\sigma^2\\mathbb{E}[h'(X)]\\)\n\\(\\text{Cov}(X, h(X)) = \\sigma^2\\mathbb{E}[h'(X)]\\)\nProof: Note we can assume w.l.o.g. \\(h(0) = 0\\) (why?) First assume \\(\\theta = 0\\), \\(\\sigma^2 = 1\\)\n\\[\\mathbb{E}[Xh(X)] = \\int xh(x)\\phi(x)dx = \\int xh(x)\\phi(x)dx - \\int h(x)\\phi'(x)dx = \\int h'(x)\\phi(x)dx\\]\nIn the last step we have used \\(\\phi'(x) = -x\\phi(x)\\)\nSimilar argument shows \\(\\int h(x)\\phi(x)dx = \\int h'(x)\\phi(x)dx\\)\nResult holds for \\(\\theta = 0\\), \\(\\sigma^2 = 1\\)\nGeneral \\(\\theta\\), \\(\\sigma^2 \\neq 1\\): write \\(X = \\theta + \\sigma Z\\), \\(Z \\sim N(0,1)\\)\n\\[\\mathbb{E}[(X-\\theta)h(X)] = \\sigma\\mathbb{E}[Zh(\\theta+\\sigma Z)] = \\sigma^2\\mathbb{E}[h'(\\theta+\\sigma Z)] = \\sigma^2\\mathbb{E}[h'(X)]\\]\n\n\n3.2 Multivariate Version\nDefine Frobenius norm: \\(\\|A\\|_F^2 = \\sum_{i,j} A_{ij}^2 = \\text{tr}(A^TA)\\)\n\n\n3.3 Theorem (Stein’s Lemma - Multivariate)\n\\(X \\sim N_d(\\theta, \\sigma^2 I_d)\\), \\(\\theta \\in \\mathbb{R}^d\\) \\(h: \\mathbb{R}^d → \\mathbb{R}^d\\) differentiable, \\(\\mathbb{E}\\|Dh(X)\\|_F &lt; \\infty\\)\nThen \\(\\mathbb{E}[(X-\\theta)^Th(X)] = \\sigma^2\\mathbb{E}[\\text{tr}(Dh(X))]\\)\n\\(\\mathbb{E}[(X-\\theta)h(X)^T] = \\sigma^2\\mathbb{E}[Dh(X)]\\)\nProof:\n\\[\\mathbb{E}[(X_i-\\theta_i)h_i(X)] = \\mathbb{E}[\\mathbb{E}[(X_i-\\theta_i)h_i(X)|X_{-i}]]\\] \\[= \\sigma^2\\mathbb{E}[\\frac{\\partial h_i}{\\partial x_i}(X)]\\]"
  },
  {
    "objectID": "reader/empirical-bayes.html#steins-unbiased-risk-estimator-sure",
    "href": "reader/empirical-bayes.html#steins-unbiased-risk-estimator-sure",
    "title": "Empirical Bayes and the James-Stein Paradox",
    "section": "4 Stein’s Unbiased Risk Estimator (SURE)",
    "text": "4 Stein’s Unbiased Risk Estimator (SURE)\nUnbiased estimator of the MSE of any \\(\\delta(X)\\)\nApply Stein’s Lemma with \\(h(x) = X - \\delta(x)\\)\nAssume \\(\\sigma^2 = 1\\):\n\\[R(\\theta, \\delta) = \\mathbb{E}_\\theta\\|\\delta(X) - \\theta\\|^2 = \\mathbb{E}_\\theta\\|X - \\theta\\|^2 + \\mathbb{E}_\\theta\\|\\delta(X) - X\\|^2 - 2\\mathbb{E}_\\theta[(X-\\theta)^T(X-\\delta(X))]\\] \\[= d + \\mathbb{E}_\\theta\\|\\delta(X) - X\\|^2 - 2\\mathbb{E}_\\theta[\\text{tr}(D(X-\\delta(X)))]\\]\n\\[\\hat{R}(X) = d + \\|\\delta(X) - X\\|^2 - 2\\text{tr}(D\\delta(X))\\]\nis unbiased for the MSE estimator \\(\\delta(X)\\)\nOnly depends on X\nCan also compute MSE via \\(R = \\mathbb{E}_\\theta[\\hat{R}]\\)\n\\[\\mathbb{E}_\\theta[\\|\\delta(X) - h(X)\\|^2] = \\mathbb{E}_\\theta[\\|\\delta(X) - \\theta\\|^2] + \\mathbb{E}_\\theta[\\|h(X) - \\theta\\|^2] - 2\\mathbb{E}_\\theta[(\\delta(X) - \\theta)^T(h(X) - \\theta)]\\]\n\\[R = d + R(\\theta, \\delta) - 2\\mathbb{E}_\\theta[\\text{tr}(D\\delta(X))]\\]\nExample: \\(\\delta(X) = (1-c)X\\) for fixed \\(c\\)\n\\(h(x) = cX\\), \\(Dh = cI_d\\)\n\\(\\hat{R} = d + c^2\\|X\\|^2 - 2cd\\)\n\n4.1 Risk of James-Stein\n\\(\\delta_{JS}(X) = (1 - \\frac{d-2}{\\|X\\|^2})X\\)\n\\(h(X) = \\frac{d-2}{\\|X\\|^2}X\\), \\(Dh(X) = \\frac{d-2}{\\|X\\|^2}I_d - 2(d-2)\\frac{XX^T}{\\|X\\|^4}\\)\n\\(\\|h(X)\\|^2 = \\frac{(d-2)^2}{\\|X\\|^2}\\)\n\\[\\text{tr}(Dh(X)) = \\frac{d(d-2)}{\\|X\\|^2} - \\frac{2(d-2)}{\\|X\\|^2} = \\frac{(d-2)^2}{\\|X\\|^2}\\]\n\\[\\hat{R} = d + \\frac{(d-2)^2}{\\|X\\|^2} - 2\\frac{(d-2)^2}{\\|X\\|^2} = d - \\frac{(d-2)^2}{\\|X\\|^2}\\]\n\\[R(\\theta) = \\mathbb{E}_\\theta[\\hat{R}] = d - (d-2)^2\\mathbb{E}_\\theta[\\frac{1}{\\|X\\|^2}]\\]\nIf \\(\\theta = 0\\) then \\(\\mathbb{E}_0[\\frac{1}{\\|X\\|^2}] = \\frac{1}{d-2}\\)\n\\[R(0) = d - (d-2) = 2\\]\nPossibly surprising\nIf \\(\\theta \\neq 0\\) then \\(\\mathbb{E}_\\theta[\\frac{1}{\\|X\\|^2}] &lt; \\frac{1}{\\|\\theta\\|^2}\\)\n\\[R(\\theta) &lt; d - \\frac{(d-2)^2}{\\|\\theta\\|^2 + d}\\]\nSmaller and smaller advantage, but always better\nNote: \\(\\delta_{JS}(X)\\) also inadmissible\n\\(\\delta_{+}(X) = (1 - \\frac{d-2}{\\|X\\|^2})_+ X\\) is strictly better\nPractically more useful version:\n\\[\\delta_{JS+}(X) = (1 - \\frac{d-3}{\\|X\\|^2})_+ X\\]\nDominates \\(\\delta(X) = X\\) for \\(d \\geq 4\\)\nTaken to logical extreme, suggestion seems dumb: should everyone at Berkeley pool their estimates?\nNote: \\(\\mathbb{E}\\|\\hat{\\theta}\\|^2\\) is improved, but \\(\\mathbb{E}[(\\hat{\\theta}_i - \\theta_i)^2]\\) may get worse for individual coordinates."
  },
  {
    "objectID": "reader/likelihood-inference.html",
    "href": "reader/likelihood-inference.html",
    "title": "Likelihood-Based Inference: Wald, Score, and Generalized Likelihood Ratio Tests",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/likelihood-inference.html#likelihood-based-inference",
    "href": "reader/likelihood-inference.html#likelihood-based-inference",
    "title": "Likelihood-Based Inference: Wald, Score, and Generalized Likelihood Ratio Tests",
    "section": "1 Likelihood-Based Inference",
    "text": "1 Likelihood-Based Inference\n\n1.1 Setting\n\\(X_1, \\ldots, X_n \\stackrel{\\text{iid}}{\\sim} p_\\theta(x)\\), \\(p_\\theta \\in \\cP\\), smooth in \\(\\theta\\)\nAssume: - \\(\\mathbb{E}_\\theta[\\nabla \\ell_\\theta(X)] = 0\\) - \\(\\text{Var}_\\theta[\\nabla \\ell_\\theta(X)] = \\mathbb{E}_\\theta[-\\nabla^2 \\ell_\\theta(X)] = J(\\theta) &gt; 0\\) - MLE \\(\\hat{\\theta}\\) Consistent\nThen if \\(\\theta = \\theta_0\\): - \\(\\nabla \\ell_n(\\theta_0; X) \\sim N(0, nJ(\\theta_0))\\) - \\(-\\nabla^2 \\ell_n(\\theta_0; X) \\xrightarrow{p} nJ(\\theta_0)\\)\nUsed \\(\\theta = \\hat{\\theta} + J^{-1}(\\theta_0) \\nabla \\ell_n(\\theta_0; X)/n + o_p(n^{-1/2})\\) to get \\(\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\sim N(0, J^{-1}(\\theta_0))\\)\nCan use this for inference on \\(\\theta_0\\)"
  },
  {
    "objectID": "reader/likelihood-inference.html#wald-type-confidence-regions",
    "href": "reader/likelihood-inference.html#wald-type-confidence-regions",
    "title": "Likelihood-Based Inference: Wald, Score, and Generalized Likelihood Ratio Tests",
    "section": "2 Wald-Type Confidence Regions",
    "text": "2 Wald-Type Confidence Regions\nAssume we have some estimator \\(\\hat{\\theta}_n\\) s.t. \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\xrightarrow{d} N(0, J^{-1}(\\theta_0))\\). Then we can plug in:\nIf \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\sim N_d(0, J^{-1}(\\theta_0))\\), then \\(nJ(\\theta_0)(\\hat{\\theta}_n - \\theta_0) \\sim N_d(0, I_d)\\)\nSo \\(n(\\hat{\\theta}_n - \\theta_0)^T J(\\theta_0)(\\hat{\\theta}_n - \\theta_0) \\sim \\chi^2_d\\) (Slutsky)\nLeads to test of \\(H_0: \\theta = \\theta_0\\) \\(H_1: \\theta \\neq \\theta_0\\): Reject if \\(n(\\hat{\\theta}_n - \\theta_0)^T J(\\theta_0)(\\hat{\\theta}_n - \\theta_0) &gt; \\chi^2_{d,1-\\alpha}\\)\nSo \\(\\mathbb{P}_{\\theta_0}(n(\\hat{\\theta}_n - \\theta_0)^T J(\\theta_0)(\\hat{\\theta}_n - \\theta_0) \\leq \\chi^2_{d,1-\\alpha}) = 1-\\alpha\\)\nNote: we reject \\(\\theta_0\\) iff \\(\\{\\theta_n: n(\\hat{\\theta}_n - \\theta)^T J(\\theta)(\\hat{\\theta}_n - \\theta) \\leq \\chi^2_{d,1-\\alpha}\\}\\) reject \\(\\theta_0\\) iff \\(\\theta_0 \\notin \\{\\theta: n(\\hat{\\theta}_n - \\theta)^T J(\\theta)(\\hat{\\theta}_n - \\theta) \\leq \\chi^2_{d,1-\\alpha}\\}\\)\nRegion \\(\\{\\theta: n(\\hat{\\theta}_n - \\theta)^T J(\\theta)(\\hat{\\theta}_n - \\theta) \\leq \\chi^2_{d,1-\\alpha}\\}\\) is confidence ellipsoid\nMore info = smaller ellipse (shrinks like \\(\\sqrt{n}\\))\n\n2.1 Estimating \\(J(\\theta)\\)\nTwo options is to plug-in the MLE: 1. MLE for \\(J_n(\\theta)\\): \\(J_n(\\hat{\\theta}_n) = -\\frac{1}{n}\\nabla^2 \\ell_n(\\hat{\\theta}_n; X)\\) 2. \\(\\hat{J}_n(\\theta) = \\frac{1}{n}\\text{Var}_\\theta[\\nabla \\ell_n(\\theta; X)] = \\frac{1}{n}\\sum_{i=1}^n \\nabla \\ell_\\theta(X_i) \\nabla \\ell_\\theta(X_i)^T\\)\nNB: \\(\\text{Var}_\\theta[\\nabla \\ell_n(\\theta; X)] = n\\text{Var}_\\theta[\\nabla \\ell_\\theta(X)] = 0\\)\nOr \\(\\hat{J}_n = \\mathbb{E}_{\\hat{\\theta}_n}[-\\nabla^2 \\ell_{\\hat{\\theta}_n}(X)]\\)\n\n2.1.1 Remarks\n\nBoth have \\(\\hat{J}_n \\xrightarrow{p} J(\\theta_0)\\) in nice iid sampling setting\nBoth make sense outside of iid setting\nHeuristically: plug-in measures info about \\(\\theta\\) in typical data set, but obs info measures info about \\(\\theta\\) in this data set\n\n\n\n\n2.2 Wald Interval for \\(\\theta_j\\)\nIf \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\sim N_d(\\theta_0, J_n^{-1}(\\theta_0))\\) then \\(\\hat{\\theta}_n \\sim N_d(\\theta_0, J_n^{-1}(\\theta_0)/n)\\)\nLeads to univariate interval: \\(s.e.(\\hat{\\theta}_{n,j}) = \\sqrt{[J_n^{-1}(\\hat{\\theta}_n)]_{jj}/n}\\)\n\\(C_j = [\\hat{\\theta}_{n,j} \\pm z_{1-\\alpha/2} \\cdot s.e.(\\hat{\\theta}_{n,j})]\\)\nglm function in R uses these intervals/p-values with \\(\\hat{J}_n = J_n(\\hat{\\theta}_n)\\)\nConf ellipsoid for \\(\\theta_0\\): \\(\\{\\theta: n(\\hat{\\theta}_n - \\theta)^T \\hat{J}_n(\\hat{\\theta}_n)(\\hat{\\theta}_n - \\theta) \\leq \\chi^2_{d,1-\\alpha}\\}\\)\nMore generally, if \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\xrightarrow{d} N(0, \\Sigma(\\theta_0))\\) and \\(\\hat{\\Sigma}_n(\\theta) \\xrightarrow{p} \\Sigma(\\theta_0)\\) (not nec. MLE) then we can do the same things\n\n\n2.3 Example: Generalized Linear Model with Fixed Design\n\\(X_1, \\ldots, X_n \\in \\mathbb{R}^d\\) fixed \\(Y_1, \\ldots, Y_n \\sim p_{\\eta_i}(y)\\) indep, \\(Y_i | X_i \\sim p_{\\eta_i}(y)\\) \\(\\eta_i = \\beta^T X_i\\) (canonical form)\nLet \\(\\mu_i(\\beta) = \\mathbb{E}_\\beta[Y_i] = \\psi'(\\eta_i)\\)\nMore general: \\(\\eta_i = f(\\beta^T X_i)\\) for \\(f\\) monotone\nMost common examples include: - Logistic regression: \\(Y_i \\sim \\text{Bern}(e^{\\eta_i}/(1+e^{\\eta_i}))\\) - Poisson log-linear model: \\(Y_i \\sim \\text{Pois}(e^{\\eta_i})\\)\n\\(\\ell_n(\\beta; Y) = \\sum_{i=1}^n [Y_i \\eta_i - \\psi(\\eta_i) + \\log h(Y_i)]\\)\n\\(\\nabla \\ell_n(\\beta; Y) = \\sum_{i=1}^n (Y_i - \\mu_i(\\beta)) X_i\\)\n\\(\\mathbb{E}_\\beta[Y_i] = \\mu_i(\\beta) = \\psi'(\\eta_i)\\)\n\\(\\nabla^2 \\ell_n(\\beta; Y) = -\\sum_{i=1}^n \\psi''(\\eta_i) X_i X_i^T\\)\n\\(\\text{Var}_\\beta(Y_i) = \\psi''(\\eta_i)\\) (not random)\n\\(\\hat{\\beta} \\stackrel{\\cdot}{\\sim} N(\\beta, J_n^{-1}(\\beta))\\) in finite samples \\(\\xrightarrow{d} N(0, J^{-1})\\)\nUnder regularity cond. on \\(X\\): Taylor expansion of \\(\\ell_n\\) leads to \\(\\sqrt{n}(\\hat{\\beta}_n - \\beta) \\xrightarrow{d} N(0, J^{-1})\\)\n\n2.3.1 Advantages of Wald Test\n\nEasy to invert, simple conf regions\nAsymptotically correct\n\n\n\n2.3.2 Disadvantages\n\nHave to compute MLE\nDepends on parameterization\nRelies on two approximations: \\(\\ell_n\\) Normal and \\(\\ell_n\\) quadratic\nNeed MLE to be consistent\nConfidence interval/ellipsoid might go outside \\(\\Theta\\)"
  },
  {
    "objectID": "reader/likelihood-inference.html#score-test",
    "href": "reader/likelihood-inference.html#score-test",
    "title": "Likelihood-Based Inference: Wald, Score, and Generalized Likelihood Ratio Tests",
    "section": "3 Score Test",
    "text": "3 Score Test\nTest \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta \\neq \\theta_0\\)\nWe can bypass quadratic approximation entirely by using score as test stat:\n\\(\\nabla \\ell_n(\\theta_0; X) \\sim N(0, nJ(\\theta_0))\\) or \\(J_n^{-1/2}(\\theta_0)\\nabla \\ell_n(\\theta_0; X) \\stackrel{\\cdot}{\\sim} N_d(0, I_d)\\)\nSo we can reject \\(H_0: \\theta = \\theta_0\\) if \\(\\|\\hat{J}_n^{-1/2}(\\theta_0)\\nabla \\ell_n(\\theta_0; X)\\|^2 &gt; \\chi^2_{d,1-\\alpha}\\)\n\\(\\nabla \\ell_n(\\theta_0; X)^T \\hat{J}_n^{-1}(\\theta_0) \\nabla \\ell_n(\\theta_0; X) \\sim \\chi^2_d\\)\nCan do 1-sided tests\n\n3.1 Remarks\n\nNo quadratic approx, no MLE\nNo need to estimate Fisher info at \\(\\theta_0\\)\nCan be generalized to case with nuisance params\nTypically estimate via MLE on \\(\\Theta_0\\)\n\n\n\n3.2 Score Test is Invariant to Reparameterization\nAssume \\(\\Theta \\subset \\mathbb{R}^d\\), \\(\\eta = g(\\theta)\\), \\(\\Psi = g(\\Theta)\\)\n\\(q_\\eta(x) = p_{g^{-1}(\\eta)}(x)\\)\n\\(\\ell_\\eta(x) = \\log q_\\eta(x) = \\ell_\\theta(x)\\)\n\\(\\nabla_\\eta \\ell_\\eta(x) = \\nabla_\\theta \\ell_\\theta(x) \\cdot \\nabla g^{-1}(\\eta)\\)\n\\(J_\\eta(\\eta) = J_\\theta(g^{-1}(\\eta)) \\cdot \\nabla g^{-1}(\\eta) \\cdot \\nabla g^{-1}(\\eta)^T\\)\nSo \\(\\nabla_\\eta \\ell_\\eta(x)^T J_\\eta^{-1}(\\eta) \\nabla_\\eta \\ell_\\eta(x) = \\nabla_\\theta \\ell_\\theta(x)^T J_\\theta^{-1}(\\theta) \\nabla_\\theta \\ell_\\theta(x)\\)\nif \\(\\eta_0 = g(\\theta_0)\\)\n\n\n3.3 Example: 1-Parameter Exponential Family\n\\(X_1, \\ldots, X_n \\stackrel{\\text{iid}}{\\sim} e^{\\eta T(x) - A(\\eta)} h(x)\\)\n\\(\\nabla \\ell_n(\\eta; X) = \\sum T(X_i) - n\\mu(\\eta)\\)\n\\(\\ell_n''(\\eta; X) = -n\\text{Var}_\\eta[T(X)]\\)\n\\(\\hat{\\eta}_n = \\text{MLE} = A'^{-1}(\\bar{T})\\)\n\\(\\frac{\\sum T(X_i) - n\\mu(\\eta_0)}{\\sqrt{n\\text{Var}_{\\eta_0}[T(X)]}} \\sim N(0,1)\\)\n\n\n3.4 Example: \\(X_1, \\ldots, X_n \\stackrel{\\text{iid}}{\\sim} \\text{Laplace}(\\theta, 2\\sqrt{2})\\)\nTest \\(H_0: \\theta = 0\\) vs \\(H_1: \\theta \\neq 0\\) (two-tailed)\n\\(\\ell_n(\\theta; X) = \\sum |X_i - \\theta| - n\\log(4\\sqrt{2})\\)\n\\(\\nabla \\ell_n(\\theta; X) = \\sum \\text{sgn}(\\theta - X_i) = \\sum [\\mathbb{I}(X_i &lt; \\theta) - \\mathbb{I}(X_i &gt; \\theta)]\\)\n\\(\\nabla \\ell_n(0; X) = \\sum [\\mathbb{I}(X_i &lt; 0) - \\mathbb{I}(X_i &gt; 0)] = \\sum \\text{sgn}(-X_i)\\)\n\\(J_n(0) = n/2\\)\n\\(\\sqrt{2/n} \\sum \\text{sgn}(-X_i) \\sim N(0,1)\\) (sign test)\nNote: this test is the exact NP/UMP test for \\(H_0: \\theta = 0\\) vs \\(H_0: |\\theta| = \\epsilon\\) for \\(\\epsilon &gt; 0\\)\nIntuition: Maximize power for nearby alternatives since we’ll have power for \\(\\theta \\gg 0\\)\nMore generally, one-sided score test is almost UMP for nearby alternatives\n\\(p_\\theta(x) \\approx p_0(x)[1 + \\epsilon \\ell'_0(X)]\\) for small \\(\\epsilon &gt; 0\\)\n\n\n3.5 Example: Pearson’s \\(\\chi^2\\) Test (Goodness of Fit)\n\\(N = (N_1, \\ldots, N_d) \\sim \\text{Multi}(n, \\pi)\\), \\(\\pi_i \\geq 0\\), \\(\\sum \\pi_i = 1\\)\n\\(\\ell_n(\\pi; N) = \\sum N_i \\log \\pi_i\\)\nNote \\(\\mathbb{E}[\\pi] = 1\\) so this is a full rank \\(d-1\\) parameter exp family e.g. \\(T_j = \\mathbb{I}(\\text{category} = j)\\), \\(j=1,\\ldots,d-1\\)\n\\(\\nabla \\ell_n(\\pi; N) = (N_1/\\pi_1, \\ldots, N_d/\\pi_d)^T - n1_d\\)\n\\(\\hat{\\pi} = \\text{MLE} = (N_1/n, \\ldots, N_d/n)\\)\n$J_n() = n[(_1^{-1}, , _d^{-1})"
  },
  {
    "objectID": "reader/exponential-families.html",
    "href": "reader/exponential-families.html",
    "title": "Exponential Families",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/exponential-families.html#exponential-family-structure",
    "href": "reader/exponential-families.html#exponential-family-structure",
    "title": "Exponential Families",
    "section": "Exponential family structure",
    "text": "Exponential family structure\nThus far we have discussed statistical models in the abstract without making any assumptions about them. Exponential families, the topic of this lecture, are models with a special structure that makes them especially easy to work with.\n\nWe say the model \\(\\cP = \\{P_\\eta:\\eta \\in \\Xi\\}\\) is an \\(s\\)-parameter exponential family if it is defined by a family of densities of the form:\n\\[\np_\\eta(x) = e^{\\eta'T(x) - A(\\eta)}h(x),\n\\tag{1}\\]\nall with respect to a common dominating measure \\(\\mu\\), i.e. a measure \\(\\mu\\) such that \\(P_\\eta \\ll \\mu\\) for all \\(\\eta\\in \\Xi\\).\nThe different parts of the expression in Equation 1 have distinct names referring to the role they play in defining the density:\n\\[\n\\begin{aligned}\nT&:\\; \\cX \\to \\RR^s &\\qquad &\\text{ is called the {\\it sufficient statistic}}\\\\\nh&:\\; \\cX \\to [0,\\infty) &\\qquad &\\text{ is called the {\\it carrier density} or {\\it base density}}\\\\\n\\eta &\\in \\Xi \\subseteq \\RR^s &\\qquad &\\text{ is called the {\\it natural parameter}}\\\\\nA&:\\; \\Xi \\to \\RR &\\qquad &\\text{ is called the {\\it log-partition function}}\n\\end{aligned}\n\\]\nThe function \\(A(\\eta)\\) is totally determined by \\(T\\) and \\(h\\), and plays the role of normalizing the density so that \\(P_\\eta(\\cX) = 1\\):\n\\[\nA(\\eta) = \\log\\left( \\int_\\cX e^{\\eta'T(x)}h(x)\\td\\mu(x)\\right) \\leq \\infty\n\\tag{2}\\]\nIf \\(A(\\eta) = \\infty\\) then there is no way to normalize \\(p_\\eta\\), so \\(\\eta\\) is not an allowed value for the natural parameter to take. The natural parameter space, which we denote \\(\\Xi_1\\), is the set of all \\(\\eta\\) values for which the family is normalizable:\n\\[\n\\Xi_1 = \\{\\eta:\\; A(\\eta) &lt; \\infty\\} \\subseteq \\RR^s.\n\\]\nWe will show in Homework 1 that \\(A(\\eta)\\) is a convex function, so \\(\\Xi_1\\) is a convex set.\nNote that, because \\(\\mu\\) is allowed to be an arbitrary measure, \\(h(x)\\) also plays an inessential role since we could always absorb it into the base measure \\(\\mu\\). More precisely, suppose we define a new dominating measure \\(\\nu\\) whose density with respect to \\(\\mu\\) is \\(h\\) (informally we can write \\(\\td\\nu = h\\td\\mu\\)). Then \\(P_\\eta\\), which had density \\(e^{\\eta'T(x)}h(x)\\) with respect to \\(\\mu\\), now has density \\(e^{\\eta'T(x)}\\) with respect to \\(\\nu\\).\nAs a result, if we were aiming for maximal parsimony we could assume without loss of generality that \\(h(x) = 1\\), removing it from the definition Equation 1 and replacing \\(\\mu\\) with our new, bespoke measure \\(\\nu\\). However, it is convenient to leave Equation 1 as is if it allows us to take \\(\\mu\\) to be some simple default measure like a counting measure or Lebesgue measure. In that case, \\(p_\\eta\\) will be a standard pmf or pdf, so we can discuss it without going over the head of anyone who lacks a background in measure theory.\nExample (Poisson): The Poisson distribution \\(\\textrm{Pois}(\\lambda)\\) has probability mass function\n\\[\np_\\lambda(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad \\textrm{ on } x = 0, 1, 2, \\ldots.\n\\]\nFormally this pmf is a density over the counting measure on the set of non-negative integers \\(\\ZZ_+ = \\{0,1,\\ldots\\}\\).\nLetting \\(\\lambda\\) range over \\([0, \\infty)\\) yields an exponential family, but it is not immediately obvious from the form of the density. To see this, we need to massage \\(p_\\lambda(x)\\) a bit, by observing that\n\\[\np_\\lambda(x) = \\exp\\{(\\log \\lambda) x - \\lambda\\}\\frac{1}{x!}.\n\\]\nNow we see that we can reparameterize the family by setting \\(\\eta = \\log\\lambda\\), leading to\n\\[\np_\\eta(x) = \\exp\\{\\eta x - e^\\eta\\} \\frac{1}{x!}.\n\\] At this point, we immediately recognize \\(p_\\eta\\) as an exponential family with sufficient statistic \\(T(x) = x\\), and base density \\(h(x) = \\frac{1}{x!}\\), and log-partition function \\(A(\\eta) = e^{\\eta}\\).\nNote that this is not the only way to decompose the Poisson distribution as an exponential family. For example, we could just as well take \\(T(x) = x/2\\); then we would have \\(\\eta = 2\\log\\lambda\\) and \\(A(\\eta) = e^{\\eta/2}\\). Or, we could take \\(T(x) = x + 1\\) and \\(A(\\eta) = e^{\\eta} + \\eta\\).\nThis is not just a property of the Poisson distribution; the decomposition is generally non-unique for exponential families. For any exponential family of the form Equation 1 , if \\(U \\in \\RR^{s\\times s}\\) is invertible and \\(v \\in \\RR^s\\) then we can write the same density as \\(e^{\\zeta'S(x) - B(\\zeta)}h(x)\\), for new sufficient statistic \\(S(x) = U T(x) + v\\), new natural parameter \\(\\zeta = (U^{-1})'\\eta\\), and and new log-partition function \\(B(\\zeta) = A(U'\\zeta) + \\zeta'U^{-1}v\\). So “the” sufficient statistic of an exponential family is defined only up to invertible affine transformations."
  },
  {
    "objectID": "reader/exponential-families.html#differential-identities",
    "href": "reader/exponential-families.html#differential-identities",
    "title": "Exponential Families",
    "section": "Differential identities",
    "text": "Differential identities\nExponentiating Equation 2, we obtain the equation\n\\[\ne^{A(\\eta)} = \\int_\\cX e^{\\eta'T(x)}h(x)\\td\\mu(x)\n\\tag{3}\\]\nWe can derive many interesting identities by differentiating this function, and related functions, with respect to \\(\\eta\\). We will always evaluate derivatives by differentiating under the integral sign. This is not always a correct operation, but by Theorem 2.4 in Keener, it is correct on the interior of the natural parameter space \\(\\Xi_1\\). We refer the reader to Keener for details.\n\nMean of \\(T(X)\\)\nPartially differentiating Equation 3 once with respect to a generic coordinate \\(\\eta_j\\), for \\(j =1, \\ldots, s\\), we obtain\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\eta_j} e^{A(\\eta)} &= \\int_\\cX \\frac{\\partial}{\\partial \\eta_j} e^{\\eta'T(x)}h(x)\\td\\mu(x)\\\\[7pt]\ne^{A(\\eta)} \\frac{\\partial A}{\\partial \\eta_j}(\\eta) &= \\int_\\cX T_j(x) e^{\\eta'T(x)}h(x)\\td\\mu(x)\\\\[7pt]\n\\frac{\\partial A}{\\partial \\eta_j}(\\eta) &= \\int_\\cX T_j(x) e^{\\eta'T(x) - A(\\eta)}h(x)\\td\\mu(x)\\\\[7pt]\n&= \\EE_\\eta \\left[\\,T_j(X)\\,\\right]\\\\\n\\end{aligned}\n\\]\nArranging these partial derivatives into a vector, we obtain\n\\[\n\\nabla A(\\eta) = \\EE_\\eta\\left[\\,T(X)\\,\\right],\n\\]\nwhich gives us a very convenient method for evaluating the expectation of the sufficient statistic.\n\n\nVariance of \\(T(X)\\)\nPushing our luck further, we can take a second partial derivative:\n\\[\n\\begin{aligned}\n\\frac{\\partial^2}{\\partial \\eta_j\\partial \\eta_k} e^{A(\\eta)} &= \\int_\\cX \\frac{\\partial^2}{\\partial \\eta_j\\partial \\eta_k} e^{\\eta'T(x)}h(x)\\td\\mu(x)\\\\[7pt]\ne^{A(\\eta)}\\left(\\frac{\\partial^2 A}{\\partial \\eta_j\\partial \\eta_k} + \\frac{\\partial A}{\\partial \\eta_j} \\frac{\\partial A}{\\partial \\eta_k} \\right)&= \\int_\\cX T_j(x) T_k(x)e^{\\eta'T(x)}h(x)\\td\\mu(x)\\\\[7pt]\n\\frac{\\partial^2 A}{\\partial \\eta_j\\partial \\eta_k} + \\EE_\\eta[T_j(X)]\\EE_\\eta[T_k(X)] &= \\EE_\\eta \\left[\\,T_j(X) T_k(X)\\,\\right]\\\\\n\\frac{\\partial^2 A}{\\partial \\eta_j\\partial \\eta_k} &= \\text{Cov}_\\eta\\left(T_j(X), T_k(X)\\right).\n\\end{aligned}\n\\]\nAgain, collecting these second partials into a Hessian matrix gives us\n\\[\n\\nabla^2 A(\\eta) = \\text{Var}_\\eta(T(X)),\n\\]\nwhere the right-hand side denotes the \\(s\\times s\\) variance-covariance matrix of the random vector \\(T(X)\\).\nExample (Poisson, continued): As we showed above, in the Poisson exponential family the sufficient statistic is \\(T(X)=X\\), the natural parameter is \\(\\eta = \\log\\lambda\\), and the log-partition function is \\(A(\\eta) = e^\\eta \\;(=\\lambda)\\).\nNote: This calculation would not have worked correctly if we had instead said \\(A(\\eta) = \\lambda\\), and differentiated that expression with respect to \\(\\lambda\\). We would then get \\(\\EE_\\eta[X] = 1\\) and \\(\\text{Var}_\\eta(X) = 0\\), which are clearly incorrect.\n\n\nMoment-generating function and cumulant-generating function\nThe moment generating function (MGF) of a \\(d\\)-dimensional random vector \\(X\\sim P\\) is defined as \\(M^X(u) = \\EE[e^{u'X}]\\), for \\(u\\in \\RR^d\\). If the MGF is well-defined in a neighborhood of \\(u=0\\), then we can use it to calculated moments of \\(X\\) by evaluating its derivatives at 0.\nWe can show this using manipulations very similar to the ones we saw above. To evaluate the first moment of \\(X_j\\), we can differentiate once with respect to \\(u_j\\), since\n\\[\n\\frac{\\partial}{\\partial u_j} M^X(u) = \\int_\\cX \\frac{\\partial}{\\partial u_j} e^{u'x}\\td P(x) = \\int_\\cX x_j e^{u'x}\\td P(x) = \\int_\\cX x_j e^{u'x} \\td P(x).\n\\]\nHere we have again assumed that we can differentiate under the integral sign; this is a technical condition that we would check if we were being more careful.\nEvaluating the derivative at \\(u=0\\), we obtain \\(\\frac{\\partial}{\\partial u_j} M^X(0) = \\int_\\cX x_j \\td P(x) = \\EE[X_j]\\). Moreover, we can repeat this trick as many times as we want, leading to a formula for mixed partial derivatives of any order:\n\\[\n\\left.\\frac{\\partial^{m_1 + \\cdots + m_d}}{\\partial u_1^{m_1}\\cdots\\partial u_d^{m_d}} M^X(u)\\right|_{u=0} = \\left.\\int_{\\cX} x_1^{m_1}\\cdots x_d^{m_d} e^{u'x}\\td P(x)\\right|_{u=0} = \\EE\\left[\\,X_1^{m_1} \\cdots X_d^{m_d}\\,\\right].\n\\tag{4}\\]\nThe MGF is therefore very useful for evaluating moments of \\(X\\). It is also useful for finding distributions of sums of independent random variables, because \\(M^{X + Y}(u) = M^X(u)M^Y(u),\\) if \\(X\\) and \\(Y\\) are independent. If two random variables have the same MGF then they have the same distribution.\nIn an exponential family, the MGF of \\(T(X)\\), under sampling from \\(P_\\eta\\), is simple to evaluate:\n\\[\n\\begin{aligned}\nM^{T(X)}_\\eta(u) &= \\EE_\\eta\\left[\\,e^{u'T(X)}\\,\\right]\\\\[5pt]\n&= \\int_\\cX e^{u'T(x)}e^{\\eta'T(x) - A(\\eta)}h(x)\\td\\mu(x) \\\\[5pt]\n&= e^{-A(\\eta)}\\int_\\cX e^{(u+\\eta)'T(x)} h(x)\\td\\mu(x)\\\\[5pt]\n&= e^{A(\\eta+u)-A(\\eta)}\n\\end{aligned}\n\\]\nExample (Poisson, continued): The MGF for \\(X \\sim \\text{Pois}(\\lambda)\\), with \\(\\eta = \\log\\lambda\\), is\n\\[\nM^{X}_\\eta(u) = \\exp\\{e^{\\eta + u} - e^{\\eta}\\} = \\exp\\{\\lambda (e^u - 1)\\}\n\\]\nTo see how the MGF is useful, suppose we have \\(X_i \\sim \\text{Pois}(\\lambda_i)\\), independently for \\(i = 1,2,\\ldots,n\\), and we want to know the distribution of \\(X_+ = \\sum_i X_i\\). Then we can multiply the MGF’s for \\(X_1,\\ldots,X_n\\) together to obtain the MGF for \\(X_+\\):\n\\[\nM^{X_+}(u) = \\prod_i M_{\\eta_i}^{X_i}(u) = \\exp\\left\\{\\sum_i \\lambda_i (e^u-1)\\right\\}.\n\\]\nAs a result, we have \\(X_+ \\sim \\text{Pois}(\\lambda_+)\\), for \\(\\lambda_+ = \\sum_j \\lambda_i\\).\nLikewise, the closely related cumulant-generating function (CGF) is defined as the log of the MGF:\n\\[\nK^{T(X)}_\\eta(u) = \\log M^{T(X)}_\\eta(u) = A(\\eta+u)-A(\\eta)\n\\]\nEvaluating the CGF’s derivatives at \\(u=0\\) gives us the distribution’s cumulants instead of its moments (the first two cumulants are the mean and variance). \\(K_\\eta^{T(X)}\\) and \\(A\\) are closely related. In particular, note that\n\\[\n\\left.\\frac{\\partial}{\\partial \\eta_j}K_\\eta^{T}(u)\\right|_{u=0} = \\frac{\\partial}{\\partial \\eta_j} A(\\eta),\n\\]\nThis relationship explains why we can also get cumulants for \\(T(X)\\) under \\(P_\\eta\\) by differentiating \\(A(\\eta)\\). It also partly explains why \\(A(\\eta)\\) is sometimes referred to as the CGF even though it is generally not the CGF for \\(T(X)\\)."
  },
  {
    "objectID": "reader/exponential-families.html#other-parameterizations",
    "href": "reader/exponential-families.html#other-parameterizations",
    "title": "Exponential Families",
    "section": "Other parameterizations",
    "text": "Other parameterizations\nSometimes instead of parameterizing \\(\\cP\\) by the natural parameter \\(\\eta\\), it is more convenient to parameterize the family by another parameter \\(\\theta\\). Then we can write the density in terms of this alternative parameterization as\n\\[\np_\\theta(x) = e^{\\eta(\\theta)'T(x) - B(\\theta)}h(x), \\quad \\text{ where } B(\\theta) = A(\\eta(\\theta)).\n\\]\nThe Poisson distribution, if indexed by the mean \\(\\lambda\\), is an example of such an alternative parameterization, with \\(\\eta(\\lambda) = \\log\\lambda\\) and \\(B(\\lambda) = \\lambda\\). Another example is the normal family:\nExample (Normal): Consider the model \\(X\\sim \\cN(\\mu, \\sigma^2)\\), for \\(\\mu\\in\\RR\\) and \\(\\sigma^2&gt;0\\). The usual parameter vector for this problem is \\(\\theta = (\\mu, \\sigma^2)\\). The density in that parameterization is\n\\[\np_\\theta(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-(\\mu-x)^2/2\\sigma^2\\right\\}.\n\\]\nExpanding the square and raising the \\(\\sqrt{2\\pi\\sigma^2}\\) into the exponent, we can massage the density into the exponential family structure we are looking for:\n\\[\np_\\theta(x)\n= \\exp\\left\\{\\frac{\\mu}{\\sigma^2} x - \\frac{1}{2\\sigma^2}x^2 - \\frac{\\mu}{2\\sigma^2} - \\frac{1}{2}\\log\\left(2\\pi\\sigma^2\\right)\\right\\},\n\\]\nwhich we can recognize as an exponential family with sufficient statistic \\(T(x) = (x,x^2)\\), natural parameter \\(\\eta(\\theta) = (\\mu/\\sigma^2,-1/2\\sigma^2)\\), carrier density \\(h(x)=1\\), and log-partition function\n\\[\nB(\\theta) = \\frac{\\mu^2}{2\\sigma^2} + \\frac{1}{2}\\log\\left(2\\pi\\sigma^2\\right).\n\\]\nWe can rewrite the log-partition function in terms of \\(\\eta_1 = \\mu^2/2\\sigma^2\\) and \\(\\eta_2=-1/2\\sigma^2\\) to complete the natural parameterization:\n\\[\np_\\eta(x) = e^{\\eta'T(x) - A(\\eta)}, \\quad \\text{ for } A(\\eta) = \\frac{-\\eta_1^2}{4\\eta_2} + \\frac{1}{2}\\log(-\\pi/\\eta_2)\n\\]\nHence, the Gaussian is the (only) exponential family with sufficient statistic \\(T(x) = (x,x^2)\\), and carrier density \\(h(x)=1\\), with respect to the Lebesgue measure on \\(\\RR\\).\nExample (Binomial): Next, consider the model \\(X \\sim \\text{Binom}(n,\\theta)\\), which has pmf\n\\[\np_\\theta(x) = \\theta^x (1-\\theta)^{n-x}\\binom{n}{x}.\n\\]\nWe can again massage this into canonical form by raising the parameters into the exponent and collecting terms\n\\[\n\\begin{aligned}\np_\\theta(x)\n&= \\exp\\left\\{x\\log\\theta + (n-x)\\log(1-\\theta)\\right\\}\\binom{n}{x}\\\\\n&= \\exp\\left\\{x\\log\\left(\\frac{\\theta}{1-\\theta}\\right)-n\\log(1-\\theta)\\right\\}\\binom{n}{x},\n\\end{aligned}\n\\]\nwhich we recognize as an exponential family structure with \\(T(x)=x\\), natural parameter \\(\\eta=\\log\\left(\\frac{\\theta}{1-\\theta}\\right)\\) The natural parameter \\(\\eta\\) is called the log-odds or logit, which is used in classification models such as logistic regression and the many extensions thereof.\nExample (Beta): The Beta distribution is a common family of distributions on the unit interval. If \\(X \\sim \\text{Beta}(\\alpha,\\beta)\\) then \\(X\\) has pdf\n\\[\n\\begin{aligned}\np_{\\alpha,\\beta}(x)\n&= \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\\\[5pt]\n&= \\exp\\{\\alpha \\log x + \\beta\\log (1-x) - \\log B(\\alpha,\\beta)\\}\\cdot \\frac{1}{x(1-x)},\n\\end{aligned}\n\\]\nwhere \\(B(\\alpha,\\beta) = \\int_0^1 t^{\\alpha-1}(1-t)^{\\beta-1}\\td t\\) is called the beta function. We recognize this as an exponential family with \\(T(x) = (\\log x, \\log(1-x))\\), \\(\\eta = (\\alpha,\\beta)\\), and \\(h(x) = \\frac{1}{x(1-x)}\\), though as always there are other ways to decompose the density.\nIn addition to these examples, we could add most of the distributional families detailed on Wikipedia: the Gamma, multinomial, Dirichlet, Pareto, Wishart, and many others. We will see more exponential family examples throughout the course."
  },
  {
    "objectID": "reader/exponential-families.html#exponential-tilting",
    "href": "reader/exponential-families.html#exponential-tilting",
    "title": "Exponential Families",
    "section": "Exponential tilting",
    "text": "Exponential tilting\nTo help interpret what it means for a model to have an exponential family structure, we can think of \\(p_\\eta(x) = e^{\\eta'T(x) - A(\\eta)} h(x)\\) as an exponential tilt of the carrier density \\(h(x)\\). That is, beginning with \\(h(x)\\), we first multiply by \\(e^{\\eta'T(x)}\\), increasing the density of points in the sample space for which \\(\\eta'T(x)\\) is largest relative to those for which \\(\\eta'T(x)\\) is smaller. Then, we re-normalize by \\(e^{-A(\\eta)}\\) to obtain a probability distribution.\nThis is easiest to understand in a one-parameter family with sufficient statistic \\(T(X) = X\\)\n\nPlot = require(\"@observablehq/plot\")\nd3 = require(\"d3\")\n\nkatex = require(\"https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js\")\n\n// Function to render LaTeX\nfunction tex(string) {\n  let span = document.createElement('span');\n  katex.render(string, span, {output: \"mathml\", throwOnError: false});\n  return span;\n}\n\n// Create sliders for mean and standard deviation\nviewof eta1 = Inputs.range([-5, 5], {step: 0.1, label: tex(\"\\\\eta_1\"), value: 0})\nviewof eta2 = Inputs.range([-5, 5], {step: 0.1, label: tex(\"\\\\eta_2\"), value: 0})\nviewof eta3 = Inputs.range([-0.3, 0.3], {step: 0.01, label: tex(\"\\\\eta_3\"), value: 0})\n\n// Function to generate normal distribution data\nfunction explDistribution(eta1, eta2, eta3, n = 2000) {\n  const x = d3.range(-1, 1, 2/n);\n  const unnorm = x.map(x =&gt; ({\n    x: x,\n    y: Math.exp(eta1 * x + eta2 * x * x + eta3 * Math.cos(30 * x * Math.PI)) * (1 - Math.abs(x))\n  }));\n  const normConst = d3.sum(unnorm, point =&gt; point.y) * 2 / n;\n  return unnorm.map(point =&gt; ({\n    x: point.x,\n    y: point.y / normConst\n  }));\n}\n\n// Create the plot\nPlot.plot({\n  width: 640,\n  height: 400,\n  x: {label: \"X\"},\n  y: {label: \"Density\"},\n  marks: [\n    Plot.line(explDistribution(eta1, eta2, eta3), {x: \"x\", y: \"y\", stroke: \"steelblue\"}),\n    Plot.ruleY([0])\n  ]\n})"
  },
  {
    "objectID": "reader/exponential-families.html#repeated-sampling-from-exponential-families",
    "href": "reader/exponential-families.html#repeated-sampling-from-exponential-families",
    "title": "Exponential Families",
    "section": "Repeated sampling from exponential families",
    "text": "Repeated sampling from exponential families\nOne of the most important properties of exponential families is that a large sample can be summarized by a low-dimensional statistic. Suppose we observe a vector of observations \\(X = (X_1,\\ldots,X_n)\\) representing an independent and identically distributed (i.i.d.) sample from an exponential family. Let \\(p_\\eta^{(1)\\) denote the density for a single observation:\n\\[\nX_1\\ldots,X_n \\simiid p_\\eta^{(1)}(x) = e^{\\eta'T(x) - A(\\eta)}h(x).\n\\]\nThen the random vector \\(X = (X_1,\\ldots,X_n)\\) follows another closely related exponential family:\n\\[\n\\begin{aligned}\np_\\eta(x)\n&= \\prod_{i=1}^n e^{\\eta'T(x_i) - A(\\eta)}h(x_i)\\\\[7pt]\n&= \\exp\\left\\{\\eta'\\sum_{i=1}^n T(x_i) - nA(\\eta)\\right\\} \\prod_{i=1}^n h(x_i).\n\\end{aligned}\n\\]\nThis new density \\(p_\\eta\\), which governs the distribution of the entire sample, is an exponential family with the same natural parameter as before, sufficient statistic \\(\\sum_i T(X_i)\\), carrier density \\(\\prod_i h(x_i)\\), and log-partition function \\(nA(\\eta)\\).\nFor reasons that will become clearer in the next lecture, it is very significant that the sufficient statistic does not increase in dimension as the sample size grows. This means that the \\(s\\)-dimensional vector \\(\\sum_i T(X_i)\\) is for all intents and purposes a complete summary of the entire sample, no matter how large \\(n\\) is."
  },
  {
    "objectID": "reader/testing-one-parameter.html",
    "href": "reader/testing-one-parameter.html",
    "title": "Testing with One Real Parameter",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/testing-one-parameter.html#uniformly-most-powerful-tests",
    "href": "reader/testing-one-parameter.html#uniformly-most-powerful-tests",
    "title": "Testing with One Real Parameter",
    "section": "1 Uniformly Most Powerful Tests",
    "text": "1 Uniformly Most Powerful Tests\n\n1.1 Definition of UMP Test\nIf \\(\\phi(x)\\) has significance level \\(\\alpha\\) and for any other level \\(\\alpha\\) test \\(\\psi\\), we have:\n\\[\\beta_\\phi(\\theta) \\geq \\beta_\\psi(\\theta) \\quad \\forall \\theta \\in \\Theta_1\\]\nthen \\(\\phi\\) is uniformly most powerful (UMP).\nTypically, UMP tests only exist for 1-sided testing in certain 1-parameter families.\n\n\n1.2 Identifiability and Monotone Likelihood Ratio\nDefinition (Identifiability): A model \\(\\cP\\) is identifiable if:\n\\[\\forall \\theta_1 \\neq \\theta_2, \\, \\exists A \\text{ s.t. } P_{\\theta_1}(A) \\neq P_{\\theta_2}(A)\\]\nDefinition (Monotone Likelihood Ratio): Assume \\(\\cP = \\{P_\\theta: \\theta \\in \\Theta \\subseteq \\mathbb{R}\\}\\) has densities \\(p_\\theta\\) and is identifiable. We say \\(\\cP\\) has monotone likelihood ratios (MLR) if there is some statistic \\(T(X)\\) s.t. \\(\\frac{p_{\\theta_2}(x)}{p_{\\theta_1}(x)}\\) is a non-decreasing function of \\(T(X)\\) for any \\(\\theta_2 &gt; \\theta_1\\) (same \\(T(\\cdot)\\) for all \\(\\theta\\)’s).\nExample: Exponential family \\(e^{\\eta \\cdot T(x) - A(\\eta)}h(x)\\) has MLR in \\(\\eta \\cdot T(x)\\).\n\n\n1.3 Theorem: UMP for One-Sided Tests\nAssume \\(\\cP\\) has MLR. Test \\(H_0: \\theta \\leq \\theta_0\\) vs \\(H_1: \\theta &gt; \\theta_0\\) at level \\(\\alpha\\). Let:\n\\[\\phi(x) = \\begin{cases}\n1 & \\text{if } T(x) &gt; c \\\\\n\\gamma & \\text{if } T(x) = c \\\\\n0 & \\text{if } T(x) &lt; c\n\\end{cases}\\]\nwith \\(c, \\gamma\\) chosen so that \\(\\mathbb{E}_{\\theta_0}[\\phi(X)] = \\alpha\\).\nThen:\n\n\\(\\phi\\) is a UMP level \\(\\alpha\\) test\nIf \\(\\theta &lt; \\theta_0\\), then \\(\\phi\\) minimizes \\(\\mathbb{E}_\\theta[\\phi(X)]\\) among all tests with \\(\\mathbb{E}_{\\theta_0}[\\phi(X)] = \\alpha\\)\n\nProof:\n\nSuppose \\(\\theta &gt; \\theta_0\\) and \\(\\psi\\) has level \\(\\alpha\\):\n\\(\\mathbb{E}_\\theta[\\phi(X)] \\geq \\mathbb{E}_\\theta[\\psi(X)]\\) since \\(\\phi\\) is a LRT for \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta = \\theta\\).\nIf \\(\\theta &lt; \\theta_0\\), assume \\(\\mathbb{E}_\\theta[\\psi] = \\mathbb{E}_\\theta[\\phi] + \\delta\\), \\(\\delta &gt; 0\\)\nBoth \\(\\frac{1}{1+\\delta}\\psi\\) and \\(\\phi\\) are tests of \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta = \\theta\\), both have significance level \\(\\alpha\\).\n\\(\\phi\\) is a LRT since \\(\\frac{p_\\theta(x)}{p_{\\theta_0}(x)}\\) is non-increasing in \\(T(X)\\).\nTherefore, \\(\\mathbb{E}_\\theta[\\phi] \\geq \\mathbb{E}_\\theta[\\frac{1}{1+\\delta}\\psi] = \\frac{1}{1+\\delta}(\\mathbb{E}_\\theta[\\phi] + \\delta)\\)\n\nIntuition: \\(\\phi\\) is a LRT for \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta = \\theta\\) for any pair \\(\\theta, \\theta_0\\). Significance level depends on \\(\\theta_0\\).\n\n\n1.4 UMP Test Picture\n[Insert visual representation of UMP test here]\nBest on \\(\\theta &gt; \\theta_0\\), exactly level \\(\\alpha\\) at \\(\\theta_0\\), best among tests with \\(\\mathbb{E}_{\\theta_0}[\\phi] = \\alpha\\)."
  },
  {
    "objectID": "reader/testing-one-parameter.html#one-sided-tests-in-general",
    "href": "reader/testing-one-parameter.html#one-sided-tests-in-general",
    "title": "Testing with One Real Parameter",
    "section": "2 One-Sided Tests in General",
    "text": "2 One-Sided Tests in General\n\\(\\cP = \\{P_\\theta: \\theta \\in \\Theta \\subseteq \\mathbb{R}\\}\\), \\(\\Theta \\subseteq \\mathbb{R}\\)\n\\(H_0: \\theta \\leq \\theta_0\\) vs \\(H_1: \\theta &gt; \\theta_0\\) called one-sided hypothesis.\nOften no UMP test exists.\nExample 1: \\(p_\\theta(x) = \\theta e^{-\\theta x} 1_{x&gt;0} + (1-\\theta)\\delta_0(x)\\), \\(\\theta \\in [0,1]\\)\nLRT for \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta = \\theta_1 &gt; \\theta_0\\):\n\\[\\log LR(x) = \\begin{cases}\n\\log \\frac{\\theta_1}{\\theta_0} - (\\theta_1 - \\theta_0)x & \\text{if } x &gt; 0 \\\\\n\\log \\frac{1-\\theta_1}{1-\\theta_0} & \\text{if } x = 0\n\\end{cases}\\]\nReject for: - \\(x \\leq \\theta_0\\) if \\(\\theta_1 &lt; \\frac{1}{2}\\) - \\(x \\geq \\theta_0\\) if \\(\\theta_1 &gt; \\frac{1}{2}\\)\nVery dependent on specific values of \\(\\theta_0\\) and \\(\\theta_1\\).\nTest \\(H_0: \\theta \\leq \\theta_0\\) vs \\(H_1: \\theta &gt; \\theta_0\\): No UMP test.\nExample 2: Test \\(H_0: \\theta \\leq 0\\) vs \\(H_1: \\theta &gt; 0\\) for \\(X_i \\in \\{-1,0,1\\}\\)\n\\(\\mathbb{E}[X_i] = \\theta\\), \\(\\text{Var}(X_i) = 1 - \\theta^2\\)\n\\(T = \\sum X_i\\), \\(\\sum |X_i|\\), \\(\\frac{T}{\\sum |X_i|}\\)\nTail \\(\\{X_i\\}\\) vs \\(\\{|X_i|\\}\\) vs \\(\\{\\text{sign}(X_i)\\}\\)\n\\(n^{-1/2} \\sum X_i \\sim N(0,1)\\) vs \\(\\text{Binom}(n, \\frac{1}{2})\\) (sign test)\n\n2.1 Stochastically Increasing\nDefinition: A real-valued statistic \\(T(X)\\) is stochastically increasing in \\(\\theta\\) if \\(\\mathbb{P}_\\theta(T(X) &gt; t)\\) is non-decreasing in \\(\\theta\\), \\(\\forall t\\).\nIf \\(\\phi_t\\) is right-tailed test based on \\(T(X)\\):\n\\[\\phi_t(x) = 1\\{T(x) &gt; c\\} + \\gamma 1\\{T(x) = c\\}\\]\nand \\(T(X)\\) is stochastically increasing in \\(\\theta\\):\n\\[\\beta_{\\phi_t}(\\theta) = \\mathbb{P}_\\theta(T(X) &gt; c) + \\gamma\\mathbb{P}_\\theta(T(X) = c)\\]\nis increasing in \\(\\theta\\).\nExample 1: \\(X_i \\sim \\text{iid}\\) location family $T(X) = $ sample mean, median, sign statistic\nExample 2: \\(X_i \\sim \\text{iid}\\) scale family \\(T(X) = \\sum |X_i|\\) or median \\(\\{|X_1|, \\ldots, |X_n|\\}\\)"
  },
  {
    "objectID": "reader/testing-one-parameter.html#two-sided-alternatives",
    "href": "reader/testing-one-parameter.html#two-sided-alternatives",
    "title": "Testing with One Real Parameter",
    "section": "3 Two-Sided Alternatives",
    "text": "3 Two-Sided Alternatives\nSetup: \\(\\cP = \\{P_\\theta: \\theta \\in \\Theta \\subseteq \\mathbb{R}\\}\\), \\(\\Theta \\subseteq \\mathbb{R}\\)\nTest \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta \\neq \\theta_0\\)\nCan be generalized naturally to \\(H_0: \\theta \\in [\\theta_0, \\theta_1]\\)\nTwo-tailed test rejects when \\(T(X)\\) is extreme:\n\\[\\phi(x) = \\begin{cases}\n1 & \\text{if } T(X) &gt; c_2 \\text{ or } T(X) &lt; c_1 \\\\\n\\gamma_2 & \\text{if } T(X) = c_2 \\\\\n\\gamma_1 & \\text{if } T(X) = c_1 \\\\\n0 & \\text{if } c_1 &lt; T(X) &lt; c_2\n\\end{cases}\\]\nTwo ways to reject. How to balance?\nFor symmetric distributions like \\(N(\\theta, 1)\\), natural choice is to equalize lobes of rejection region: \\(\\alpha/2 = \\mathbb{P}(\\text{left lobe}) = \\mathbb{P}(\\text{right lobe})\\) for \\(H_0: \\theta = \\theta_0\\).\nFor asymmetric distributions or interval null \\(H_0: \\theta \\in [\\theta_0, \\theta_1]\\), more complicated.\n\n3.1 Equal-Tailed Unbiased Tests\nRecall \\(H_0: \\theta = \\theta_0\\)\nLet \\(\\alpha_1 = \\mathbb{P}_{\\theta_0}(T &lt; c_1) + \\gamma_1\\mathbb{P}_{\\theta_0}(T = c_1)\\) \\(\\alpha_2 = \\mathbb{P}_{\\theta_0}(T &gt; c_2) + \\gamma_2\\mathbb{P}_{\\theta_0}(T = c_2)\\)\nValid if \\(\\alpha_1 + \\alpha_2 = \\alpha\\) (\\(\\alpha_2\\) is free parameter)\nIdea 1: Equal-tailed test \\(\\alpha_1 = \\alpha_2 = \\frac{\\alpha}{2}\\)\nExample: \\(X \\sim \\text{Exp}(\\theta)\\), test \\(H_0: \\theta = 1\\)\nSolve for cutoffs: \\(1 - e^{-c_1} = e^{-c_2} = \\frac{\\alpha}{2}\\)\n\\(c_1 = -\\log(1-\\frac{\\alpha}{2})\\), \\(c_2 = -\\log(\\frac{\\alpha}{2})\\)\n\\(\\phi(x) = 1\\{x &lt; -\\log(1-\\frac{\\alpha}{2}) \\text{ or } x &gt; -\\log(\\frac{\\alpha}{2})\\}\\)\n\\(\\beta(\\theta) = \\mathbb{P}_\\theta(X &lt; c_1) + \\mathbb{P}_\\theta(X &gt; c_2)\\)\n\\(= 1 - e^{-\\theta c_1} + e^{-\\theta c_2}\\)\n\\(= 1 - (1-\\frac{\\alpha}{2})^\\theta + (\\frac{\\alpha}{2})^\\theta\\)\nPower curve for \\(\\theta \\in [0,2]\\):\n[Insert power curve graph here]\n\n\n3.2 Unbiased Tests\nDefinition: \\(\\phi(x)\\) is unbiased if \\(\\forall \\theta \\neq \\theta_0\\), \\(\\mathbb{E}_\\theta[\\phi(X)] \\geq \\alpha\\)\nIdea 2 (Unbiased): ensure min \\(\\beta(\\theta) = \\alpha\\)\nChoose \\(\\gamma_1, \\gamma_2, c_1, c_2\\) to solve:\n\\(\\beta(\\theta_0) = \\alpha\\) \\(\\beta'(\\theta_0) = 0\\)\n2 equations, 2 unknowns\nExample: 1-parameter exponential family, \\(H_0: \\eta = \\eta_0\\) vs \\(H_1: \\eta \\neq \\eta_0\\)\n\\(p_\\eta(x) = e^{\\eta T(x) - A(\\eta)}h(x)\\) (MLR in \\(T(x)\\))\nAssume \\(T(x)\\) continuous, set \\(c_1 &lt; c_2\\):\n\\(\\alpha = \\mathbb{P}_{\\eta_0}(T &lt; c_1) + \\mathbb{P}_{\\eta_0}(T &gt; c_2)\\) \\(0 = \\mathbb{E}_{\\eta_0}[T \\cdot 1\\{T &lt; c_1\\}] + \\mathbb{E}_{\\eta_0}[T \\cdot 1\\{T &gt; c_2\\}]\\)\n\n\n3.3 Theorem: UMPU Tests\nAssume \\(X_1, \\ldots, X_n \\sim p_\\eta(x) = e^{\\eta T(x) - A(\\eta)}h(x)\\)\n\\(H_0: \\eta \\in [\\eta_1, \\eta_2]\\) vs \\(H_1: \\eta &lt; \\eta_1 \\text{ or } \\eta &gt; \\eta_2\\)\n(possibly \\(\\eta_1 = \\eta_2\\))\nThen:\n\nThe unbiased test rejecting for \\(T(X) \\notin [c_1, c_2]\\) with significance level \\(\\alpha\\) is UMP among all unbiased tests (UMPU)\nIf \\(\\eta_1 = \\eta_2\\), the UMPU test can be found by solving for \\(c_1, c_2\\) s.t. \\(\\mathbb{E}_{\\eta_0}[\\phi] = \\alpha\\), \\(\\mathbb{E}_{\\eta_0}[T\\phi] = \\alpha\\mathbb{E}_{\\eta_0}[T]\\)\nIf \\(\\eta_1 &lt; \\eta_2\\), the UMPU test can be found by solving for \\(c_1, c_2\\) s.t. \\(\\mathbb{E}_{\\eta_1}[\\phi] = \\alpha\\) and \\(\\mathbb{E}_{\\eta_2}[\\phi] = \\alpha\\)\n\nProof in Keener."
  },
  {
    "objectID": "reader/bayes-estimation.html",
    "href": "reader/bayes-estimation.html",
    "title": "Bayes Estimation",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/bayes-estimation.html#bayes-risk-and-bayes-estimator",
    "href": "reader/bayes-estimation.html#bayes-risk-and-bayes-estimator",
    "title": "Bayes Estimation",
    "section": "1 Bayes Risk and Bayes Estimator",
    "text": "1 Bayes Risk and Bayes Estimator\n\n1.1 Definitions\nThe Bayes risk is the average case risk:\n\\[\nr(\\pi, \\delta) = \\EE_\\pi[R(\\theta, \\delta)] = \\int R(\\theta, \\delta) \\,d\\pi(\\theta)\n\\]\nwhere \\(\\pi(\\theta)\\) is a probability measure (for now, we assume it’s proper; later we will allow it to be improper).\nNote: \\(\\pi\\) and \\(\\delta\\) are functionally equivalent for average risk, which makes sense even if we don’t believe \\(\\pi\\).\n\\[\nr(\\pi, \\delta) = \\EE_\\pi[\\EE_\\theta[L(\\theta, \\delta(X))]] = \\EE[L(\\theta, \\delta(X))]\n\\]\nwhere \\((\\theta, X) \\sim p(\\theta, x) = p(x|\\theta)\\pi(\\theta)\\)\nAn estimator \\(\\delta\\) minimizing \\(r_\\text{Bayes}(\\delta)\\) is called a Bayes estimator. It depends on \\(\\pi\\) and \\(L\\).\n\\[\n\\delta_\\pi = \\argmin_\\delta \\EE[L(\\theta, \\delta(X))]\n\\]\n\n\n1.2 Prior and Posterior\n\nThe usual interpretation of \\(\\pi\\) is the prior belief about \\(\\theta\\) before seeing the data.\nThe conditional distribution \\(\\pi(\\theta|X)\\) is called the posterior distribution (belief after seeing the data).\n\nDensities: - Prior: \\(\\pi(\\theta)\\) - Likelihood: \\(p(x|\\theta)\\) - Joint density: \\(p(\\theta, x) = \\pi(\\theta)p(x|\\theta)\\) - Marginal density: \\(q(x) = \\int p(\\theta, x) \\,d\\theta\\) - Posterior density: \\(\\pi(\\theta|x) = \\frac{p(\\theta, x)}{q(x)}\\)\nThe Bayes estimator depends on the posterior:\n\\[\n\\delta_\\pi(x) = \\argmin_d \\EE[L(\\theta, d)|X=x] = \\argmin_d \\int L(\\theta, d) \\pi(\\theta|x) \\,d\\theta\n\\]\n\n\n1.3 Theorem: Characterization of Bayes Estimators\nSuppose \\(X \\sim p_\\theta(x)\\) and \\(\\delta_\\pi(x) = \\delta(x)\\) for some function \\(\\delta\\). Then \\(\\delta\\) is Bayes with respect to \\(\\pi\\) if and only if \\(\\delta(x) \\in \\argmin_d \\EE[L(\\theta, d)|X=x]\\) for almost every \\(x\\).\nProof: 1. Let \\(\\delta'\\) be any other estimator. 2. \\(r(\\pi, \\delta') = \\int \\EE[L(\\theta, \\delta'(X))|X=x] q(x) \\,dx\\) 3. \\(r(\\pi, \\delta) = \\int \\EE[L(\\theta, \\delta(X))|X=x] q(x) \\,dx\\) 4. Define \\(E_x(d) = \\EE[L(\\theta, d)|X=x]\\) 5. If \\(\\delta(x) \\in \\argmin_d E_x(d)\\), then \\(E_x(\\delta(x)) \\leq E_x(\\delta'(x))\\) for all \\(x\\) 6. This implies \\(r(\\pi, \\delta) \\leq r(\\pi, \\delta')\\)"
  },
  {
    "objectID": "reader/bayes-estimation.html#special-cases-and-examples",
    "href": "reader/bayes-estimation.html#special-cases-and-examples",
    "title": "Bayes Estimation",
    "section": "2 Special Cases and Examples",
    "text": "2 Special Cases and Examples\n\n2.1 Squared Error Loss\nIf \\(L(\\theta, d) = (\\theta - d)^2\\), then the Bayes estimator is the posterior mean:\n\\[\n\\delta_\\pi(x) = \\EE[\\theta|X=x]\n\\]\nProof: \\[\n\\begin{aligned}\n\\EE[(\\theta - d)^2|X=x] &= \\EE[\\theta^2|X=x] - 2d\\EE[\\theta|X=x] + d^2 \\\\\n&= \\Var(\\theta|X=x) + (\\EE[\\theta|X=x] - d)^2 + \\EE[\\theta|X=x]^2 - 2d\\EE[\\theta|X=x] + d^2\n\\end{aligned}\n\\]\nThe minimum occurs when \\(d = \\EE[\\theta|X=x]\\).\n\n\n2.2 Weighted Squared Error\nFor \\(L(\\theta, d) = w(\\theta)(\\theta - d)^2\\) (e.g., squared relative error), the Bayes estimator is:\n\\[\n\\delta_\\pi(x) = \\frac{\\EE[w(\\theta)\\theta|X=x]}{\\EE[w(\\theta)|X=x]}\n\\]"
  },
  {
    "objectID": "reader/bayes-estimation.html#examples",
    "href": "reader/bayes-estimation.html#examples",
    "title": "Bayes Estimation",
    "section": "3 Examples",
    "text": "3 Examples\n\n3.1 Beta-Binomial\n\n\\(X|\\theta \\sim \\text{Binomial}(n, \\theta)\\), \\(\\theta \\in [0,1]\\)\n\\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\), \\(\\alpha, \\beta &gt; 0\\)\n\nThe marginal distribution of \\(X\\) is called Beta-Binomial.\nPosterior: \\[\n\\pi(\\theta|x) \\propto \\theta^x (1-\\theta)^{n-x} \\cdot \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} \\propto \\theta^{x+\\alpha-1}(1-\\theta)^{n-x+\\beta-1}\n\\]\nTherefore, \\(\\theta|X \\sim \\text{Beta}(x+\\alpha, n-x+\\beta)\\)\n\\[\n\\EE[\\theta|X] = \\frac{x+\\alpha}{n+\\alpha+\\beta}\n\\]\nInterpret \\(\\alpha+\\beta\\) as pseudo-trials and \\(\\alpha\\) as pseudo-successes.\n\n\n3.2 Normal Mean\n\n\\(X_i|\\theta \\sim N(\\theta, \\sigma^2)\\), \\(\\sigma^2\\) known\n\\(\\theta \\sim N(\\mu, \\tau^2)\\)\n\nPosterior: \\[\n\\pi(\\theta|x) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\theta)^2\\right) \\exp\\left(-\\frac{1}{2\\tau^2}(\\theta-\\mu)^2\\right)\n\\]\nComplete the square:\n\\[\n\\theta|X \\sim N\\left(\\frac{\\frac{n}{\\sigma^2}\\bar{x} + \\frac{1}{\\tau^2}\\mu}{\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}}, \\frac{1}{\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}}\\right)\n\\]\n\\[\n\\EE[\\theta|X] = \\frac{\\frac{n}{\\sigma^2}\\bar{x} + \\frac{1}{\\tau^2}\\mu}{\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}} = w\\bar{x} + (1-w)\\mu\n\\]\nwhere \\(w = \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}\\)\nIf \\(\\frac{1}{\\tau^2} = k\\), interpret as \\(k\\) pseudo-observations with mean \\(\\mu\\)."
  },
  {
    "objectID": "reader/bayes-estimation.html#conjugate-priors",
    "href": "reader/bayes-estimation.html#conjugate-priors",
    "title": "Bayes Estimation",
    "section": "4 Conjugate Priors",
    "text": "4 Conjugate Priors\nIn both examples, the prior and likelihood have a similar functional form, and the posterior comes from the same exponential family as the prior. When the posterior is from the same family as the prior, we say the prior is conjugate to the likelihood.\n\n4.1 Conjugate Priors for Exponential Families\nSuppose \\(p_\\theta(x) = h(x)\\exp(\\eta(\\theta)'T(x) - A(\\theta))\\) for carrier \\(h\\). Define:\n\\[\n\\pi(\\theta) = g(\\theta)\\exp(\\lambda'u(\\theta) - \\psi(\\lambda))\n\\]\nThen:\n\\[\n\\pi(\\theta|x) \\propto \\exp((\\lambda + T(x))'u(\\theta) - (A(\\theta) + \\psi(\\lambda)))\n\\]\nOften, \\(u(\\theta) = \\eta(\\theta)\\) and \\(\\lambda\\) is interpreted as pseudo-observations.\n\n\n4.2 Conjugate Prior Examples\n\nNormal-Normal:\n\n\\(X_i|\\theta \\sim N(\\theta, \\sigma^2)\\), \\(\\sigma^2\\) known\n\\(\\theta \\sim N(\\mu, \\tau^2)\\)\n\nPoisson-Gamma:\n\n\\(X|\\theta \\sim \\text{Poisson}(\\theta)\\), \\(\\theta &gt; 0\\)\n\\(\\theta \\sim \\text{Gamma}(\\alpha, \\beta)\\), \\(\\alpha, \\beta &gt; 0\\)\n\nPosterior: \\(\\theta|X \\sim \\text{Gamma}(\\alpha + \\sum x_i, \\beta + n)\\)\nInterpret \\(\\alpha\\) as pseudo-counts and \\(\\beta\\) as pseudo-exposure."
  },
  {
    "objectID": "reader/introduction.html",
    "href": "reader/introduction.html",
    "title": "Course introduction",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/introduction.html#what-is-the-theory-of-statistics",
    "href": "reader/introduction.html#what-is-the-theory-of-statistics",
    "title": "Course introduction",
    "section": "What is the theory of statistics?",
    "text": "What is the theory of statistics?\nStatistics is the study of methods that use data to understand the world. Statistical methods are used throughout the natural and social sciences, in machine learning and artificial intelligence, and in engineering. Despite the ubiquitous use of statistics, its practitioners are perpetually accused of not actually understanding what they are doing. Statistics theory is, broadly speaking, about trying to understand what we are doing when we use statistical methods.\nWhile there are many possible ways to analyze data, most (but certainly not all) statistical methods are based on statistical modeling: treating the data as a realization of some random data-generating process with attributes, usually called parameters, that are a priori unknown. The goal of the analyst, then, is to use the data to draw accurate inferences about these parameters and/or to make accurate predictions about future data. If the modeling has been done well (a very big “if”) then these unknown parameters will correspond well to whatever real-world questions initially motivated the analysis. Applied statistics courses like Stat 215A and B concern questions about how to ensure that the statistical modeling exercise successfully captures something interesting about reality.\nIn this course we will instead focus on how the analyst can use the data most effectively within the context of a given mathematical setup. We will discuss the structure of statistical models, how to evaluate the quality of a statistical method, how to design good methods for new settings, and the philosophy of Bayesian vs frequentist modeling frameworks. We will cover estimation, confidence intervals, and hypothesis testing, in parametric and nonparametric methods, in finite samples and asymptotic regimes."
  },
  {
    "objectID": "reader/introduction.html#relationship-of-stat-210a-to-other-berkeley-courses",
    "href": "reader/introduction.html#relationship-of-stat-210a-to-other-berkeley-courses",
    "title": "Course introduction",
    "section": "Relationship of Stat 210A to other Berkeley courses",
    "text": "Relationship of Stat 210A to other Berkeley courses\nStat 210A focuses on classical statistical contexts: either inference in finite samples, or in fixed-dimensional asymptotic regimes. Stat 210B (for which 210A is a prerequisite) is more technical and covers topics like empirical process theory and high-dimensional statistics.\nBerkeley’s graduate course on Statistical Learning Theory (CS 281A / Stat 241A) is also very popular and has some overlap in its topics. Roughly speaking, it is more tilted toward “machine learning”: it spends more time on topics in predictive modeling (i.e. classification and regression, which are covered in Stat 215A), optimization, and signal processing, but spends less time on inferential questions and (I believe) does not cover topics like hypothesis testing, confidence intervals, and causal inference. Both courses cover estimation and exponential families."
  },
  {
    "objectID": "reader/introduction.html#deductive-vs-inductive-reasoning",
    "href": "reader/introduction.html#deductive-vs-inductive-reasoning",
    "title": "Course introduction",
    "section": "Deductive vs inductive reasoning",
    "text": "Deductive vs inductive reasoning\nMost mathematics courses are entirely concerned with deductive reasoning: drawing conclusions that follow logically from premises. For example:\n\nAll real, symmetric matrices have real eigenvalues.\n\\(A\\) is a real, symmetric matrix.\nTherefore, \\(A\\) has real eigenvalues.\n\nDeductive reasoning comes up in everyday life, for example\n\nNo one in my daughter’s preschool class has a nut allergy.\nZoe is in my daughter’s preschool class.\nTherefore, Zoe is not allergic to peanuts.\n\nThis type of argument is risk-free in the sense that, as long as the premises are true, the conclusions must hold. Of course, the premises could be false: I might be confusing my neighbor Zoe with a different Zoe who is in my daughter’s class. But that is the only way my conclusion could be wrong.\nDeductive arguments can involve statements about probability:\n\nThis die has six faces labeled 1, 2, 3, 4, 5, and 6.\nIf I roll it, it is equally likely to land on any face.\nTherefore, the chance of rolling a 4 is exactly 1/6.\n\nA probability course like Stat 205A is about statements like this.\nStatistics, on the other hand, is the mathematical science of inductive reasoning: reasoning from observations to make general claims about the world. Unlike deductive reasoning, such arguments are inherently risky: the conclusions we draw can be false even when the premises are correct.\n\n\n\n\n\n\nCaution\n\n\n\nNote that inductive proofs in mathematics are not an example of inductive reasoning as we mean it here. Inductive proofs are really examples of deductive reasoning because they provide a logically valid argument (i.e. the inductive step) for extending the conclusion to the entire class of objects under study.\n\n\nFor example:\n\nI ate a blueberry from the free sample tray at the supermarket.\nIt was ripe and delicious.\nTherefore, if I buy a carton of blueberries, they will probably be ripe and delicious.\n\nHere we have added the weasel word “probably” not to convey a rigorous quantitative statement about probability, but just to informally convey some uncertainty about the conclusion.\nThis example would be more persuasive if we had taken a sample randomly from the carton we planned on buying:\n\nI ate five blueberries at random from the carton I intended to buy.\nThey were all ripe and delicious.\nTherefore, if I buy the carton, the rest of the blueberries will probably be ripe and delicious.\n\nOf course, we could still always be wrong: maybe there were only five good blueberries in the whole carton and we just happened to take those. But that is not very likely.\nScientists very often reason inductively. For example:\n\nWater at 1atm of pressure has been observed to boil at 100°C every time it has been measured in the laboratory.\nTherefore, water at 1atm of pressure probably always boils at 100°C.\n\nInductive reasoning is the basis of all of the empirical sciences.\nWe can also make inductive statements about probability:\n\nI flipped this penny 1000 times and got 502 heads.\nTherefore, it probably has about a 50% chance of landing heads.\n\nFor now, we’ll assume we know what it means for a penny to have a 50% chance of landing heads; something like: it’s physical properties give it an equal chance of landing heads or tails (and a negligible chance of landing on its side or flying off into space). Generally, there is some controversy among different camps of philosophers and statisticians about what probability means, but not too much when it comes to coin flips. We’ll discuss this more later in the semester."
  },
  {
    "objectID": "reader/introduction.html#the-problem-of-induction",
    "href": "reader/introduction.html#the-problem-of-induction",
    "title": "Course introduction",
    "section": "The problem of induction",
    "text": "The problem of induction\nUnfortunately, inductive reasoning is not valid in the sense meant by logicians or mathematicians. It doesn’t matter how many times I’ve seen real symmetric matrices that had real eigenvalues. Without a proof, I can’t make the general claim. There are entertaining examples of patterns being unexpectedly violated in math, such as:\n\\[\n\\begin{aligned}\n\\int_0^\\infty \\frac{\\sin x}{x}\\,dx &= \\frac{\\pi}{2}\\\\[10pt]\n\\int_0^\\infty \\frac{\\sin x}{x}\\, \\frac{\\sin(x/3)}{x/3}\\,dx &= \\frac{\\pi}{2}\\\\[10pt]\n\\int_0^\\infty \\frac{\\sin x}{x}\\, \\frac{\\sin(x/3)}{x/3}\\, \\frac{\\sin(x/5)}{x/5}\\,dx &= \\frac{\\pi}{2}\\\\[10pt]\n&\\vdots\\\\[10pt]\n\\int_0^\\infty \\frac{\\sin x}{x}\\, \\frac{\\sin(x/3)}{x/3}\\,\\cdots\\, \\frac{\\sin(x/13)}{x/13}\\,dx &= \\frac{\\pi}{2}\\\\[10pt]\n\\int_0^\\infty \\frac{\\sin x}{x}\\, \\frac{\\sin(x/3)}{x/3}\\,\\cdots\\, \\frac{\\sin(x/15)}{x/15}\\,dx &= \\frac{\\pi}{2} - 2.31 \\times 10^{-11}.\n\\end{aligned}\n\\]\nBertrand Russell also warned about how inductive inference can go awry in real life:\n\nDomestic animals expect food when they see the person who usually feeds them. We know that all these rather crude expectations of uniformity are liable to be misleading. The man who has fed the chicken every day throughout its life at last wrings its neck instead, showing that more refined views as to the uniformity of nature would have been useful to the chicken.\n\nDavid Hume’s work A Treatise of Human Nature (1739) first proposed the problem of induction, namely that inductive reasoning presumes — seemingly without justification — that yet-to-be-observed cases will be similar to observed cases. This presumption is sometimes called the uniformity principle, and it is difficult to see how we can justify it. We can’t justify it through a direct logical argument, because it’s not logically justified. We could justify it by past experience — for example, at a low enough level, physical properties of the world generally seem to be uniform across space and time — but that argument is circular! Just because the future has been like the past in the past, doesn’t mean that it will be like the past in the future.\nHume allowed that people have to reason inductively all the time, but he called it a “custom” or “habit” and challenged philosophers to justify it. Now almost 300 years later, there does not seem to have been a fully satisfactory answer; most philosophers of science (like Karl Popper, for example) admit that inductive reasoning is fallible, but think there are reasonable ways for scientists to deal with this."
  },
  {
    "objectID": "reader/introduction.html#statistical-evasions-of-the-problem-of-induction",
    "href": "reader/introduction.html#statistical-evasions-of-the-problem-of-induction",
    "title": "Course introduction",
    "section": "Statistical evasions of the problem of induction",
    "text": "Statistical evasions of the problem of induction\nWe seem to be in trouble if we are trying to build a mathematical science of inductive reasoning, when the first thing we know about induction is that it is not mathematically valid. Statisticians have two main ways of evading this problem, which lead to the two main mathematical frameworks for statistical inference:\nEvasion 1: Bayesian reasoning. Bayesians respond to Hume that, whatever our a priori beliefs are about the world, we at least know how to update them in the light of experience using the mathematics of conditional probability. Our prior beliefs may not ultimately be justified, but there is only one rational way to update them (note this is somewhat disputed). We may hope that after enough experience they will “wash out” and observers with different prior beliefs will eventually converge in their beliefs after seeing enough data. We will study Bayesian statistics in a few weeks.\nEvasion 2: Inductive behavior (frequentist statistics). Another way of evading the problem is to design methods for inference whose fallibility can be quantified. As long as certain assumptions hold concerning the conditions under which the data were collected, we may be able to come up with methods that we can mathematically prove give correct conclusions with high probability."
  },
  {
    "objectID": "reader/introduction.html#coin-flipping",
    "href": "reader/introduction.html#coin-flipping",
    "title": "Course introduction",
    "section": "Coin flipping",
    "text": "Coin flipping\nAlthough physical randomizers like coins and dice seem to be the firmest ground on which we can build a theory of probability, recent work has found surprising"
  },
  {
    "objectID": "reader/bayes-interpretation.html",
    "href": "reader/bayes-interpretation.html",
    "title": "Interpretations of Probability and Sources of Priors",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/bayes-interpretation.html#interpretations-of-probability",
    "href": "reader/bayes-interpretation.html#interpretations-of-probability",
    "title": "Interpretations of Probability and Sources of Priors",
    "section": "1 Interpretations of Probability",
    "text": "1 Interpretations of Probability\nWhy do we model anything as random? What does probability mean in the real world?\n\n1.1 1. Long-run Frequency Over Repeated Trials\n\nExamples:\n\nRepeatedly flipping a coin\nShooting electrons at a double slit\n\n\nNote: “You can never step into the same river twice.”\n\n\n1.2 2. Systematic Random Sampling from a Population\n\nExamples:\n\nSurvey of 500 random voters\nRandom assignment to treatment/control in controlled experiments\n\n\n\n\n1.3 3. Subjective Uncertainty About an Outcome\n\nExamples:\n\nChance that President Biden is re-elected\nHiggs boson having a given mass\nP = NP\n\n\nNotes: - Could be broad intersubjective agreement - These are often conflated with #1 - What if survey sampling is pseudo-random? - Probably relying on shared ignorance"
  },
  {
    "objectID": "reader/bayes-interpretation.html#where-does-the-prior-come-from",
    "href": "reader/bayes-interpretation.html#where-does-the-prior-come-from",
    "title": "Interpretations of Probability and Sources of Priors",
    "section": "2 Where Does the Prior Come From?",
    "text": "2 Where Does the Prior Come From?\nBayesian rejoinder: Where does \\(P\\) come from?\nFour main sources for prior on \\(\\theta\\):\n\n2.1 Source 1: Subjective Beliefs\nPros: - Brings all relevant info to bear - Straightforward interpretation of posterior\nCons: - Posterior is therefore subjective - Embarrassing to write “I think” in abstract - Hard if \\(\\theta\\) high-dimensional or \\(P\\) nonparametric\nExample: Flip coin 20 times, get 7 heads - 0.5 probably a better estimate than 0.35 - My subjective prior on coins:\n\\[\n\\pi(\\theta) \\propto \\theta^{10}(1-\\theta)^{10}\n\\]\n\n\n2.2 Source 2: Objective or Vague Prior\nPros: - Using default prior removes subjectivity\nCons: - What does the posterior mean?\nExamples:\n\nFlat prior: \\(\\pi(\\theta) \\propto 1\\) on \\([0,1]\\)\n\nIndifference in \\(\\theta\\) parameterization\nOften improper, e.g., \\(\\pi(\\theta) \\propto 1\\) on \\(\\mathbb{R}\\), but usually ok\n\n\\(\\theta \\in \\mathbb{R}\\), flat prior on \\(\\mathbb{R}\\)\n\n\\(X|\\theta \\sim N(\\theta, \\sigma^2)\\)\n\\(\\pi(\\theta|x) \\propto p(x|\\theta)\\)\n\\(\\mathbb{E}[\\theta|x] = \\bar{x}\\), \\(\\theta|x \\sim N(\\bar{x}, \\frac{\\sigma^2}{n})\\)\n\nJeffreys prior: \\(\\pi(\\theta) \\propto \\sqrt{I(\\theta)}\\)\n\nHigher density where \\(p_\\theta\\) changing faster\nInvariant to parameterization (HW 5)\n\n\\(X|\\theta \\sim \\text{Binom}(n, \\theta)\\)\n\n\\(\\pi(\\theta) \\propto \\theta^{-1/2}(1-\\theta)^{-1/2} = \\text{Beta}(1/2, 1/2)\\)\n\\(\\theta \\to 0\\) or \\(1\\) as \\(\\theta \\to 0\\) or \\(1\\)\n\n\n\n\n2.3 Intersubjective Agreement\nData may effectively rule out most \\(\\theta\\) values, making posterior uncontroversial.\nExample: \\(X \\sim \\text{Binom}(10^4, \\theta)\\), observe \\(X = 3000\\) - SD \\(\\approx \\sqrt{n} = 10 \\approx 0.005\\) - Likelihood \\(\\theta|X = 0.3\\) outside \\([0.29, 0.31]\\) - All reasonable priors may be \\(\\approx\\) flat on \\([0.29, 0.31]\\) - \\(\\pi(\\theta|x) \\propto \\text{Lik}(\\theta|x) \\approx \\exp(-\\frac{(\\theta - 0.3)^2}{2(0.005)^2})\\) - \\(\\theta|x \\approx N(0.3, (0.005)^2)\\) (HW 5)\nData swamps everyone’s prior.\n\n\n2.4 Gaussian Sequence Model\n\\(X|\\theta \\sim N(\\theta, I_d)\\), \\(\\theta \\in \\mathbb{R}^d\\)\n\nJeffreys prior is flat: \\(\\pi(\\theta) \\propto 1\\) on \\(\\mathbb{R}^d\\)\n\\(\\pi(\\theta|x) \\propto N(\\theta|x, I_d)\\), \\(\\mathbb{E}[\\theta|x] = x\\)\nSame as UMVU\nWhat about \\(\\|\\theta\\|_1\\)? Recall:\n\n\\(\\hat{\\mu} = \\arg\\min_d \\|\\theta - d\\|_1 \\Rightarrow \\hat{\\mu}_j = \\text{sign}(x_j)(|x_j| - \\lambda)_+\\)\nRecall James-Stein: \\(\\|\\hat{\\mu}\\|_2^2 \\leq \\|x\\|_2^2 - 2d\\lambda + d\\lambda^2\\)\n\\(\\text{MSE}(\\theta) = \\mathbb{E}[\\|\\hat{\\mu} - \\theta\\|_2^2] = \\text{Var}(\\hat{\\mu}) + \\text{Bias}^2\\)\n\\(\\text{Var}(\\hat{\\mu}) \\leq d\\), \\(\\text{Bias}^2 \\leq d\\lambda^2\\)\n\n\nWhat went wrong? Examine Jeffreys prior: - \\(P(\\|\\theta\\|_2 \\leq r) = \\text{Vol}(\\text{Ball of radius } r) \\propto r^d\\) - \\(P(\\|\\theta\\|_2 \\geq r) = 1 - cr^d\\) - \\(P(\\|\\theta\\|_2 \\geq \\gamma d^{1/2}) \\approx 1\\) for \\(\\gamma &lt; 1\\)\nGrows rapidly. Prior expects \\(\\|\\theta\\|\\) to be huge.\n\n\n2.5 Source 3: Prior or Concurrent Experience\n\nMay have many instances of same problem\nAssume true \\(\\theta\\) values drawn from a population\nHierarchical Bayes / empirical Bayes\nCan be hard to choose right reference class\n\nExample: Estimate batting average - Player \\(i\\) has \\(n_i\\) at-bats, true batting avg \\(\\theta_i\\) - Hierarchical model (players \\(i=1,\\ldots,m\\)): - Hyperparameter \\(\\alpha, \\beta \\sim \\pi_0(\\alpha, \\beta)\\) (hyperprior) - \\(\\theta_i|\\alpha, \\beta \\sim \\text{Beta}(\\alpha, \\beta)\\) - \\(X_i|\\theta_i, n_i \\sim \\text{Binom}(n_i, \\theta_i)\\) - \\(\\theta_i|X \\sim \\text{Beta}(\\alpha + X_i, \\beta + n_i - X_i)\\) - \\(\\mathbb{E}[\\theta_i|X] = \\frac{\\alpha + X_i}{\\alpha + \\beta + n_i}\\) - If \\(m\\) large, \\(\\alpha, \\beta\\) may be almost known - Choice of \\(\\pi_0\\) doesn’t matter much - Reference class problem: which players to include?\n\n\n2.6 Flexibility of Bayes\nAny \\((\\pi, P, L, g(\\theta))\\) defined straightforwardly: \\[\n\\delta(x) = \\arg\\min_d \\int L(\\theta, d) \\pi(\\theta|x) d\\theta\n\\]\n\nProblem reduced to (possibly hard) computation\nPosterior is one-stop shop for all answers\nNo need for:\n\nSpecial family structure (exp fam, completeness)\nSpecial estimator (U-estimable)\nConvex or nice \\(L\\)\n\nHighly expressive modeling & estimation\nCaveat: Limited by ability to do computations (topic of next lecture)\n\n\n\n2.7 Source 4: Convenience Priors\n\nChoosing conjugate or other nice priors\nMuch faster computations, esp. in high dim\nBut what does the posterior mean?\n\nExample: - \\(X_i \\stackrel{iid}{\\sim} p\\), \\(p\\) unknown density on \\(\\mathbb{R}\\) - Estimand: \\(m =\\) median\\((p)\\) - Estimator: \\(\\delta(x) =\\) median\\((X)\\) - Good estimator: robust, nonparametric - Large \\(n\\): \\(\\delta(x) \\sim N(m, \\frac{1}{4np(m)^2})\\) - Not Bayes for any realistic prior\nBayes approach: 1. Define prior over \\(p\\) (infinite dim) 2. Calculate posterior (horrific unless we pick special prior) 3. Return e.g., \\(\\mathbb{E}[m|X]\\)\nIf it differs substantially from median\\((X)\\), do we trust it?"
  },
  {
    "objectID": "reader/bayes-interpretation.html#gaussian-hierarchical-model",
    "href": "reader/bayes-interpretation.html#gaussian-hierarchical-model",
    "title": "Interpretations of Probability and Sources of Priors",
    "section": "3 Gaussian Hierarchical Model",
    "text": "3 Gaussian Hierarchical Model\n\\(\\theta_i \\sim N(\\mu, \\tau^2)\\), \\(X_i|\\theta_i \\sim N(\\theta_i, \\sigma^2)\\)\nPosterior mean: \\[\n\\mathbb{E}[\\theta_i|X] = \\mathbb{E}[\\mathbb{E}[\\theta_i|X, \\mu, \\tau^2]|X] = \\mathbb{E}[\\frac{\\tau^2}{\\tau^2 + \\sigma^2}X_i + \\frac{\\sigma^2}{\\tau^2 + \\sigma^2}\\mu|X]\n\\]\nLinear shrinkage estimator: - Bayes optimal shrinkage estimated from data - Likelihood for \\(\\mu, \\tau^2\\) (marginalize over \\(\\theta_i\\)): - \\(X_i|\\mu, \\tau^2 \\sim N(\\mu, \\tau^2 + \\sigma^2)\\) - \\(\\bar{X} \\sim N(\\mu, \\frac{\\tau^2 + \\sigma^2}{n})\\) - \\(S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\sim \\frac{\\tau^2 + \\sigma^2}{n-1}\\chi^2_{n-1}\\)\nDefine \\(B = \\tau^2 + \\sigma^2\\): \\[\n\\delta(x) = \\mathbb{E}[\\mathbb{E}[\\theta_i|X, B]|X] = \\mathbb{E}[\\frac{B - \\sigma^2}{B}X_i + \\frac{\\sigma^2}{B}\\bar{X}|X]\n\\]\nConjugate prior: \\[\n\\pi(B|\\lambda, \\nu) \\propto B^{-\\nu/2-2}\\exp(-\\frac{\\lambda}{2B})\n\\]\n\\[\nB|X \\sim \\text{InvGamma}(\\frac{n+\\nu}{2}, \\frac{\\lambda + (n-1)S^2}{2})\n\\]\n\\[\n\\mathbb{E}[\\frac{1}{B}|X] = \\frac{n+\\nu}{\\lambda + (n-1)S^2}\n\\]\n\\[\n\\delta_i(x) = \\frac{(n-3)S^2}{(n-1)S^2 + \\lambda}X_i + \\frac{\\lambda + 2S^2}{(n-1)S^2 + \\lambda}\\bar{X}\n\\]\nMight want to truncate prior to \\([\\sigma^2, \\infty)\\) if \\(\\lambda\\) small."
  },
  {
    "objectID": "reader/testing-interpretation.html",
    "href": "reader/testing-interpretation.html",
    "title": "p-Values, Confidence Regions, and Misinterpreting Tests",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/testing-interpretation.html#p-values",
    "href": "reader/testing-interpretation.html#p-values",
    "title": "p-Values, Confidence Regions, and Misinterpreting Tests",
    "section": "1 p-Values",
    "text": "1 p-Values\n\n1.1 Informal Definition\nSuppose \\(\\phi(x)\\) rejects for \\(T(x) &gt; c\\). The p-value is:\n\\[p(x) = \\mathbb{P}_0(T(X) \\geq T(x)_{\\text{observed}}) = \\mathbb{P}_0(T(X) \\geq t)\\]\nExample: \\(X \\sim N(\\theta, 1)\\), \\(H_0: \\theta = 0\\) vs \\(H_1: \\theta \\neq 0\\)\nTwo-sided test rejects for large \\(|T(X)| = |X|\\):\n\\[p(x) = \\mathbb{P}_0(|X| \\geq |x|) = 2(1 - \\Phi(|x|))\\]\nThe two-sided p-value is \\(p(X)\\), where:\n\\[p(x) = \\mathbb{P}_0(|X| \\geq |x|) = 2\\min\\{\\Phi(x), 1-\\Phi(x)\\}\\]\n\n\n1.2 Formal Definition\nAssume we have a test \\(\\phi_\\alpha\\) for each significance level \\(\\alpha\\): \\(\\mathbb{E}_0[\\phi_\\alpha(X)] \\leq \\alpha\\)\nIn the non-randomized case: \\(\\phi_\\alpha(x) = 1\\{x \\in R_\\alpha\\}\\)\nAssume tests are monotone in \\(\\alpha\\): if \\(\\alpha \\leq \\alpha'\\), then \\(\\phi_\\alpha(x) \\leq \\phi_{\\alpha'}(x)\\)\n(In non-randomized case: \\(R_\\alpha \\subseteq R_{\\alpha'}\\))\nThen:\n\\[p(x) = \\inf\\{\\alpha \\in [0,1]: \\phi_\\alpha(x) = 1\\} = \\inf\\{\\alpha: x \\in R_\\alpha\\}\\]\nIt’s possible to define randomized p-value, but not worth it.\nNote: \\(p(x) \\leq \\alpha \\iff \\phi_\\alpha(x) = 1\\)\nFor \\(\\theta = \\theta_0\\): \\(\\mathbb{P}_0(p(X) \\leq \\alpha) = \\mathbb{E}_0[\\phi_\\alpha(X)] \\leq \\alpha\\)\np-value stochastically dominates \\(U(0,1)\\)\nIf \\(\\phi_\\alpha\\) rejects for large \\(T(X)\\), reduces to original definition.\nNote: The p-value depends on: - The model - Null hypothesis - The data AND - The choice of test\nExample: \\(X \\sim N(\\theta, I_d)\\), \\(H_0: \\theta = 0\\) vs \\(H_1: \\theta \\neq 0\\)\nWe can use \\(T(x) = \\|x\\|_2^2\\) (\\(\\chi^2\\) test) or \\(T(x) = \\max_i |x_i|\\) (max test)\nVery different p-values, power if \\(d\\) large Choice reflects belief about whether \\(\\theta\\) is sparse\n\n\n1.3 Accept/Reject Decisions\nAccept/reject decisions are not interesting Usually, we care how big \\(\\theta\\) is Tiny p-value doesn’t imply big \\(\\theta\\) Big p-value doesn’t imply small \\(\\theta\\) either"
  },
  {
    "objectID": "reader/testing-interpretation.html#confidence-regions",
    "href": "reader/testing-interpretation.html#confidence-regions",
    "title": "p-Values, Confidence Regions, and Misinterpreting Tests",
    "section": "2 Confidence Regions",
    "text": "2 Confidence Regions\nDefinition: \\(C: \\cX \\to \\cP(\\Theta)\\) is a \\(1-\\alpha\\) confidence set for \\(g(\\theta)\\) if:\n\\[\\mathbb{P}_\\theta(C(X) \\ni g(\\theta)) \\geq 1-\\alpha \\quad \\forall \\theta \\in \\Theta\\]\nWe say \\(C(x)\\) covers \\(g(\\theta)\\) if \\(C(x) \\ni g(\\theta)\\)\nCoverage probability: \\(\\mathbb{P}_\\theta(C(X) \\ni g(\\theta))\\)\n\\(\\inf_\\theta \\mathbb{P}_\\theta(C(X) \\ni g(\\theta))\\) is confidence level\nNote: \\(C(X)\\) is random, not \\(g(\\theta)\\)\nOften misinterpreted as Bayesian guarantee Say “\\(C(x)\\) has a 95% chance of covering” Not “\\(g(\\theta)\\) has a 95% chance of being in \\(C\\)” NEVER “95% chance \\(g(\\theta) \\in [0.5, 1.5]\\)” e.g.\n\n2.1 Duality of Tests/Confidence Sets\nSuppose we have a level \\(\\alpha\\) test \\(\\phi(\\cdot, a)\\) of \\(H_0: g(\\theta) = a\\) vs \\(H_1: g(\\theta) \\neq a\\), \\(\\forall a \\in \\Theta\\)\nWe can use it to make a confidence set for \\(g(\\theta)\\):\nLet \\(C(X) = \\{a: \\phi(X, a) = 0\\}\\) (all non-rejected values of \\(a\\))\nThen \\(\\mathbb{P}_\\theta(C(X) \\ni g(\\theta)) = \\mathbb{P}_\\theta(\\phi(X, g(\\theta)) = 0) \\geq 1-\\alpha\\)\nAlternatively, suppose \\(C(X)\\) is a \\(1-\\alpha\\) confidence set for \\(g(\\theta)\\)\nWe can use \\(C\\) to construct a test \\(\\phi\\) of \\(H_0: g(\\theta) = a\\) vs \\(H_1: g(\\theta) \\neq a\\):\n\\(\\phi(x) = 1\\{a \\notin C(x)\\}\\)\nFor \\(\\theta\\) s.t. \\(g(\\theta) = a\\):\n\\[\\mathbb{E}_\\theta[\\phi(X)] = \\mathbb{P}_\\theta(a \\notin C(X)) = \\mathbb{P}_\\theta(C(X) \\not\\ni g(\\theta)) \\leq \\alpha\\]\nThis is called inverting a test.\n\n\n2.2 Confidence Intervals/Bounds\nIf \\(C(X) = [C_L(X), C_U(X)]\\), we say: - \\(C(X)\\) is a confidence interval (CI) - \\(C_L(X)\\) is a lower confidence bound (LCB) - \\(C_U(X)\\) is an upper confidence bound (UCB)\nWe usually get LCB, UCB by inverting a one-sided test in appropriate direction Called uniformly most accurate (UMA) if test UMP\nGet CI by inverting a two-sided test Called UMAU if test is UMPU\nExample: \\(X \\sim \\text{Exp}(\\theta)\\), \\(n=1\\), \\(\\mathbb{E}[X] = \\frac{1}{\\theta}\\), \\(\\theta &gt; 0\\)\nCDF: \\(\\mathbb{P}_\\theta(X \\leq x) = 1 - e^{-\\theta x}\\)\nLCB: Invert test for \\(H_0: \\theta \\leq \\theta_0\\) Solve \\(\\alpha = 1 - \\mathbb{P}_{\\theta_0}(X \\leq x) = e^{-\\theta_0 x}\\)\n\\(\\theta_0 = -\\frac{1}{x}\\log(\\alpha)\\)\n\\(C_L(X) = -\\frac{1}{X}\\log(\\alpha)\\), \\(\\mathbb{P}_\\theta(\\theta \\geq C_L(X)) = 1-\\alpha\\)\nUCB: Similar \\(C_U(X) = -\\frac{1}{X}\\log(1-\\alpha)\\)\nEqual-tailed: Invert equal-tailed test of \\(H_0: \\theta = \\theta_0\\)\n\\(\\theta_0 e^{-\\theta_0 x} = \\frac{\\alpha}{2}\\), \\(1 - e^{-\\theta_0 x} = 1 - \\frac{\\alpha}{2}\\)\n\\(C(X) = [\\frac{-\\log(\\alpha/2)}{X}, \\frac{-\\log(\\alpha/2)}{X}]\\)\nSimilar for UMPU 2-sided test"
  },
  {
    "objectID": "reader/testing-interpretation.html#misinterpreting-hypothesis-tests",
    "href": "reader/testing-interpretation.html#misinterpreting-hypothesis-tests",
    "title": "p-Values, Confidence Regions, and Misinterpreting Tests",
    "section": "3 Misinterpreting Hypothesis Tests",
    "text": "3 Misinterpreting Hypothesis Tests\nHypothesis tests ubiquitous in science Common misinterpretations:\n\np &lt; 0.05, therefore there is an effect (or the effect size = the estimate)\np &gt; 0.05, therefore there is no effect\np = 10^-6, therefore the effect is huge\np &lt; 10^-6, therefore the data are significant and everything about our model is correct (in most naive interpretation)\nEffect CI for men is [0.2, 3.2], for women is [-0.2, 2.8], therefore there is an effect for men and not for women\n\nDichotomous test doesn’t eliminate uncertainty CIs usually less misleading to novices\nInterpreting tests is not easy or automatic Hypothesis tests let us ask specific questions under specific modeling assumptions Interpreting them requires care and experience\nTop-tier medical journals let people publish claims reporting p-values without saying what model was used or what test was employed Pretty bad when you think about it\nHyp. tests can be a good companion to critical thinking, never a substitute All models are wrong, some are useful, but need experience and theory to understand when assumptions do or don’t cause real trouble\n\n3.1 Common Objections to Hypothesis Testing\n\nWhy should I test \\(\\theta = 0\\)? Is \\(\\theta\\) ever exactly 0?\nA: a. Test \\(H_0: |\\theta| &lt; \\epsilon\\) if you want If \\(\\sigma_{\\hat{\\theta}} \\ll \\epsilon\\), not much difference\n\nMost two-sided tests justify directional interest: If \\(T &gt; c\\), declare \\(\\theta &gt; 0\\), if \\(T &lt; -c\\), declare \\(\\theta &lt; 0\\) with \\(\\mathbb{P}(\\text{false claim}) &lt; \\alpha\\)\nHarder to answer in non-parametric problems e.g. \\(H_0: P = Q\\) vs \\(H_1: P \\neq Q\\) for perm test, but alternative frameworks like Bayes force very strong assumptions on us\n\nPeople only like frequentist results like p-values, CIs because they mistake them for Bayesian results “95% chance \\(\\theta &gt; 0\\)” is misinterpreted as a claim about \\(\\mathbb{P}(\\theta &gt; 0 | X)\\)\nA: True, but subjective Bayesian results often misinterpreted as the posterior dist. of \\(\\theta\\) when really should be “posterior opinion about \\(\\theta\\)”"
  },
  {
    "objectID": "reader/convergence.html",
    "href": "reader/convergence.html",
    "title": "Asymptotic Theory: Convergence, Continuous Mapping, and Delta Method",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/convergence.html#introduction-to-asymptotic-theory",
    "href": "reader/convergence.html#introduction-to-asymptotic-theory",
    "title": "Asymptotic Theory: Convergence, Continuous Mapping, and Delta Method",
    "section": "1 Introduction to Asymptotic Theory",
    "text": "1 Introduction to Asymptotic Theory\nSo far, everything has been finite sample, often using special properties of model \\(\\cP\\) (e.g., exponential family) to do exact calculations. For generic models, exact calculations may be intractable or impossible. However, we may be able to approximate our problem with a simpler problem in which calculations are easy.\nTypically, we approximate by Gaussian by taking the limit as observations \\(n \\to \\infty\\). But this is only interesting if the approximation is good for reasonable sample sizes."
  },
  {
    "objectID": "reader/convergence.html#convergence",
    "href": "reader/convergence.html#convergence",
    "title": "Asymptotic Theory: Convergence, Continuous Mapping, and Delta Method",
    "section": "2 Convergence",
    "text": "2 Convergence\nLet \\(X_1, X_2, \\ldots \\in \\mathbb{R}^d\\) be a sequence of random vectors. We care about two kinds of convergence:\n\nConvergence in probability: \\(X_n \\to\\) a constant\nConvergence in distribution: \\(X_n \\to N(0, I_d)\\) (usually)\n\n\n2.1 Convergence in Probability\nWe say the sequence converges in probability to \\(c \\in \\mathbb{R}^d\\) (\\(X_n \\xrightarrow{p} c\\)) if:\n\\[\\mathbb{P}(\\|X_n - c\\| &gt; \\epsilon) \\to 0 \\quad \\forall \\epsilon &gt; 0\\]\n(Could really be any distance on any \\(\\cX\\))\nCan converge to a r.v. \\(X\\) too, but we don’t need this.\n\n\n2.2 Convergence in Distribution\nWe say the sequence converges in distribution to random variable \\(X\\) (\\(X_n \\xrightarrow{d} X\\)) if:\n\\[\\mathbb{E}[f(X_n)] \\to \\mathbb{E}[f(X)] \\text{ for all bounded continuous } f: \\cX \\to \\mathbb{R}\\]\nTheorem: \\(X_n, X \\in \\mathbb{R}\\). Fix \\(\\mathbb{P}(X = x) = 0\\). Let \\(F_n(x) = \\mathbb{P}(X_n \\leq x)\\), \\(F(x) = \\mathbb{P}(X \\leq x)\\). Then \\(X_n \\xrightarrow{d} X\\) iff \\(F_n(x) \\to F(x)\\) \\(\\forall x: F\\) is continuous at \\(x\\).\nAlso known as weak convergence.\n\n\n2.3 Example\nIf \\(X_n \\xrightarrow{d} X \\sim g\\), then \\(X_n \\xrightarrow{d} X\\):\n\\[F_n(x) = \\begin{cases}\n1 & \\text{if } x &gt; 0 \\\\\n1 - \\frac{1}{n} & \\text{if } x = 0 \\\\\n0 & \\text{if } x &lt; 0\n\\end{cases}\\]\n\\[F(x) = \\begin{cases}\n1 & \\text{if } x &gt; 0 \\\\\n0 & \\text{if } x \\leq 0\n\\end{cases}\\]\n\n\n2.4 Proof: \\(X_n \\xrightarrow{p} c \\implies X_n \\xrightarrow{d} c\\)\nLet \\(f_\\epsilon(x) = \\max\\{1 - \\frac{\\|x-c\\|}{\\epsilon}, 0\\}\\). Then \\(\\forall \\epsilon &gt; 0\\):\n\\[\\mathbb{P}(\\|X_n - c\\| &gt; \\epsilon) \\leq \\mathbb{E}[1 - f_\\epsilon(X_n)] \\to 0\\]\n\\(f\\) bounded continuous. Note \\(\\mathbb{E}[f(c)] = f(c)\\).\n\\(\\forall \\epsilon &gt; 0\\), \\(\\exists \\delta &gt; 0\\) s.t. \\(\\|x - c\\| &lt; \\delta \\implies |f(x) - f(c)| &lt; \\epsilon\\)\n\\[|\\mathbb{E}[f(X_n)] - f(c)| \\leq |\\mathbb{E}[f(X_n) - f(c)]1_{\\|X_n - c\\| &lt; \\delta}| + |\\mathbb{E}[(f(X_n) - f(c))1_{\\|X_n - c\\| \\geq \\delta}]|\\] \\[\\leq \\epsilon + 2\\sup |f| \\cdot \\mathbb{P}(\\|X_n - c\\| \\geq \\delta)\\]\nFor sufficiently large \\(n\\). \\(\\square\\)\nIn a sequence of statistical models \\(\\cP_n = \\{P_{n,\\theta}: \\theta \\in \\Theta\\}\\) with \\(X_n \\sim P_{n,\\theta}\\), we say \\(\\hat{\\theta}_n\\) is consistent for \\(g(\\theta)\\) if \\(\\hat{\\theta}_n \\xrightarrow{p} g(\\theta)\\), meaning:\n\\[\\mathbb{P}_\\theta(|\\hat{\\theta}_n - g(\\theta)| &gt; \\epsilon) \\to 0\\]\nUsually, we omit the index \\(n\\); sequence is implicit.\n\n\n2.5 Law of Large Numbers (LLN)\nLet \\(\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\)\nIf \\(\\mathbb{E}|X_i| &lt; \\infty\\), \\(\\mathbb{E}X_i = \\mu\\), then \\(\\bar{X}_n \\xrightarrow{p} \\mu\\)\n\n\n2.6 Central Limit Theorem (CLT)\nIf \\(\\text{Var}(X_i) = \\sigma^2 &lt; \\infty\\), then \\(\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)\\)\nThere are stronger versions of both the LLN and CLT, but this will generally be enough for us."
  },
  {
    "objectID": "reader/convergence.html#continuous-mapping-theorem",
    "href": "reader/convergence.html#continuous-mapping-theorem",
    "title": "Asymptotic Theory: Convergence, Continuous Mapping, and Delta Method",
    "section": "3 Continuous Mapping Theorem",
    "text": "3 Continuous Mapping Theorem\nTheorem (Continuous Mapping): Let \\(g\\) be continuous, \\(X_n, X\\) r.v.’s.\n\nIf \\(X_n \\xrightarrow{d} X\\), then \\(g(X_n) \\xrightarrow{d} g(X)\\)\nIf \\(X_n \\xrightarrow{p} c\\), then \\(g(X_n) \\xrightarrow{p} g(c)\\)\n\nProof: \\(f\\) bounded continuous \\(\\implies f \\circ g\\) bounded continuous If \\(X_n \\xrightarrow{d} X\\), then \\(\\mathbb{E}[f(g(X_n))] \\to \\mathbb{E}[f(g(X))]\\) \\(X_n \\xrightarrow{p} c\\) special case with \\(X \\equiv c\\)"
  },
  {
    "objectID": "reader/convergence.html#slutskys-theorem",
    "href": "reader/convergence.html#slutskys-theorem",
    "title": "Asymptotic Theory: Convergence, Continuous Mapping, and Delta Method",
    "section": "4 Slutsky’s Theorem",
    "text": "4 Slutsky’s Theorem\nTheorem (Slutsky): Assume \\(X_n \\xrightarrow{d} X\\), \\(Y_n \\xrightarrow{p} c\\). Then:\n\n\\(X_n + Y_n \\xrightarrow{d} X + c\\)\n\\(X_n Y_n \\xrightarrow{d} cX\\)\n\\(X_n / Y_n \\xrightarrow{d} X/c\\) if \\(c \\neq 0\\)\n\nProof: Show \\((X_n, Y_n) \\xrightarrow{d} (X, c)\\), apply continuous mapping.\nWouldn’t normally be true that \\(X_n \\xrightarrow{d} X\\), \\(Y_n \\xrightarrow{d} Y\\) implies \\(X_n + Y_n \\xrightarrow{d} X + Y\\) without specifying joint dist."
  },
  {
    "objectID": "reader/convergence.html#delta-method",
    "href": "reader/convergence.html#delta-method",
    "title": "Asymptotic Theory: Convergence, Continuous Mapping, and Delta Method",
    "section": "5 Delta Method",
    "text": "5 Delta Method\nTheorem (Delta Method): If \\(\\sqrt{n}(X_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)\\) \\(f(x)\\) differentiable at \\(x = \\mu\\)\nThen \\(\\sqrt{n}(f(X_n) - f(\\mu)) \\xrightarrow{d} N(0, [f'(\\mu)]^2 \\sigma^2)\\)\nInstant: \\(X \\sim N(\\mu, \\sigma^2/n) \\implies f(X) \\sim N(f(\\mu), [f'(\\mu)]^2 \\sigma^2/n + o(1/n))\\)\nProof: \\(f(X_n) = f(\\mu) + f'(\\mu)(X_n - \\mu) + o(X_n - \\mu)\\) \\(\\sqrt{n}(f(X_n) - f(\\mu)) = f'(\\mu)\\sqrt{n}(X_n - \\mu) + \\sqrt{n}o(X_n - \\mu)\\) \\(N(0, \\sigma^2) + 0 \\xrightarrow{d} N(0, [f'(\\mu)]^2 \\sigma^2)\\)\nMultivariate: \\(\\sqrt{n}(X_n - \\mu) \\xrightarrow{d} N(0, \\Sigma)\\), \\(f: \\mathbb{R}^d \\to \\mathbb{R}^k\\) Derivative \\(Df(\\mu)\\) exists at \\(\\mu\\)\nThen \\(\\sqrt{n}(f(X_n) - f(\\mu)) \\xrightarrow{d} N(0, Df(\\mu) \\Sigma Df(\\mu)^T)\\)\n\\(N(f(\\mu), Df(\\mu) \\Sigma Df(\\mu)^T/n + o(1/n))\\) if \\(k=1\\)\n\n5.1 Example: Delta Method Application\n\\(X_1, \\ldots, X_n \\sim \\text{Unif}[0, \\theta]\\) iid \\(Y_1, \\ldots, Y_m \\sim \\text{Unif}[0, \\theta]\\) iid \\(X, Y\\) independent\nFor large \\(n, m\\), what is the distribution of \\(T = \\frac{\\bar{X}}{\\bar{Y}}\\)?\n\n\\(\\sqrt{n}(\\bar{X} - \\frac{\\theta}{2}) \\xrightarrow{d} N(0, \\frac{\\theta^2}{12})\\) as \\(n \\to \\infty\\)\n\\(\\sqrt{m}(\\bar{Y} - \\frac{\\theta}{2}) \\xrightarrow{d} N(0, \\frac{\\theta^2}{12})\\) as \\(m \\to \\infty\\)\n\n\\(T_n = \\frac{\\bar{X}}{\\bar{Y}} = \\frac{\\theta/2}{\\theta/2} = 1 + O_p(n^{-1/2} + m^{-1/2})\\)\nLet \\(f(x,y) = x/y\\)\n\\(f_x'(\\frac{\\theta}{2}, \\frac{\\theta}{2}) = \\frac{1}{\\theta/2} = \\frac{2}{\\theta}\\) \\(f_y'(\\frac{\\theta}{2}, \\frac{\\theta}{2}) = -\\frac{\\theta/2}{(\\theta/2)^2} = -\\frac{2}{\\theta}\\)\n\\(f'(\\frac{\\theta}{2}, \\frac{\\theta}{2}) = (\\frac{2}{\\theta}, -\\frac{2}{\\theta})\\)\n\\(\\sqrt{n}(T_n - 1) \\xrightarrow{d} N(0, \\frac{4}{\\theta^2} \\cdot \\frac{\\theta^2}{12} \\cdot \\frac{1}{n} + \\frac{4}{\\theta^2} \\cdot \\frac{\\theta^2}{12} \\cdot \\frac{n}{m})\\)\n\\(= N(0, \\frac{1}{3n} + \\frac{1}{3m})\\)\nMore accurate: \\(\\sqrt{n}(T_n - 1) \\xrightarrow{d} N(0, \\frac{4}{3}(1 + \\frac{n}{m}))\\)\n\n\n5.2 What if \\(\\mu = 0\\)?\n\nWhat if \\(\\mu_1 = \\mu_2 = 0\\)? Conclusion still holds: \\(T_n = \\frac{1 + O_p(n^{-1/2})}{1 + O_p(m^{-1/2})} = 1 + O_p(n^{-1/2} + m^{-1/2})\\)\n\nNote: \\(\\frac{1}{1 + n^{-1/2}} \\to 1\\) (continuous mapping) Not Slutsky\nSo \\(n(T_n - 1)^2 \\xrightarrow{d} \\chi^2_1\\) (continuous mapping) Why not delta method?\nIn general, can do higher-order Taylor expansions for delta method if derivatives \\(\\neq 0\\):\n\\(f(X_n) = f(\\mu) + f'(\\mu)(X_n - \\mu) + \\frac{1}{2}f''(\\mu)(X_n - \\mu)^2 + O_p(n^{-3/2})\\)\nIf \\(f'(\\mu) = 0\\), use second-order term: \\(n(f(X_n) - f(\\mu)) \\xrightarrow{d} \\frac{1}{2}f''(\\mu)\\chi^2_1\\)"
  },
  {
    "objectID": "reader/testing-linear.html",
    "href": "reader/testing-linear.html",
    "title": "t and F Distributions, Canonical and General Linear Models",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/testing-linear.html#t-and-f-distributions",
    "href": "reader/testing-linear.html#t-and-f-distributions",
    "title": "t and F Distributions, Canonical and General Linear Models",
    "section": "1 t and F Distributions",
    "text": "1 t and F Distributions\n\n1.1 Definitions and Properties\n\n\\(\\chi^2_d\\): If \\(X_i \\sim N(0,1)\\) iid, then \\(V = \\sum_{i=1}^d X_i^2 \\sim \\chi^2_d\\)\n\n\\(\\mathbb{E}[V] = d\\), \\(\\text{Var}(V) = 2d\\)\nCLT: \\(V \\approx N(d, 2d)\\) for large \\(d\\)\n\n\\(t_d\\): If \\(Z \\sim N(0,1)\\) and \\(V \\sim \\chi^2_d\\) independent, then \\(T = \\frac{Z}{\\sqrt{V/d}} \\sim t_d\\)\n\nInformally: \\(Y \\sim N(\\mu, 1)\\), \\(\\hat{\\sigma}^2 \\sim \\frac{\\chi^2_d}{d}\\), \\(\\frac{Y - \\mu}{\\hat{\\sigma}} \\sim t_d\\)\n\nIf \\(Z \\sim N(0,1)\\) and \\(V \\sim \\chi^2_d\\), \\(Z/\\sqrt{V} \\sim t_d\\) as \\(d \\to \\infty\\)\nIf \\(V \\sim \\chi^2_d\\) and \\(V_2 \\sim \\chi^2_{d_2}\\) independent, \\(V/V_2 \\sim F_{d,d_2}\\), then:\n\\(\\frac{V/d}{V_2/d_2} \\sim F_{d,d_2}\\) as \\(d,d_2 \\to \\infty\\)\n\nNote: If \\(T \\sim t_d\\), then \\(T^2 \\sim F_{1,d}\\)\nRecall: \\(Z \\sim N_d(\\mu, \\Sigma)\\) iff \\(A Z + b \\sim N_d(A\\mu + b, A\\Sigma A^T)\\)\n\n\n1.2 Geometric Interpretation\nLet \\(X \\sim N_n(\\mu, I_n)\\), \\(\\mu = \\alpha e_1\\), where \\(\\{e_1, \\ldots, e_n\\}\\) is a complete orthonormal basis (e.g., via Gram-Schmidt)\n\\(X = \\sum_{i=1}^n \\langle X, e_i \\rangle e_i = \\alpha e_1 + \\sum_{i=1}^n Z_i e_i\\), \\(Z_i \\sim N(0,1)\\) iid\nNew basis: \\(Z = Q'X\\), \\(\\|X\\|^2 = \\|Z\\|^2\\)\n\\(\\begin{pmatrix} Z_1 \\\\ Z_{2:n} \\end{pmatrix} = \\begin{pmatrix} Q_1' \\\\ Q_{2:n}' \\end{pmatrix} X \\sim N\\left(\\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix}, I_n\\right)\\)\n\\(Z_1 = Q_1' X \\sim N(\\alpha, 1)\\) \\(Z_{2:n} = Q_{2:n}' X \\sim N(0, I_{n-1})\\)\n\\(S^2 = \\|Z_{2:n}\\|^2 = \\sum_{i=2}^n Z_i^2\\) and \\(Z_1\\) independent (we already knew from Basu)\n\n\n1.3 Geometric Interpretation (continued)\nIndependent of total magnitude under \\(H_0\\):\n\n\\(n\\bar{X}^2 = \\alpha^2 \\sim \\text{Gamma}(\\frac{1}{2}, \\frac{2}{n})\\)\n\\(\\sum_{i=1}^n (X_i - \\bar{X})^2 \\sim \\text{Gamma}(\\frac{n-1}{2}, 2)\\)\n\\(\\|X\\|^2 = n\\bar{X}^2 + \\sum_{i=1}^n (X_i - \\bar{X})^2 \\sim \\text{Gamma}(\\frac{n}{2}, 2)\\)\n\n\\(\\frac{n\\bar{X}^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\sim \\text{Beta}(\\frac{1}{2}, \\frac{n-1}{2})\\) independent of \\(\\|X\\|^2\\)\n\\(F_{1,n-1}\\) related to \\(\\text{Beta}(\\frac{1}{2}, \\frac{n-1}{2})\\): If \\(U \\sim \\text{Beta}(\\frac{a}{2}, \\frac{b}{2})\\), then \\(\\frac{b}{a} \\cdot \\frac{U}{1-U} \\sim F_{a,b}\\)"
  },
  {
    "objectID": "reader/testing-linear.html#canonical-linear-model",
    "href": "reader/testing-linear.html#canonical-linear-model",
    "title": "t and F Distributions, Canonical and General Linear Models",
    "section": "2 Canonical Linear Model",
    "text": "2 Canonical Linear Model\nAssume \\(Z = \\begin{pmatrix} Z_1 \\\\ Z_2 \\end{pmatrix} \\sim N\\left(\\begin{pmatrix} \\mu \\\\ 0 \\end{pmatrix}, \\sigma^2 I_d\\right)\\), \\(d = d_0 + d_1\\), \\(\\mu \\in \\mathbb{R}^{d_0}\\), \\(\\sigma^2 &gt; 0\\)\nTest \\(H_0: \\mu = 0\\) vs \\(H_1: \\mu \\neq 0\\) (or possibly one-sided if \\(d_0 = 1\\))\nExponential Family:\n\\[f(z) = f(z_0, z_1) = \\frac{1}{(2\\pi\\sigma^2)^{d/2}} \\exp\\left(-\\frac{\\|z_1\\|^2 + \\|z_0 - \\mu\\|^2}{2\\sigma^2}\\right)\\]\n\n2.1 Case 1: \\(\\sigma^2\\) Known\nCondition on \\(Z_1\\), reject for large/small/extreme \\(Z_0\\)\n\\(Z_0 \\sim N(\\mu, \\sigma^2 I_{d_0})\\)\n\\(\\chi^2\\) test: Reject for large \\(\\|Z_0\\|^2\\)\nt-test: If \\(d_0 = 1\\), reject for large \\(|Z_0|\\)\n\n\n2.2 Case 2: \\(\\sigma^2\\) Unknown\nCondition on \\(Z_1\\), \\(\\|Z_1\\|^2\\), \\(\\|Z_0\\|^2\\) sufficient\nReject for large/small/extreme \\(Z_0\\)\nReject for large \\(\\frac{\\|Z_0\\|^2/d_0}{\\|Z_1\\|^2/d_1} \\sim F_{d_0,d_1}\\) under \\(H_0\\)\nF-test: \\(d_0 &gt; 1\\), Reject for conditionally large \\(\\|Z_0\\|^2\\)\nReject for large \\(\\frac{\\|Z_0\\|^2/d_0}{\\|Z_1\\|^2/d_1} \\sim F_{d_0,d_1}\\)\nt-test: \\(d_0 = 1\\), Reject for conditionally large \\(|Z_0|\\)\nReject for large \\(\\frac{|Z_0|}{\\sqrt{\\|Z_1\\|^2/d_1}} \\sim t_{d_1}\\)\nHere, \\(\\frac{\\|Z_1\\|^2}{d_1}\\) functioning as estimator of \\(\\sigma^2\\): \\(\\mathbb{E}[\\frac{\\|Z_1\\|^2}{d_1}] = \\sigma^2\\), \\(\\text{Var}(\\frac{\\|Z_1\\|^2}{d_1}) = \\frac{2\\sigma^4}{d_1}\\)\nGeneral case: \\(Z \\sim N(\\mu, \\sigma^2 I_d)\\), \\(\\mu \\not\\in \\mathbb{R}^{d_0} \\times \\{0\\}^{d_1}\\)\nTranslate problem:\n\\(Z_0 \\sim N_{d_0}(\\mu_0, \\sigma^2 I_{d_0})\\) \\(Z_1 \\sim N_{d_1}(\\mu_1, \\sigma^2 I_{d_1})\\)\nCan do some tests with \\(Z - \\mu_1\\) replacing \\(Z\\)\nInvert: \\(1-\\alpha\\) CI: \\(\\mu_0 \\in Z_0 \\pm \\sigma t_{d_1,1-\\alpha/2} \\sqrt{\\frac{\\|Z_1 - \\mu_1\\|^2}{d_1}}\\)\n\\(1-\\alpha\\) confidence ellipsoid: \\(\\|\\mu_0 - Z_0\\|^2 \\leq \\frac{d_0}{d_1} \\|Z_1 - \\mu_1\\|^2 F_{d_0,d_1,1-\\alpha}\\)\n\\(1-\\alpha\\) prediction interval: \\(Z_{\\text{new}} \\in Z_0 \\pm \\sigma t_{d_1,1-\\alpha/2} \\sqrt{1 + \\frac{\\|Z_1 - \\mu_1\\|^2}{d_1}}\\)"
  },
  {
    "objectID": "reader/testing-linear.html#general-linear-model",
    "href": "reader/testing-linear.html#general-linear-model",
    "title": "t and F Distributions, Canonical and General Linear Models",
    "section": "3 General Linear Model",
    "text": "3 General Linear Model\nMany problems can be put into canonical linear model after change of basis.\n\n3.1 Basic Setup\nObserve \\(Y \\sim N(X\\beta, \\sigma^2 I_n)\\), \\(\\sigma^2\\) known or unknown Test \\(\\beta \\in \\Theta_0\\) vs \\(\\beta \\in \\Theta_1\\) where \\(\\Theta_0 \\subset \\Theta_1\\) are subspaces of \\(\\mathbb{R}^p\\) \\(\\text{dim}(\\Theta_0) = d_0\\), \\(\\text{dim}(\\Theta_1) = d = d_0 + d_1\\)\nIdea: rotate into canonical form\n\\(d_0\\), \\(d_1\\), \\(n-d\\) \\(\\Theta_0\\), \\(\\Theta_1 \\setminus \\Theta_0\\), \\(\\mathbb{R}^n \\setminus \\Theta_1\\)\n\\(Q = (Q_0 | Q_1 | Q_2)\\) orthonormal basis for \\((\\Theta_0 | \\Theta_1 \\setminus \\Theta_0 | \\mathbb{R}^n \\setminus \\Theta_1)\\)\n\\(Z = Q'Y \\sim N_n(Q'\\beta, \\sigma^2 I_n)\\)\n\\(H_0: Q_1'\\beta = 0\\)\nDo \\(Z\\) \\(\\chi^2\\) or \\(F\\) test as appropriate\n\n\n3.2 Example 1: Linear Regression\n\\(Y_i = X_i'\\beta + \\epsilon_i\\), \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) \\(Y \\sim N_n(X\\beta, \\sigma^2 I_n)\\), \\(X \\in \\mathbb{R}^{n \\times p}\\)\nAssume \\(X\\) has full column rank \\(\\Theta = X\\beta \\in \\Theta = \\text{Span}(X_1, \\ldots, X_p)\\)\n\\(H_0: \\beta = (\\beta_0', 0')' \\in \\Theta_0 = \\text{Span}(X_1, \\ldots, X_q)\\) or \\(\\beta_q = 0\\) if \\(d_1 = 1\\)\n\\(\\|\\hat{\\beta} - \\beta\\|^2 = \\|Y - \\text{Proj}_\\Theta Y\\|^2\\) \\(\\hat{\\beta} = \\arg\\min_\\beta \\|Y - X\\beta\\|^2 = (X'X)^{-1}X'Y\\)\n\\(\\hat{Y} = X\\hat{\\beta}\\)\nResidual sum of squares (RSS): \\(\\|Y - \\hat{Y}\\|^2 = \\|Y - X\\hat{\\beta}\\|^2\\) \\(\\text{RSS}_0 - \\text{RSS}_1\\)\nF-statistic is:\n\\[F = \\frac{(\\text{RSS}_0 - \\text{RSS}_1)/d_1}{\\text{RSS}_1/(n-d)} \\sim F_{d_1,n-d}\\]\n\\(n-d\\) called residual degrees of freedom\nLet \\(X = (X_0 | X_1)\\), \\(X \\in \\mathbb{R}^{n \\times p}\\) Let \\(X_1^\\perp = X_1 - \\text{Proj}_{X_0} X_1\\) \\(X = (X_0 | X_0^\\perp)\\)\nReparametrize: \\(X_1^\\perp \\beta_1 = X_1 \\beta_1 - X_0 \\beta_0\\) \\(\\Theta = X\\beta = X_0 \\beta_0 + X_1^\\perp \\beta_1\\)\n\\(\\hat{\\beta}_1 = (X_1^{\\perp'} X_1^\\perp)^{-1} X_1^{\\perp'} Y\\) \\(\\|\\hat{\\beta}_1\\|^2 = \\text{RSS}_0 - \\text{RSS}_1\\) \\(\\text{SE}(\\hat{\\beta}_1) = \\hat{\\sigma}^2 (X_1^{\\perp'} X_1^\\perp)^{-1}\\)\nt-statistic: \\(t = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)} \\sim t_{n-d}\\)\n\n\n3.3 Example 2: Two-sample t-test (equal variance)\n\\(Y_1, \\ldots, Y_n \\sim N(\\mu_1, \\sigma^2)\\), \\(Y_{n+1}, \\ldots, Y_{n+m} \\sim N(\\mu_2, \\sigma^2)\\)\n\\(Y = (Y_1, \\ldots, Y_{n+m})'\\), \\(\\mathbb{E}[Y] = \\mu_1 1_n + \\mu_2 1_m\\) Model: \\(\\Theta = \\text{Span}(1_{n+m}, (1_n', 0_m')')\\)\n\\(H_0: \\mu_1 = \\mu_2 \\implies \\Theta_0 = \\text{Span}(1_{n+m})\\)\n\\(d_0 = 1\\), \\(d = 2\\), \\(d_1 = n+m-2\\)\nOrthogonalize \\(1_{n+m}\\)\nReject for large:\n\\[t = \\frac{\\bar{Y}_1 - \\bar{Y}_2}{\\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{1}{m}}} \\sim t_{n+m-2}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (Y_i - \\bar{Y}_1)^2 + \\sum_{i=1}^m (Y_i - \\bar{Y}_2)^2}{n+m-2}\\)\n\n\n3.4 Example 3: One-way ANOVA (fixed effects)\n\\(Y_{ki} \\sim N(\\mu_k, \\sigma^2)\\), \\(k=1,\\ldots,m\\), \\(i=1,\\ldots,n\\)\n\\(H_0: \\mu_1 = \\cdots = \\mu_m\\)\n\\(Y_{ki} = \\mu + \\alpha_k + \\epsilon_{ki}\\), \\(\\sum \\alpha_k = 0\\)\n\\(\\bar{Y}_{k\\cdot} = \\frac{1}{n} \\sum_{i=1}^n Y_{ki}\\), ${Y} = \\frac{1}{mn"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 210a: Theoretical Statistics",
    "section": "",
    "text": "Under construction"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Statistics 210a: Theoretical Statistics",
    "section": "Schedule",
    "text": "Schedule\nHere we use Quarto’s EJS templating.\nFor a style similar to that of Stat 20, modify index.qmd to point to assets/{schedule-alt.ejs,buttons-alt.ejs} and to {schedule-alt.yml,buttons-alt.yml} and _quarto.yml to point to assets/styles-alt.scss.\n\n\n   Week 1\n   \n   \n   \n   \n           \n           Aug 29:\n           Lecture 1 Introduction\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 2\n   \n   \n   \n   \n           \n           Sep 3:\n           Lecture 2 Probability\n                \n                  Handwritten notes\n                \n           \n           \n           Sep 5:\n           Lecture 3 Decision theory\n                \n                  Handwritten notes\n                \n           \n           \n           Sep 6:\n           Recitation 1 Probability review\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 3\n   \n   \n   \n   \n           \n           Sep 10:\n           Lecture 4 Estimation\n                \n                  Handwritten notes\n                \n           \n           \n           Sep 11:\n           Homework 1 Homework 1 due 11:59pm\n                \n                  LaTeX template\n                \n           \n           \n           Sep 12:\n           Lecture 5 Sufficiency\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 17\n   \n   \n   \n   \n           \n           Dec 18:\n           Exam  Final Exam, 8-11am\n                \n           \n   \n   \n\nNo matching items"
  },
  {
    "objectID": "units/unit2.html",
    "href": "units/unit2.html",
    "title": "Unit 2: Next",
    "section": "",
    "text": "This is an example of using an ipynb file as source rather than qmd. It follows instructions from https://github.com/DS-100/course-notes/README.md.\nLink to the data."
  },
  {
    "objectID": "units/unit2.html#latex",
    "href": "units/unit2.html#latex",
    "title": "Unit 2: Next",
    "section": "\\(\\LaTeX\\)",
    "text": "\\(\\LaTeX\\)\nHere is some \\(\\LaTeX\\). \\[\n\\theta = 7\n\\]"
  },
  {
    "objectID": "units/unit2.html#evaluated-python-code",
    "href": "units/unit2.html#evaluated-python-code",
    "title": "Unit 2: Next",
    "section": "Evaluated Python code",
    "text": "Evaluated Python code\nNote that to get code output shown, the underlying notebook must have executed the code.\n\n\nCode\na=7\nprint(a)\n\n\n7"
  },
  {
    "objectID": "units/unit2.html#callout",
    "href": "units/unit2.html#callout",
    "title": "Unit 2: Next",
    "section": "Callout",
    "text": "Callout\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title."
  },
  {
    "objectID": "units/unit1.html",
    "href": "units/unit1.html",
    "title": "Unit 1: Intro",
    "section": "",
    "text": "This is an example of using qmd as the source document."
  },
  {
    "objectID": "units/unit1.html#evaluated-python-code-chunk-with-a-plot",
    "href": "units/unit1.html#evaluated-python-code-chunk-with-a-plot",
    "title": "Unit 1: Intro",
    "section": "Evaluated Python code chunk, with a plot",
    "text": "Evaluated Python code chunk, with a plot\n\n\nCode\nimport numpy as np\nx = np.random.normal(size=100)\nimport matplotlib.pyplot as plt\nplt.hist(x)\nplt.show()\nnp.mean(x)\n\n\n\n\n\n-0.1326276863453903"
  },
  {
    "objectID": "units/unit1.html#latex",
    "href": "units/unit1.html#latex",
    "title": "Unit 1: Intro",
    "section": "\\(\\LaTeX\\)",
    "text": "\\(\\LaTeX\\)\n\\[\n\\theta = \\int_0^\\infty f(x,\\theta)d\\theta\n\\]"
  },
  {
    "objectID": "units/unit1.html#latex-macro",
    "href": "units/unit1.html#latex-macro",
    "title": "Unit 1: Intro",
    "section": "\\(\\LaTeX\\) macro",
    "text": "\\(\\LaTeX\\) macro\n\nWarning: need to look back at this as having include-before-body in the yaml causes extra space at top of page.\n\n\\[\nA = X \\trans Y\n\\]"
  },
  {
    "objectID": "units/unit1.html#styled-div-via-direct-html",
    "href": "units/unit1.html#styled-div-via-direct-html",
    "title": "Unit 1: Intro",
    "section": "Styled div via direct html",
    "text": "Styled div via direct html\n\nThis content can be styled via the border class."
  },
  {
    "objectID": "units/unit1.html#a-callout",
    "href": "units/unit1.html#a-callout",
    "title": "Unit 1: Intro",
    "section": "A callout",
    "text": "A callout\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title."
  },
  {
    "objectID": "units/unit1.html#tabset",
    "href": "units/unit1.html#tabset",
    "title": "Unit 1: Intro",
    "section": "Tabset",
    "text": "Tabset\n\nRPython\n\n\nThis code is not executed.\nfizz_buzz &lt;- function(fbnums = 1:50) {\n  output &lt;- dplyr::case_when(\n    fbnums %% 15 == 0 ~ \"FizzBuzz\",\n    fbnums %% 3 == 0 ~ \"Fizz\",\n    fbnums %% 5 == 0 ~ \"Buzz\",\n    TRUE ~ as.character(fbnums)\n  )\n  print(output)\n}\n\nfizz_buzz(3)\n\n\nThis code is executed.\n\n\nCode\ndef fizz_buzz(num):\n  if num % 15 == 0:\n    print(\"FizzBuzz\")\n  elif num % 5 == 0:\n    print(\"Buzz\")\n  elif num % 3 == 0:\n    print(\"Fizz\")\n  else:\n    print(num)\n    \nfizz_buzz(3)\n\n\nFizz"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "[UNDER CONSTRUCTION]\nStat 210A is an introductory Ph.D.-level course in theoretical statistics. It is a fast-paced and demanding course intended to prepare students for research careers in statistics."
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "Syllabus",
    "section": "Course information",
    "text": "Course information\n\nInstructors\n\nPrimary Instructor Prof. Will Fithian\n\nOffice Hours: Tuesday 3:30-4:30pm on Zoom, Thursday 9:30-10:30am in Evans 301\nEmail: wfithian@berkeley.edu\n\nGSI Dohyeong Ki\n\nOffice Hours: Wednesday 9-10am, Friday 4:30-5:30pm (location TBD)\nEmail: dohyeong_ki@berkeley.edu\n\n\n\n\nCourse schedule\n\nLectures: Tuesday and Thursday 11am-12:30pm, Evans 60\nRecitation sections: Every second F 3:30-4:30pm (location TBD), starting September 6\nElection week:\n\nBlanket 2-day extension on homework 9 (due Friday November 8, 11:59pm)\n\nThanksgiving week:\n\nTuesday November 26: lecture 11-12:30 on Zoom, 3:30-4:30 OH on Zoom\nNo lecture Thursday November 28\nNo in-person office hours\n\nFinal exam review: (a.k.a. last recitation section) Friday, December\nFinal exam: Wed December 18, 8-11am\n\n\n\nCourse communications\n\nLecture videos and homework solutions at [https://bcourses.berkeley.edu bCourses]\nEmail policy: You can email me or Dohyeong about administrative questions, with “[Stat 210A]” in the subject line. No math over email, please.\nEd page for announcements and technical discussion (no homework spoilers!)\nGradescope for turning in homework"
  },
  {
    "objectID": "syllabus.html#about-stat-210a",
    "href": "syllabus.html#about-stat-210a",
    "title": "Syllabus",
    "section": "About Stat 210A",
    "text": "About Stat 210A\n\nWhat is the theory of statistics?\nStatistics is the study of methods that use data to understand the world. Statistical methods are used throughout the natural and social sciences, in machine learning and artificial intelligence, and in engineering. Despite the ubiquitous use of statistics, its practitioners are perpetually accused of not actually understanding what they are doing. Statistics theory is, broadly speaking, the subject of what exactly we are doing when we apply statistical methods.\nWhile there are many possible ways to analyze data, most (but certainly not all) statistical methods are based on statistical modeling: treating the data as a realization of some random data-generating process with attributes, usually called parameters, that are a priori unknown. The goal of the analyst, then, is to use the data to draw accurate inferences about these parameters and/or to make accurate predictions about future data. If the modeling has been done well (a very big “if”) then these unknown parameters will correspond well to whatever real-world questions initially motivated the analysis. Applied statistics courses like Stat 215A and B delve deeply into questions about how to ensure that the statistical modeling exercise successfully captures something interesting about reality.\nIn this course we will instead focus on how the analyst can use the data most effectively within the context of a given mathematical setup. We will discuss the structure of statistical models, how to evaluate the quality of a statistical method, how to design good methods for new settings, and the philosophy of Bayesian vs frequentist modeling frameworks. We will cover estimation, confidence intervals, and hypothesis testing, in parametric and nonparametric methods, in finite samples and asymptotic regimes.\n\n\nTopics\nStatistical decision theory (frequentist and Bayesian), exponential families, point estimation, hypothesis testing, resampling methods, estimating equations and maximum likelihood, empirical Bayes, large-sample theory, high-dimensional testing, multiple testing and selective inference.\n\n\nPrerequisites\nThe course prerequisites are linear algebra, analysis, probability, and statistics.\n\n\nRelationship of Stat 210A to other Berkeley courses\nStat 210A focuses on classical statistical contexts: either inference in finite samples, or in fixed-dimensional asymptotic regimes. Stat 210B (for which 210A is a prerequisite) is more technical and covers topics like empirical process theory and high-dimensional statistics.\nBerkeley’s graduate course on Statistical Learning Theory (CS 281A / Stat 241A) is also very popular and has some overlap in its topics. Roughly speaking, it is more tilted toward “machine learning”: it spends more time on topics in predictive modeling (i.e. classification and regression, which are covered in Stat 215A), optimization, and signal processing, but spends less time on inferential questions and (I believe) does not cover topics like hypothesis testing, confidence intervals, and causal inference. Both courses cover estimation and exponential families."
  },
  {
    "objectID": "syllabus.html#references",
    "href": "syllabus.html#references",
    "title": "Syllabus",
    "section": "References",
    "text": "References\nThe online notes for this course are self-contained, however it can be helpful to see a different presentation in the following supplementary texts (all links are to public websites or Springer Link):\n\nKeener, Theoretical Statistics: Topics for a Core Course, Springer 2010.\nLehmann and Casella, Theory of Point Estimation, Springer 1998.\nLehmann and Romano, Testing Statistical Hypotheses, Springer 2005.\nCandes, Stats 300C Lecture notes, Stanford 2016.\n\nUndergrad-level review texts for prerequisites:\n\nAxler, Linear Algebra Done Right\nAbbott, Understanding Analysis\nAdhikari & Pitman, Probability for Data Science"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nYour final grade is based on:\n\nWeekly problem sets: 50%\nFinal exam: 50%\n\nLateness policy: Homework must be submitted to Gradescope at 11:59pm on Wednesday nights. Late problem sets will not be accepted, but we will drop your lowest two grades.\nCollaboration policy: For homework, you are welcome to work with each other or consult articles or textbooks online, with the following caveats:\n\nYou must write up your solution by yourself.\nYou may NOT consult any solutions from previous iterations of this course.\nNo generative AI allowed for problem sets.\nIf you collaborate or use any resources other than course texts, you must acknowledge your collaborators and the resources you used.\n\nAcademic integrity: You are expected to abide by the Berkeley honor code. Violating the collaboration policy, or cheating in any other way, will result in a failing grade for the semester and you will be reported to the University Office of Student Conduct.\nWhile the final exam is nominally one half of the grade, it is quite difficult and typically accounts for most of the variance in final course grades. If you take shortcuts on the homework, you will save yourself time in the short run, but you may do quite poorly on the final."
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\nStudents with disabilities: Please see me as soon as possible if you need particular accommodations, and we will work out the necessary arrangements.\nScheduling conflicts: Please notify me in writing by the second week of the semester about any known or potential extracurricular conflicts (such as religious observances, graduate or medical school interviews, or team activities). I will try my best to help you with making accommodations, but cannot promise them in all cases. In the event there is no mutually-workable solution, you may be dropped from the class.\nExam accommodations: If you need accommodations on the final exam due to disability, or unavoidable travel or time conflict, please fill out the exam exam accommodation form by Friday, October 4 so that I can make arrangements. To ensure exam integrity I much prefer for all students to take the exam in Berkeley at the regularly scheduled time (8am December 18th), but will try to work with you if you have a conflict."
  }
]