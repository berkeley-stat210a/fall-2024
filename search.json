[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "We’ll use data from several real world situations in class."
  },
  {
    "objectID": "data.html#data-1",
    "href": "data.html#data-1",
    "title": "Data",
    "section": "Data 1",
    "text": "Data 1"
  },
  {
    "objectID": "data.html#data-2",
    "href": "data.html#data-2",
    "title": "Data",
    "section": "Data 2",
    "text": "Data 2"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Who can / should take this course?\nThis is a fast-paced and demanding course designed to prepare PhD students for research careers in statistics. Undergraduates and PhD students in other fields are very welcome to take the class, and many have succeeded in the past. But you should be prepared to work hard.\nCan I take this course if I have a conflict with the lecture time?\nAttendance is not taken at lectures but attending is an important part of taking the course. If you want to take another course at the same time, you can do it as long as the other professor is OK with your not attending their lectures.\nHow can I prepare for the course?\nThe course prerequisites are undergraduate-level linear algebra, real analysis, and a year of upper-division probability and statistics. If you are unsure of your background in one or more of these, the texts below can be useful review materials. All books linked below, except Gelman & Hill, should be available for free with a Berkeley library subscription. Email me if you have trouble accessing them.\n\nLinear algebra: Fluency with undergrad-level abstract linear algebra is essential to understanding the course content (numerical tools like LU or Cholesky decompositions are not essential). Chapters 1-3 and 5-6 of Linear Algebra Done Right (Axler) are good review materials.\nReal analysis: Familiarity with and intuition for ideas of infinite sequences, convergence, Taylor approximations, etc. should be enough. Chapters 1-3 of Understanding Analysis (Abbott) are good review materials.\nProbability: The whole course involves probability. If Chapters 1-6, 8-9, 13-17, and 23 of Probability for Data Science (Adhikari and Pitman) aren’t mostly review for you, I strongly recommend studying them before the course begins.\nStatistics: We will derive all statistical results from first principles, but at a fairly technical level. If you have never seen them in an applied context, Chapters 1-6 of Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman & Hill) should give you some frame of reference."
  },
  {
    "objectID": "units/unit1.html",
    "href": "units/unit1.html",
    "title": "Unit 1: Intro",
    "section": "",
    "text": "This is an example of using qmd as the source document."
  },
  {
    "objectID": "units/unit1.html#evaluated-python-code-chunk-with-a-plot",
    "href": "units/unit1.html#evaluated-python-code-chunk-with-a-plot",
    "title": "Unit 1: Intro",
    "section": "Evaluated Python code chunk, with a plot",
    "text": "Evaluated Python code chunk, with a plot\n\n\nCode\nimport numpy as np\nx = np.random.normal(size=100)\nimport matplotlib.pyplot as plt\nplt.hist(x)\nplt.show()\nnp.mean(x)\n\n\n\n\n\n-0.001558925080133442"
  },
  {
    "objectID": "units/unit1.html#latex",
    "href": "units/unit1.html#latex",
    "title": "Unit 1: Intro",
    "section": "\\(\\LaTeX\\)",
    "text": "\\(\\LaTeX\\)\n\\[\n\\theta = \\int_0^\\infty f(x,\\theta)d\\theta\n\\]"
  },
  {
    "objectID": "units/unit1.html#latex-macro",
    "href": "units/unit1.html#latex-macro",
    "title": "Unit 1: Intro",
    "section": "\\(\\LaTeX\\) macro",
    "text": "\\(\\LaTeX\\) macro\n\nWarning: need to look back at this as having include-before-body in the yaml causes extra space at top of page.\n\n\\[\nA = X \\trans Y\n\\]"
  },
  {
    "objectID": "units/unit1.html#styled-div-via-direct-html",
    "href": "units/unit1.html#styled-div-via-direct-html",
    "title": "Unit 1: Intro",
    "section": "Styled div via direct html",
    "text": "Styled div via direct html\n\nThis content can be styled via the border class."
  },
  {
    "objectID": "units/unit1.html#a-callout",
    "href": "units/unit1.html#a-callout",
    "title": "Unit 1: Intro",
    "section": "A callout",
    "text": "A callout\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title."
  },
  {
    "objectID": "units/unit1.html#tabset",
    "href": "units/unit1.html#tabset",
    "title": "Unit 1: Intro",
    "section": "Tabset",
    "text": "Tabset\n\nRPython\n\n\nThis code is not executed.\nfizz_buzz &lt;- function(fbnums = 1:50) {\n  output &lt;- dplyr::case_when(\n    fbnums %% 15 == 0 ~ \"FizzBuzz\",\n    fbnums %% 3 == 0 ~ \"Fizz\",\n    fbnums %% 5 == 0 ~ \"Buzz\",\n    TRUE ~ as.character(fbnums)\n  )\n  print(output)\n}\n\nfizz_buzz(3)\n\n\nThis code is executed.\n\n\nCode\ndef fizz_buzz(num):\n  if num % 15 == 0:\n    print(\"FizzBuzz\")\n  elif num % 5 == 0:\n    print(\"Buzz\")\n  elif num % 3 == 0:\n    print(\"Fizz\")\n  else:\n    print(num)\n    \nfizz_buzz(3)\n\n\nFizz"
  },
  {
    "objectID": "units/unit2.html",
    "href": "units/unit2.html",
    "title": "Unit 2: Next",
    "section": "",
    "text": "This is an example of using an ipynb file as source rather than qmd. It follows instructions from https://github.com/DS-100/course-notes/README.md.\nLink to the data."
  },
  {
    "objectID": "units/unit2.html#latex",
    "href": "units/unit2.html#latex",
    "title": "Unit 2: Next",
    "section": "\\(\\LaTeX\\)",
    "text": "\\(\\LaTeX\\)\nHere is some \\(\\LaTeX\\). \\[\n\\theta = 7\n\\]"
  },
  {
    "objectID": "units/unit2.html#evaluated-python-code",
    "href": "units/unit2.html#evaluated-python-code",
    "title": "Unit 2: Next",
    "section": "Evaluated Python code",
    "text": "Evaluated Python code\nNote that to get code output shown, the underlying notebook must have executed the code.\n\n\nCode\na=7\nprint(a)\n\n\n7"
  },
  {
    "objectID": "units/unit2.html#callout",
    "href": "units/unit2.html#callout",
    "title": "Unit 2: Next",
    "section": "Callout",
    "text": "Callout\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 210a: Theoretical Statistics",
    "section": "",
    "text": "Course enrollment for undergraduates\n\n\n\nIf you are an undergraduate who wants to take this course, please fill out the permission code request form to let me know about your background. Anyone considering taking the course is encouraged to read the frequently asked questions regarding preparation and review materials."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Statistics 210a: Theoretical Statistics",
    "section": "Schedule",
    "text": "Schedule\n\n\n   Week 1\n   \n   \n   \n   \n           \n           Aug 29:\n           Lecture 1 Introduction\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 2\n   \n   \n   \n   \n           \n           Sep 3:\n           Lecture 2 Probability\n                \n                  Handwritten notes\n                \n           \n           \n           Sep 5:\n           Lecture 3 Estimation\n                \n                  Handwritten notes\n                \n           \n           \n           Sep 6:\n           Recitation 1 Probability review\n                \n           \n   \n   \n   Week 3\n   \n   \n   \n   \n           \n           Sep 10:\n           Lecture 4 Sufficiency\n                \n                  Handwritten notes\n                \n           \n           \n           Sep 11:\n           Homework 1 Problems\n                \n                  LaTeX source\n                \n           \n           \n           Sep 12:\n           Lecture 5 Exponential families\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 3\n   \n   \n   \n   \n           \n           Sep 17:\n           Lecture 6 Completeness\n                \n                  Handwritten notes\n                \n           \n           \n           Sep 18:\n           Homework 2 Problems\n                \n                  LaTeX source\n                \n           \n           \n           Sep 19:\n           Lecture 7 Unbiased estimation\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 4\n   \n   \n   \n   \n           \n           Sep 24:\n           Lecture 8 Score and Fisher information\n                \n                  Handwritten notes\n                \n           \n           \n           Sep 25:\n           Homework 3 Problems\n                \n                  LaTeX source\n                \n           \n           \n           Sep 26:\n           Lecture 9 Bayes estimation\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 5\n   \n   \n   \n   \n           \n           Oct 1:\n           Lecture 10 Bayesian interpretation\n                \n                  Handwritten notes\n                \n           \n           \n           Oct 2:\n           Homework 4 Problems\n                \n                  LaTeX source\n                \n           \n           \n           Oct 3:\n           Lecture 11 Hierarchical Bayes\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 6\n   \n   \n   \n   \n           \n           Oct 9:\n           Homework 5 Problems\n                \n                  LaTeX source\n                  Data for Problem 5\n                \n           \n           \n           Oct 10:\n           Lecture 12 James-Stein estimator\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 7\n   \n   \n   \n   \n           \n           Oct 15:\n           Lecture 13 Minimax estimation\n                \n                  Handwritten notes\n                \n           \n           \n           Oct 16:\n           Homework 6 Problems\n                \n                  LaTeX source\n                \n           \n           \n           Oct 17:\n           Lecture 14 Hypothesis testing\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 8\n   \n   \n   \n   \n           \n           Oct 22:\n           Lecture 15 Testing with one parameter\n                \n                  Handwritten notes\n                \n           \n           \n           Oct 23:\n           Homework 7 Problems\n                \n                  LaTeX source\n                \n           \n           \n           Oct 24:\n           Lecture 16 p-Values and confidence sets\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 9\n   \n   \n   \n   \n           \n           Oct 29:\n           Lecture 17 Testing with nuisance parameters\n                \n                  Handwritten notes\n                \n           \n           \n           Oct 30:\n           Homework 8 Problems\n                \n                  LaTeX source\n                \n           \n           \n           Oct 31:\n           Lecture 18 Linear models\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 10\n   \n   \n   \n   \n           \n           Nov 5:\n           Lecture 19 Linear models\n                \n                  Handwritten notes\n                \n           \n           \n           Nov 7:\n           Lecture 20 Asymptotics\n                \n                  Handwritten notes\n                \n           \n           \n           Nov 8:\n           Homework 9 Problems\n                \n                  LaTeX source\n                \n           \n   \n   \n   Week 11\n   \n   \n   \n   \n           \n           Nov 12:\n           Lecture 21 Maximum Likelihood estimator\n                \n                  Handwritten notes\n                \n           \n           \n           Nov 14:\n           Lecture 22 Likelihood-based inference\n                \n                  Handwritten notes\n                \n           \n           \n           Nov 15:\n           Homework 10 Problems\n                \n                  LaTeX source\n                  Data for Problem 3\n                \n           \n   \n   \n   Week 12\n   \n   \n   \n   \n           \n           Nov 19:\n           Lecture 23 MLE consistency\n                \n                  Handwritten notes\n                \n           \n           \n           Nov 21:\n           Lecture 24 Likelihood Based Inference (continuation)\n                \n                  Handwritten notes\n                \n           \n           \n           Nov 22:\n           Homework 11 Problems\n                \n                  LaTeX source\n                \n           \n   \n   \n   Week 13\n   \n   \n   \n   \n           \n           Nov 26:\n           Lecture 25 Bootstrap\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 14\n   \n   \n   \n   \n           \n           Dec 3:\n           Lecture 26 Bootstrap (Continuation)\n                \n                  Handwritten notes\n                \n           \n           \n           Dec 3:\n           Lecture 27 Multiple testing\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 17\n   \n   \n   \n   \n           \n           Dec 18:\n           Exam  Final Exam, 8-11am\n                \n           \n   \n   \n\nNo matching items"
  },
  {
    "objectID": "handwritten-notes.html",
    "href": "handwritten-notes.html",
    "title": "Handwritten notes",
    "section": "",
    "text": "Lecture 1: Induction\nLecture 2: Probability\nLecture 3: Estimation\nLecture 4: Sufficiency\nLecture 5: Exponential families\nLecture 6: Completeness\nLecture 7: Unbiased estimation"
  },
  {
    "objectID": "handwritten-notes.html#handwritten-lecture-notes",
    "href": "handwritten-notes.html#handwritten-lecture-notes",
    "title": "Handwritten notes",
    "section": "",
    "text": "Lecture 1: Induction\nLecture 2: Probability\nLecture 3: Estimation\nLecture 4: Sufficiency\nLecture 5: Exponential families\nLecture 6: Completeness\nLecture 7: Unbiased estimation"
  },
  {
    "objectID": "reader/testing-linear.html",
    "href": "reader/testing-linear.html",
    "title": "t and F Distributions, Canonical and General Linear Models",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/testing-linear.html#t-and-f-distributions",
    "href": "reader/testing-linear.html#t-and-f-distributions",
    "title": "t and F Distributions, Canonical and General Linear Models",
    "section": "1 t and F Distributions",
    "text": "1 t and F Distributions\n\n1.1 Definitions and Properties\n\n\\(\\chi^2_d\\): If \\(X_i \\sim N(0,1)\\) iid, then \\(V = \\sum_{i=1}^d X_i^2 \\sim \\chi^2_d\\)\n\n\\(\\mathbb{E}[V] = d\\), \\(\\text{Var}(V) = 2d\\)\nCLT: \\(V \\approx N(d, 2d)\\) for large \\(d\\)\n\n\\(t_d\\): If \\(Z \\sim N(0,1)\\) and \\(V \\sim \\chi^2_d\\) independent, then \\(T = \\frac{Z}{\\sqrt{V/d}} \\sim t_d\\)\n\nInformally: \\(Y \\sim N(\\mu, 1)\\), \\(\\hat{\\sigma}^2 \\sim \\frac{\\chi^2_d}{d}\\), \\(\\frac{Y - \\mu}{\\hat{\\sigma}} \\sim t_d\\)\n\nIf \\(Z \\sim N(0,1)\\) and \\(V \\sim \\chi^2_d\\), \\(Z/\\sqrt{V} \\sim t_d\\) as \\(d \\to \\infty\\)\nIf \\(V \\sim \\chi^2_d\\) and \\(V_2 \\sim \\chi^2_{d_2}\\) independent, \\(V/V_2 \\sim F_{d,d_2}\\), then:\n\\(\\frac{V/d}{V_2/d_2} \\sim F_{d,d_2}\\) as \\(d,d_2 \\to \\infty\\)\n\nNote: If \\(T \\sim t_d\\), then \\(T^2 \\sim F_{1,d}\\)\nRecall: \\(Z \\sim N_d(\\mu, \\Sigma)\\) iff \\(A Z + b \\sim N_d(A\\mu + b, A\\Sigma A^T)\\)\n\n\n1.2 Geometric Interpretation\nLet \\(X \\sim N_n(\\mu, I_n)\\), \\(\\mu = \\alpha e_1\\), where \\(\\{e_1, \\ldots, e_n\\}\\) is a complete orthonormal basis (e.g., via Gram-Schmidt)\n\\(X = \\sum_{i=1}^n \\langle X, e_i \\rangle e_i = \\alpha e_1 + \\sum_{i=1}^n Z_i e_i\\), \\(Z_i \\sim N(0,1)\\) iid\nNew basis: \\(Z = Q'X\\), \\(\\|X\\|^2 = \\|Z\\|^2\\)\n\\(\\begin{pmatrix} Z_1 \\\\ Z_{2:n} \\end{pmatrix} = \\begin{pmatrix} Q_1' \\\\ Q_{2:n}' \\end{pmatrix} X \\sim N\\left(\\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix}, I_n\\right)\\)\n\\(Z_1 = Q_1' X \\sim N(\\alpha, 1)\\) \\(Z_{2:n} = Q_{2:n}' X \\sim N(0, I_{n-1})\\)\n\\(S^2 = \\|Z_{2:n}\\|^2 = \\sum_{i=2}^n Z_i^2\\) and \\(Z_1\\) independent (we already knew from Basu)\n\n\n1.3 Geometric Interpretation (continued)\nIndependent of total magnitude under \\(H_0\\):\n\n\\(n\\bar{X}^2 = \\alpha^2 \\sim \\text{Gamma}(\\frac{1}{2}, \\frac{2}{n})\\)\n\\(\\sum_{i=1}^n (X_i - \\bar{X})^2 \\sim \\text{Gamma}(\\frac{n-1}{2}, 2)\\)\n\\(\\|X\\|^2 = n\\bar{X}^2 + \\sum_{i=1}^n (X_i - \\bar{X})^2 \\sim \\text{Gamma}(\\frac{n}{2}, 2)\\)\n\n\\(\\frac{n\\bar{X}^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\sim \\text{Beta}(\\frac{1}{2}, \\frac{n-1}{2})\\) independent of \\(\\|X\\|^2\\)\n\\(F_{1,n-1}\\) related to \\(\\text{Beta}(\\frac{1}{2}, \\frac{n-1}{2})\\): If \\(U \\sim \\text{Beta}(\\frac{a}{2}, \\frac{b}{2})\\), then \\(\\frac{b}{a} \\cdot \\frac{U}{1-U} \\sim F_{a,b}\\)"
  },
  {
    "objectID": "reader/testing-linear.html#canonical-linear-model",
    "href": "reader/testing-linear.html#canonical-linear-model",
    "title": "t and F Distributions, Canonical and General Linear Models",
    "section": "2 Canonical Linear Model",
    "text": "2 Canonical Linear Model\nAssume \\(Z = \\begin{pmatrix} Z_1 \\\\ Z_2 \\end{pmatrix} \\sim N\\left(\\begin{pmatrix} \\mu \\\\ 0 \\end{pmatrix}, \\sigma^2 I_d\\right)\\), \\(d = d_0 + d_1\\), \\(\\mu \\in \\mathbb{R}^{d_0}\\), \\(\\sigma^2 &gt; 0\\)\nTest \\(H_0: \\mu = 0\\) vs \\(H_1: \\mu \\neq 0\\) (or possibly one-sided if \\(d_0 = 1\\))\nExponential Family:\n\\[f(z) = f(z_0, z_1) = \\frac{1}{(2\\pi\\sigma^2)^{d/2}} \\exp\\left(-\\frac{\\|z_1\\|^2 + \\|z_0 - \\mu\\|^2}{2\\sigma^2}\\right)\\]\n\n2.1 Case 1: \\(\\sigma^2\\) Known\nCondition on \\(Z_1\\), reject for large/small/extreme \\(Z_0\\)\n\\(Z_0 \\sim N(\\mu, \\sigma^2 I_{d_0})\\)\n\\(\\chi^2\\) test: Reject for large \\(\\|Z_0\\|^2\\)\nt-test: If \\(d_0 = 1\\), reject for large \\(|Z_0|\\)\n\n\n2.2 Case 2: \\(\\sigma^2\\) Unknown\nCondition on \\(Z_1\\), \\(\\|Z_1\\|^2\\), \\(\\|Z_0\\|^2\\) sufficient\nReject for large/small/extreme \\(Z_0\\)\nReject for large \\(\\frac{\\|Z_0\\|^2/d_0}{\\|Z_1\\|^2/d_1} \\sim F_{d_0,d_1}\\) under \\(H_0\\)\nF-test: \\(d_0 &gt; 1\\), Reject for conditionally large \\(\\|Z_0\\|^2\\)\nReject for large \\(\\frac{\\|Z_0\\|^2/d_0}{\\|Z_1\\|^2/d_1} \\sim F_{d_0,d_1}\\)\nt-test: \\(d_0 = 1\\), Reject for conditionally large \\(|Z_0|\\)\nReject for large \\(\\frac{|Z_0|}{\\sqrt{\\|Z_1\\|^2/d_1}} \\sim t_{d_1}\\)\nHere, \\(\\frac{\\|Z_1\\|^2}{d_1}\\) functioning as estimator of \\(\\sigma^2\\): \\(\\mathbb{E}[\\frac{\\|Z_1\\|^2}{d_1}] = \\sigma^2\\), \\(\\text{Var}(\\frac{\\|Z_1\\|^2}{d_1}) = \\frac{2\\sigma^4}{d_1}\\)\nGeneral case: \\(Z \\sim N(\\mu, \\sigma^2 I_d)\\), \\(\\mu \\not\\in \\mathbb{R}^{d_0} \\times \\{0\\}^{d_1}\\)\nTranslate problem:\n\\(Z_0 \\sim N_{d_0}(\\mu_0, \\sigma^2 I_{d_0})\\) \\(Z_1 \\sim N_{d_1}(\\mu_1, \\sigma^2 I_{d_1})\\)\nCan do some tests with \\(Z - \\mu_1\\) replacing \\(Z\\)\nInvert: \\(1-\\alpha\\) CI: \\(\\mu_0 \\in Z_0 \\pm \\sigma t_{d_1,1-\\alpha/2} \\sqrt{\\frac{\\|Z_1 - \\mu_1\\|^2}{d_1}}\\)\n\\(1-\\alpha\\) confidence ellipsoid: \\(\\|\\mu_0 - Z_0\\|^2 \\leq \\frac{d_0}{d_1} \\|Z_1 - \\mu_1\\|^2 F_{d_0,d_1,1-\\alpha}\\)\n\\(1-\\alpha\\) prediction interval: \\(Z_{\\text{new}} \\in Z_0 \\pm \\sigma t_{d_1,1-\\alpha/2} \\sqrt{1 + \\frac{\\|Z_1 - \\mu_1\\|^2}{d_1}}\\)"
  },
  {
    "objectID": "reader/testing-linear.html#general-linear-model",
    "href": "reader/testing-linear.html#general-linear-model",
    "title": "t and F Distributions, Canonical and General Linear Models",
    "section": "3 General Linear Model",
    "text": "3 General Linear Model\nMany problems can be put into canonical linear model after change of basis.\n\n3.1 Basic Setup\nObserve \\(Y \\sim N(X\\beta, \\sigma^2 I_n)\\), \\(\\sigma^2\\) known or unknown Test \\(\\beta \\in \\Theta_0\\) vs \\(\\beta \\in \\Theta_1\\) where \\(\\Theta_0 \\subset \\Theta_1\\) are subspaces of \\(\\mathbb{R}^p\\) \\(\\text{dim}(\\Theta_0) = d_0\\), \\(\\text{dim}(\\Theta_1) = d = d_0 + d_1\\)\nIdea: rotate into canonical form\n\\(d_0\\), \\(d_1\\), \\(n-d\\) \\(\\Theta_0\\), \\(\\Theta_1 \\setminus \\Theta_0\\), \\(\\mathbb{R}^n \\setminus \\Theta_1\\)\n\\(Q = (Q_0 | Q_1 | Q_2)\\) orthonormal basis for \\((\\Theta_0 | \\Theta_1 \\setminus \\Theta_0 | \\mathbb{R}^n \\setminus \\Theta_1)\\)\n\\(Z = Q'Y \\sim N_n(Q'\\beta, \\sigma^2 I_n)\\)\n\\(H_0: Q_1'\\beta = 0\\)\nDo \\(Z\\) \\(\\chi^2\\) or \\(F\\) test as appropriate\n\n\n3.2 Example 1: Linear Regression\n\\(Y_i = X_i'\\beta + \\epsilon_i\\), \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) \\(Y \\sim N_n(X\\beta, \\sigma^2 I_n)\\), \\(X \\in \\mathbb{R}^{n \\times p}\\)\nAssume \\(X\\) has full column rank \\(\\Theta = X\\beta \\in \\Theta = \\text{Span}(X_1, \\ldots, X_p)\\)\n\\(H_0: \\beta = (\\beta_0', 0')' \\in \\Theta_0 = \\text{Span}(X_1, \\ldots, X_q)\\) or \\(\\beta_q = 0\\) if \\(d_1 = 1\\)\n\\(\\|\\hat{\\beta} - \\beta\\|^2 = \\|Y - \\text{Proj}_\\Theta Y\\|^2\\) \\(\\hat{\\beta} = \\arg\\min_\\beta \\|Y - X\\beta\\|^2 = (X'X)^{-1}X'Y\\)\n\\(\\hat{Y} = X\\hat{\\beta}\\)\nResidual sum of squares (RSS): \\(\\|Y - \\hat{Y}\\|^2 = \\|Y - X\\hat{\\beta}\\|^2\\) \\(\\text{RSS}_0 - \\text{RSS}_1\\)\nF-statistic is:\n\\[F = \\frac{(\\text{RSS}_0 - \\text{RSS}_1)/d_1}{\\text{RSS}_1/(n-d)} \\sim F_{d_1,n-d}\\]\n\\(n-d\\) called residual degrees of freedom\nLet \\(X = (X_0 | X_1)\\), \\(X \\in \\mathbb{R}^{n \\times p}\\) Let \\(X_1^\\perp = X_1 - \\text{Proj}_{X_0} X_1\\) \\(X = (X_0 | X_0^\\perp)\\)\nReparametrize: \\(X_1^\\perp \\beta_1 = X_1 \\beta_1 - X_0 \\beta_0\\) \\(\\Theta = X\\beta = X_0 \\beta_0 + X_1^\\perp \\beta_1\\)\n\\(\\hat{\\beta}_1 = (X_1^{\\perp'} X_1^\\perp)^{-1} X_1^{\\perp'} Y\\) \\(\\|\\hat{\\beta}_1\\|^2 = \\text{RSS}_0 - \\text{RSS}_1\\) \\(\\text{SE}(\\hat{\\beta}_1) = \\hat{\\sigma}^2 (X_1^{\\perp'} X_1^\\perp)^{-1}\\)\nt-statistic: \\(t = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)} \\sim t_{n-d}\\)\n\n\n3.3 Example 2: Two-sample t-test (equal variance)\n\\(Y_1, \\ldots, Y_n \\sim N(\\mu_1, \\sigma^2)\\), \\(Y_{n+1}, \\ldots, Y_{n+m} \\sim N(\\mu_2, \\sigma^2)\\)\n\\(Y = (Y_1, \\ldots, Y_{n+m})'\\), \\(\\mathbb{E}[Y] = \\mu_1 1_n + \\mu_2 1_m\\) Model: \\(\\Theta = \\text{Span}(1_{n+m}, (1_n', 0_m')')\\)\n\\(H_0: \\mu_1 = \\mu_2 \\implies \\Theta_0 = \\text{Span}(1_{n+m})\\)\n\\(d_0 = 1\\), \\(d = 2\\), \\(d_1 = n+m-2\\)\nOrthogonalize \\(1_{n+m}\\)\nReject for large:\n\\[t = \\frac{\\bar{Y}_1 - \\bar{Y}_2}{\\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{1}{m}}} \\sim t_{n+m-2}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (Y_i - \\bar{Y}_1)^2 + \\sum_{i=1}^m (Y_i - \\bar{Y}_2)^2}{n+m-2}\\)\n\n\n3.4 Example 3: One-way ANOVA (fixed effects)\n\\(Y_{ki} \\sim N(\\mu_k, \\sigma^2)\\), \\(k=1,\\ldots,m\\), \\(i=1,\\ldots,n\\)\n\\(H_0: \\mu_1 = \\cdots = \\mu_m\\)\n\\(Y_{ki} = \\mu + \\alpha_k + \\epsilon_{ki}\\), \\(\\sum \\alpha_k = 0\\)\n\\(\\bar{Y}_{k\\cdot} = \\frac{1}{n} \\sum_{i=1}^n Y_{ki}\\), ${Y} = \\frac{1}{mn"
  },
  {
    "objectID": "reader/minimax-estimation.html",
    "href": "reader/minimax-estimation.html",
    "title": "Minimax Estimation",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/minimax-estimation.html#minimax-risk-estimator",
    "href": "reader/minimax-estimation.html#minimax-risk-estimator",
    "title": "Minimax Estimation",
    "section": "1 Minimax Risk Estimator",
    "text": "1 Minimax Risk Estimator\n\n1.1 Definition of Minimax Risk\nThe last idea for choosing an estimator: worst-case risk.\nMinimize \\(\\sup_\\theta R(\\theta, \\delta)\\)\nThe minimum achievable sup risk is called the minimax risk of the estimation problem:\n\\[r = \\inf_\\delta \\sup_\\theta R(\\theta, \\delta)\\]\nAn estimator \\(\\delta\\) is called minimax if it achieves the minimax risk, i.e.,\n\\[\\sup_\\theta R(\\theta, \\delta) = r\\]\n\n\n1.2 Game Theory Interpretation\n\nAnalyst chooses estimator \\(\\delta\\)\nNature chooses parameter \\(\\theta\\) to maximize risk\n\nNote: Nature chooses \\(\\theta\\) adversarially, not \\(X\\).\nCompare to Bayes where Nature chooses prior from a known distribution (Nature plays a specific mixed strategy).\nWe will look for Nature’s Nash equilibrium strategy."
  },
  {
    "objectID": "reader/minimax-estimation.html#least-favorable-priors",
    "href": "reader/minimax-estimation.html#least-favorable-priors",
    "title": "Minimax Estimation",
    "section": "2 Least Favorable Priors",
    "text": "2 Least Favorable Priors\nMinimax is closely related to Bayes.\nKey observation: average case risk ≤ worst case risk\nFor proper prior \\(\\pi\\), the Bayes risk is:\n\\[r(\\pi) = \\int_\\Theta R(\\theta, \\delta_\\pi) d\\pi(\\theta)\\]\n\\[\\leq \\int_\\Theta \\sup_\\theta R(\\theta, \\delta) d\\pi(\\theta) = \\sup_\\theta R(\\theta, \\delta)\\]\nIf \\(\\delta_\\pi\\) is Bayes, then \\(r(\\pi) = \\inf_\\delta \\int_\\Theta R(\\theta, \\delta) d\\pi(\\theta)\\)\nBayes risk of any Bayes estimator lower bounds \\(r\\).\nLeast favorable prior \\(\\pi\\) gives best lower bound: \\(r(\\pi) = \\sup_\\pi r(\\pi)\\)\nSup risk of any estimator upper bounds \\(r\\):\n\\[\\sup_\\theta R(\\theta, \\delta) \\geq r \\geq \\sup_\\pi r(\\pi)\\]\nCan exhibit minimax est. & LF prior by finding \\(\\pi\\) and \\(\\delta\\) that collapse these inequalities.\n\n2.1 Theorem\nIf \\(R(\\delta_\\pi) = \\sup_\\theta R(\\theta, \\delta_\\pi)\\) with Bayes estimator \\(\\delta_\\pi\\), then:\n\n\\(\\delta_\\pi\\) is minimax\nIf \\(\\delta_\\pi\\) is unique Bayes (up to \\(\\pi\\)-a.e.), it is unique minimax\n\\(\\pi\\) is least favorable\n\nProof:\n\nAny other \\(\\delta\\): \\[\\sup_\\theta R(\\theta, \\delta) \\geq \\int R(\\theta, \\delta) d\\pi(\\theta) \\geq\\] \\[\\int R(\\theta, \\delta_\\pi) d\\pi(\\theta) = r(\\pi) = \\sup_\\theta R(\\theta, \\delta_\\pi)\\] \\(r\\) is minimax risk, \\(\\delta_\\pi\\) is minimax\nReplace \\(\\geq\\) with \\(=\\) in 2nd inequality ⟹ \\(\\delta = \\delta_\\pi\\) \\(\\pi\\)-a.e.\nAny other prior \\(\\pi'\\): \\[\\inf_\\delta r(\\pi') \\leq \\int R(\\theta, \\delta_\\pi) d\\pi'(\\theta)\\] \\[\\leq \\sup_\\theta R(\\theta, \\delta_\\pi) = r(\\pi)\\]\n\nThe above theorem gives a checkable condition: does avg risk = sup risk?\nNote: If \\(R(\\theta, \\delta_\\pi)\\) is constant, it doesn’t prove anything.\n\n\\(R(\\theta, \\delta_\\pi)\\) is constant\nAlso \\(R(\\theta, \\delta_\\pi) = \\sup_\\theta R(\\theta, \\delta_\\pi) = r(\\pi)\\)\n\n\n\n2.2 Example: Binomial\n\\(X \\sim \\text{Binom}(n, \\theta)\\), estimate \\(\\theta\\) with squared error\nTry Beta(\\(\\alpha, \\beta\\)), hope to get one with constant risk\n\\[\\delta_\\pi(X) = \\frac{X + \\alpha}{n + \\alpha + \\beta}\\]\n\\[R(\\theta, \\delta_\\pi) = \\mathbb{E}[\\theta^2] - \\mathbb{E}[\\delta_\\pi(X)^2] + \\text{Var}(\\delta_\\pi(X))\\]\n\\[= \\theta - \\frac{(\\alpha + \\beta + n + 1)(\\alpha + n\\theta)^2}{(\\alpha + \\beta + n)^2(n + \\alpha + \\beta + 1)} + \\frac{(\\alpha + n\\theta)(\\beta + n(1-\\theta))}{(\\alpha + \\beta + n)^2(n + \\alpha + \\beta + 1)}\\]\nSet \\(\\alpha + \\beta = n + 2\\), \\(\\alpha + \\beta = \\frac{n}{2}\\)\n\\[\\beta = \\frac{n+2}{2}, \\alpha = \\frac{n+2}{2}\\]\nBeta(\\(\\frac{n+2}{2}, \\frac{n+2}{2}\\)) is LF, \\(\\delta_\\pi\\) is minimax\nWe got lucky.\nQuestion: Why so much prior weight on \\(\\theta \\approx \\frac{1}{2}\\)?"
  },
  {
    "objectID": "reader/minimax-estimation.html#least-favorable-sequence",
    "href": "reader/minimax-estimation.html#least-favorable-sequence",
    "title": "Minimax Estimation",
    "section": "3 Least Favorable Sequence",
    "text": "3 Least Favorable Sequence\nSometimes there is no least favorable prior, e.g., if parameter space isn’t compact.\n\\(X \\sim N(\\theta, 1)\\): LF prior should spread mass everywhere, but that is not a proper prior.\nDefinition: A sequence \\(\\{\\pi_n\\}\\) is LF if \\(r(\\pi_n) \\to \\sup_\\pi r(\\pi)\\)\n\n3.1 Theorem\nSuppose \\(\\{\\pi_n\\}\\) is a prior sequence and \\(\\delta\\) satisfies \\(\\sup_\\theta R(\\theta, \\delta) = \\lim_n r(\\pi_n)\\). Then:\n\n\\(\\delta\\) is minimax\n\\(\\{\\pi_n\\}\\) is LF\n\nProof:\n\nOther est. \\(\\delta'\\). Then \\(\\forall n\\): \\[\\sup_\\theta R(\\theta, \\delta') \\geq \\int R(\\theta, \\delta') d\\pi_n(\\theta) \\geq r(\\pi_n)\\] \\[\\geq \\lim_n r(\\pi_n) = \\sup_\\theta R(\\theta, \\delta)\\]\nPrior \\(\\pi\\): \\[r(\\pi) = \\inf_\\delta \\int R(\\theta, \\delta) d\\pi(\\theta) \\leq \\int R(\\theta, \\delta) d\\pi(\\theta)\\] \\[\\leq \\sup_\\theta R(\\theta, \\delta) = \\lim_n r(\\pi_n)\\]\n\n\n\n3.2 Basic Picture\n\n\\(\\sup_\\theta R(\\theta, \\delta)\\) (generic \\(\\delta\\))\n\\(\\inf_\\delta \\sup_\\theta R(\\theta, \\delta)\\) (minimax risk)\n\\(\\sup_\\pi r(\\pi)\\) (if LF prior exists)\n\\(r(\\pi)\\) (generic \\(\\pi\\))\n\nIf minimax est. exists: \\(\\inf_\\delta \\sup_\\theta R(\\theta, \\delta) = \\sup_\\theta R(\\theta, \\delta^*)\\)\nIf LF prior exists: \\(\\sup_\\pi r(\\pi) = r(\\pi^*)\\)"
  },
  {
    "objectID": "reader/minimax-estimation.html#practical-applications",
    "href": "reader/minimax-estimation.html#practical-applications",
    "title": "Minimax Estimation",
    "section": "4 Practical Applications",
    "text": "4 Practical Applications\nMinimax estimators are very hard to find, but minimax bounds are often used in statistical theory to characterize hardness, especially lower bounds.\n\n4.1 Approach 1: Near-optimal Estimators\n\nPropose practical estimator \\(\\delta\\)\nFind \\(\\pi\\) for which \\(r(\\pi)\\) close to \\(\\sup_\\theta R(\\theta, \\delta)\\) (or same rate, or asymptotically)\nConclude \\(\\delta\\) can’t be improved much\n\n\n\n4.2 Approach 2: Problem Hardness\nQuantify hardness of a problem by its minimax rate in some asymptotic regime.\nCaveat: A problem might be easy throughout most of parameter space but very hard in some bizarre corner we never encounter in practice."
  },
  {
    "objectID": "reader/hypothesis-testing.html",
    "href": "reader/hypothesis-testing.html",
    "title": "Hypothesis Testing and the Neyman-Pearson Lemma",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/hypothesis-testing.html#hypothesis-testing",
    "href": "reader/hypothesis-testing.html#hypothesis-testing",
    "title": "Hypothesis Testing and the Neyman-Pearson Lemma",
    "section": "1 Hypothesis Testing",
    "text": "1 Hypothesis Testing\nAssume we have a larger model \\(\\cP = \\{P_\\theta: \\theta \\in \\Theta\\}\\) (which, as usual, may be “nonparametric” if \\(\\theta\\) is an infinite-dimensional object like a density), and two competing hypotheses about where \\(\\theta\\) lies:\n\nNull hypothesis: \\(H_0: \\theta \\in \\Theta_0\\)\nAlternative hypothesis: \\(H_1: \\theta \\in \\Theta_1\\)\n\nThese hypotheses should be disjoint, meaning \\(\\Theta_0 \\cap \\Theta_1 = \\emptyset\\), and exhaustive, meaning \\(\\Theta_0 \\cup \\Theta_1 = \\Theta\\). Sometimes \\(\\Theta_0\\) is specified and \\(\\Theta_1\\) is left unspecified; in that case you can assume \\(\\Theta_1 = \\Theta \\setminus \\Theta_0\\).\nA few examples to have in mind:\nExample 1 (Gaussian summary statistic, a.k.a. \\(Z\\)-test): We observe \\(Z \\sim N(\\theta,1)\\) (which is commonly a summary statistic \\(Z(X)\\) from a large data set) and we want to draw an inference about \\(\\theta\\). Two common hypothesis testing settings are the one-sided hypotheses \\(H_0:\\; \\theta \\leq \\theta_0\\) vs \\(H_1:\\; \\theta &gt; \\theta_0\\) and the two-sided hypotheses \\(H_0:\\; \\theta = \\theta_0\\) vs \\(H_1:\\; \\theta \\neq \\theta_0\\).\nExample 2 (Two-sample nonparametric testing): We observe two samples, \\(X_1,\\ldots,X_n \\simiid P\\) and \\(Y_1,\\ldots,Y_m \\simiid Q\\), independently of each other. Without making any further assumptions about \\(P\\) and \\(Q\\), we want to test the nonparametric hypotheses \\(H_0:\\; P = Q\\) vs \\(H_1:\\; P \\neq Q\\).\nA hypothesis is called simple if it fully specifies the data distribution, and composite otherwise. In the examples above, the point null hypothesis \\(H_0:\\; \\theta = \\theta_0\\) is a simple hypothesis, and the other five are composite.\nWe’d like to use the data \\(X\\sim P_\\theta\\) to determine which of \\(H_0\\) or \\(H_1\\) is true, but if a statistician has been called in then this is usually not possible through pure deductive reasoning. For example, if the distributions \\(P_\\theta\\) all have the same support, then any data set \\(X\\) we see is logically consistent with any value of \\(\\theta\\) in the parameter space.\nAs usual, we have two options to get around this problem: we can beg the question (the Bayesian approach) or change the subject (the frequentist approach). The Bayesian answer to this problem is clean and simple: just calculate the posterior probabilities \\(\\Lambda(\\Theta_0 \\mid X) = \\PP(\\theta \\in \\Theta_0 \\mid X)\\) and \\(\\Lambda(\\Theta_1 \\mid X) = \\PP(\\theta \\in \\Theta_1 \\mid X)\\).\nBut there are a variety of settings where this is regarded as unappealing: scientists, drug companies, and others often work very hard to design carefully controlled experiments where the only stochastic assumptions made are ones that very few people would disagree with, and we’d like to be able to analyze the data from those experiments without having to layer on any further assumptions.\nThe frequentist approach to this conundrum is to replace inductive reasoning with inductive behavior: we will come up with a decision rule to decide between the two hypotheses based on the data. Formally, we can say we will either\n\nReject \\(H_0\\) (conclude that \\(H_0\\) is implausible and \\(H_1\\) must be true), or\nAccept \\(H_0\\) (go on believing \\(H_0\\)).\n\nThere is a basic asymmetry here in that \\(H_0\\) is privileged as the default choice to be disconfirmed, or else corroborated. An analogy is often drawn to a criminal trial where the defendant is innocent until proven guilty.\nIn reality, of course, our credence in the null (or alternative) hypothesis should be continuous in the evidence that we observe; it would be ridiculous to flip from 100% belief in the null to 100% belief in the alternative just at the point where a normal random variable crosses some threshold. But there are real-world situations in which a dichotomous decision must be made. For example, should the FDA approve a drug, or not? Or, do we need to control for some variable in our experimental setup, or not? Still, it is helpful to retain some critical distance from the conceit that we are ever really dichotomously “rejecting” or “accepting” either hypothesis in an epistemic sense.\nIn many settings where hypothesis testing is applied, including the examples of two-sided \\(Z\\)-testing and two-sample nonparametric testing above, there will always be points in the alternative hypothesis that explain the data even better than the null hypothesis does. As a result, even if we accept the conceit of making a dichotomous decision about what to “conclude,” it is implausible that we would ever “accept” \\(H_0\\) in the sense of regarding \\(H_1\\) as disconfirmed, even in an approximate sense. As a result, it is usually preferable to say that we “fail to reject \\(H_0\\)” rather than saying we accept it. Though we will continue to use “accept” as a technical term in what follows, “fail to reject” has less risk of inadvertently misleading non-statisticians."
  },
  {
    "objectID": "reader/hypothesis-testing.html#the-critical-function",
    "href": "reader/hypothesis-testing.html#the-critical-function",
    "title": "Hypothesis Testing and the Neyman-Pearson Lemma",
    "section": "2 The critical function",
    "text": "2 The critical function\nWe can describe a test by its critical function (a.k.a. test function):\n\\[\n\\phi(x) = \\begin{cases}\n0 & \\text{accept } H_0 \\\\\n\\gamma \\in (0,1) & \\text{reject w.p. } \\gamma \\\\\n1 & \\text{reject } H_0\n\\end{cases}\n\\]\nThe option of randomizing our test by taking \\(\\phi(x) \\in (0,1)\\) for some values \\(x \\in \\cX\\) is helpful in theory, as we will see shortly, but it is hardly ever done in practice. A non-randomized test \\(\\phi\\) partitions \\(\\cX\\) into the rejection region \\(R = \\{x \\in \\cX: \\phi(x) = 1\\}\\) and the acceptance region \\(A = \\{x \\in \\cX:\\; \\phi(x) = 0\\}\\).\nMost tests are defined by choosing a real-valued test statistic \\(T(X)\\) and rejecting when \\(T(X)\\) is above some critical threshold \\(c \\in \\RR\\). We say \\(\\phi\\) rejects for large \\(T(X)\\) if \\[\n\\phi(x) = \\begin{cases}\n0 & T(x) &lt; c\\\\\n\\gamma \\in (0,1) & T(x) = c \\text{ (if } \\phi \\text{ is randomized)} \\\\\n1 & T(x) &gt; c\n\\end{cases}\n\\] Much of the art in designing a hypothesis test is in choosing a test statistic \\(T(X)\\) that is as effective as possible at discriminating between \\(H_0\\) and \\(H_1\\)\n\n2.1 Significance level and power\nIn carrying out a test, there are two types of errors that we can make: a Type I error (sometimes called a false positive) is when \\(H_0\\) is true, but we reject it, and a Type II error (sometimes called a false negative) is when \\(H_0\\) is false but we fail to reject it. One way to remember which is which is that the Type I error rate is of primary importance in deciding when to reject, and the Type II error rate is of secondary importance. Our usual goal, informally, is to make the probability of a Type II error under \\(H_1\\) as small as we can, while controlling the Type I error rate below a prespecified value \\(\\alpha \\in [0,1]\\). Note that if \\(H_0\\) and \\(H_1\\) are composite, we cannot necessarily speak of “the” Type I or Type II error rate, as it may depend on exactly which of the null or alternative parameter values we sample under.\nThe behavior of the test is fully summarized by the power function \\(\\beta(\\theta) = \\mathbb{E}_\\theta[\\phi(X)] = \\PP_\\theta(\\text{Reject } H_0)\\). In terms of this power function, our goal can be formally stated as \\[\n\\maxz_\\phi \\beta_\\phi(\\theta) \\text{ for } \\theta \\in \\Theta_1 \\quad \\text{ subject to } \\beta_\\phi(\\theta) \\leq \\alpha \\text{ for } \\theta \\in \\Theta_0.\n\\] We say \\(\\phi\\) is a level-\\(\\alpha\\) test if \\(\\sup_{\\theta\\in\\Theta_0} \\beta_\\phi(\\theta) \\leq \\alpha\\). If this supremum is strictly below \\(\\alpha\\), we say the test is conservative. A very common choice for \\(\\alpha\\) is \\(0.05\\); this began with a somewhat offhand remark by Ronald Fisher in his work when he introduced hypothesis testing, that he sometimes liked to use \\(0.05\\) in his scientific work. It has become “the most influential offhand remark in the history of science,” according to Brad Efron at Stanford.\nIf \\(H_0\\) is composite, this optimization problem has multiple constraints, and if \\(H_1\\) is composite it has multiple objectives. A major question for the remainder of this lecture is whether we can find a test \\(\\phi^*\\) that optimizes all objectives at once.\n\n\n2.2 Example: the \\(Z\\)-test\nA very common setting is that we observe some statistic \\(Z(X) \\sim N(\\theta, 1)\\), very often a summary statistic from a larger data set. If we are testing the one-sided hypothesis we might use the right-tailed test \\(\\phi_1(z) = 1\\{z &gt; z_\\alpha\\}\\) that rejects for large values of \\(Z\\). Here \\(z_\\alpha = \\Phi^{-1}(1-\\alpha)\\) is the upper \\(\\alpha\\) quantile of the \\(N(0,1)\\) distribution, and \\(\\Phi(z)\\) is the standard normal cdf.\nIf we want to test the two-sided hypothesis we might use the two-tailed test \\(\\phi_2(z) = 1\\{|z| &gt; z_{\\alpha/2}\\}\\). Now we are rejecting for large values of the test statistic \\(|Z|\\). The rejection regions for these tests at level \\(\\alpha = 0.1\\) are plotted below, along with the alternative distribution when \\(\\theta = 2.3\\). The shaded blue region shows the power of the test under the alternative.\n\n\n\n\n\nThe two tests’ power functions are plotted below for \\(\\alpha = 0.1\\).\n\n\n\n\n\nThe power functions for both tests intersect the vertical axis \\(\\theta=0\\) at \\(\\alpha\\), but the right-tailed test’s power function remains below \\(\\alpha\\) for all \\(\\theta &lt; 0\\) as well. Note that the right-tailed test is actually a valid test for the two-sided hypothesis, but we would be unlikely to want to use it since it has even less than \\(\\alpha\\) power to reject for negative values of \\(\\theta\\). But this may give us a hint that it will not be possible to maximize power throughout the alternative, because the two-tailed test is in fact losing out to the right-tailed test when \\(\\theta &gt; 0\\).\nFor the one-sided hypothesis testing problem, however, we might hold out hope that the right-tailed test is the best for all values in the alternative (all \\(\\theta &gt; 0\\)), and indeed it is."
  },
  {
    "objectID": "reader/hypothesis-testing.html#optimal-testing",
    "href": "reader/hypothesis-testing.html#optimal-testing",
    "title": "Hypothesis Testing and the Neyman-Pearson Lemma",
    "section": "3 Optimal testing",
    "text": "3 Optimal testing\n\n3.1 Likelihood Ratio Test\n\\[\n\\newcommand{\\LR}{\\textnormal{LR}}\n\\]\nTo begin thinking through optimality of a test, we will start with the simplest sort of hypothesis testing problem: a test of a simple null \\(H_0:\\; X\\sim P_0\\) against a simple alternative \\(H_1:\\; X \\sim P_1\\). Without loss of generality, assume that \\(P_0\\) and \\(P_1\\) have densities \\(p_0\\) and \\(p_1\\) with respect to a common dominating measure \\(\\mu\\) (such a measure always exists since we could take \\(\\mu = P_0 + P_1\\)).\nThe optimal level-\\(\\alpha\\) test in this setting is the one that rejects for large values of the likelihood ratio statistic \\[\\LR(X) = \\frac{p_1(X)}{p_0(X)}.\\] That is, our test will be of the form \\[\n\\phi(x) = \\begin{cases}\n1 & \\text{if } \\LR(x) &gt; c \\\\\n\\gamma & \\text{if } \\LR(x) = c \\\\\n0 & \\text{if } \\LR(x) &lt; c\n\\end{cases},\n\\] where \\(c &gt; 0\\) and \\(\\gamma \\in (0,1)\\) are chosen to make \\(\\EE_0 \\phi(X) = \\alpha\\) exactly.\nTo gain intuiion for why this is the right test, recall that the power of a test with rejection region \\(R\\) is \\(\\int_R p_1(x) \\,d\\mu(x)\\), while the Type I error budget is \\(\\int_R p_0(x)\\,d\\mu(x)\\). If the sample space were discrete, both integrals would just be sums over \\(x \\in R\\), and we should construct a rejection region \\(R\\) by collecting sample points for which we can buy us the most power \\(p_1(x)\\) per unit of error budget \\(p_0(x)\\) that we must spend. In short, \\(p_1(x)\\) is the “bang” we get out of including \\(x\\) in the rejection region, and \\(p_0(x)\\) is the “buck” we must spend. The likelihood ratio statistic \\(\\LR(x)\\) is the ratio representing the “bang for our buck.”\nTheorem (Neyman–Pearson lemma): The likelihood ratio test \\(\\phi^*\\) with \\(\\EE_0 \\phi(X) = \\alpha\\) maximizes power among all level-\\(\\alpha\\) tests of \\(H_0:\\; X \\sim P_0\\) vs \\(H_1:\\; X \\sim P_1\\).\nFor purposes of defining likelihood ratio tests, we use the convention that \\(\\LR(x) = \\infty\\) if \\(p_1(x) &gt; p_0(x) = 0\\) (including such points in the rejection region buys us additional power for free), and \\(\\LR(x)\\) is undefined if \\(p_0(x)=p_1(x)=0\\) (since such points never come up under the null or the alternative).\nProof: We are attempting to show \\(\\phi^*\\) solves the maximization problem \\[\n\\maxz \\int \\phi(x)p_1(x)\\,d\\mu(x) \\quad \\text{s.t. } \\int \\phi(x)p_0(x)\\,d\\mu(x) \\leq \\alpha.\n\\]\nThe Lagrange form is:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\phi; \\lambda)\n&= \\int\\phi(x) p_1(x)\\,d\\mu(x)  - \\lambda\\int \\phi(x) p_0(x) \\,d\\mu(x) \\\\\n&= \\int \\phi(x) \\left(p_1(x) - \\lambda p_0(x)\\right)\\,d\\mu(x)\\\\\n&= \\int \\phi(x) \\left(\\frac{p_1(x)}{p_0(x)} - \\lambda\\right)\\,dP_0(x)\\\\\n\\end{aligned}\n\\] If we want to maximize this expression over all functions \\(\\phi:\\; \\cX \\to [0,1]\\), it is clear we should take \\(\\phi(x)\\) as large as possible when \\(\\LR(x)-\\lambda &gt; 0\\) (since the integrand is positive), and as small as possible when \\(\\LR(x) - \\lambda &lt; 0\\) (since the integrand is negative). The best values are attained at the boundary points \\(\\{0,1\\}\\). Note it does not actually matter in this formulation what \\(\\phi(x)\\) is when \\(\\LR(x)=c\\), so any test with \\(\\phi(x) = 1\\) when \\(\\LR(x) &gt; \\lambda\\) and \\(\\phi(x) = 0\\) when \\(\\LR(x) &lt; \\lambda\\) solves this Lagrange form. As a result we see that our proposed test \\(\\phi^*\\) maximizes the Lagrangian for \\(c = \\lambda\\).\nNext, consider any other test \\(\\phi(x)\\) with \\(\\EE_0\\phi(X) \\leq \\alpha\\). We have \\[\n\\begin{aligned}\n\\EE_1\\phi(X) &\\leq \\EE_1\\phi(X) - c(\\EE_0\\phi(X) - \\alpha)\\\\\n&\\leq \\EE_1\\phi^*(X) - c(\\EE_0 \\phi^*(X) - \\alpha)\\\\\n&= \\EE_1\\phi^*(x),\n\\end{aligned}\n\\] where the first inequality comes from the assumption \\(\\EE_0\\phi(X) \\leq \\alpha\\), the second comes from the fact that \\(\\phi^*\\) maximizes the Lagrangian at \\(\\lambda = c\\), and the last from our assumption \\(\\EE_0\\phi(X) = \\alpha\\).\nTo ensure the condition \\(\\EE_0\\phi(X) = \\alpha\\), we use threshold \\(c_\\alpha\\), the upper-\\(\\alpha\\) quantile of the distribution of \\(\\LR(X)\\). Where the randomization parameter \\(\\gamma\\) comes in is that the likelihood ratio statistic may be discrete, in which case we could have \\[\n\\PP_0 (\\LR(X) &gt; c_\\alpha) &lt; \\alpha \\leq \\PP_0(\\LR(X) \\geq c_\\alpha.\n\\] In that case, we can “top off” our error budget at \\(\\alpha\\) by setting \\[\n\\gamma = \\frac{\\alpha - \\PP_0(\\LR(X) &gt; c)}{\\PP_0(\\LR(X) = c)}.\n\\]\n\n\n3.2 Example: Binomial\nTo give a sense of how this works, suppose that we observe a binomial random variable measuring the same-side bias of some human coin flipper \\[\nX \\sim \\text{Binom}(n,\\theta) = \\binom{n}{x}\\theta^x(1-\\theta)^{n-x}, \\text{ for } x=0,1,\\ldots,n.\n\\] Inspired by the Diaconis, Holmes, and Montgomery paper, we want to test \\(H_0:\\; \\theta = 0.5\\) vs \\(H_1:\\;\\theta = 0.51\\). The Neyman–Pearson lemma tells us to use the likelihood ratio: \\[\n\\LR(X) = \\frac{p_{0.51}(X)}{p_{0.5}(X)} = \\frac{0.51^X 0.49^{n-X}}{0.5^n} = \\left(\\frac{0.49}{0.5}\\right)^n\\left(\\frac{0.51}{0.49}\\right)^X\n\\] We know that the most powerful test \\(\\phi^*(X)\\) rejects for large \\(\\LR(X)\\), but because \\(\\LR(X)\\) is a strictly increasing function of \\(X\\), this is completely equivalent to rejecting for large \\(X\\). Thus, the optimal test will reject when \\(X\\) is larger than its upper-\\(\\alpha\\) quantile \\(c_\\alpha\\) under sampling from the simple null \\(P_{0.5}\\).\nIf we choose some value like \\(\\alpha = 0.05\\) that is not divisible by \\(2^{-n}\\), there is no way we can hope to get a Type I error rate of exactly \\(\\alpha\\) under the null, because \\(p_{0.5}(x) = \\binom{n}{x}2^{-n}\\). To fill our Type I error rate budget, we would then need to randomize at the rejection boundary.\nFor example, if \\(n = 100\\) and \\(\\alpha = 0.05\\), we will take \\(c_\\alpha = 58\\), which is the \\(0.95\\) quantile under the null. But \\(\\PP_{0.5}(X &gt; 58) = 0.044\\) instead of \\(0.05\\), which is the Type I error rate we asked for. As a result, we randomize at the boundary by setting \\[\n\\gamma = \\frac{0.05-0.44}{\\PP_{0.5}(X = 58)} = 0.26,\n\\] so \\(\\phi^*\\) rejects with probability \\(0.26\\) if \\(X = 58\\). The plot below shows the acceptance and rejection regions of the test on a probability mass function plot of the null distribution, with the bar at \\(c_\\alpha = 58\\) split according to the acceptance and rejection probabilities at the boundary.\n\n\n\n\n\nIn practice, we hardly ever use randomized tests. The conservative test \\(\\phi^*(X) = 1\\{X &gt; 58\\}\\) is also a likelihood ratio test, and indeed it is the most powerful test at its own level \\(0.044\\).\nWhat if instead of the alternative \\(H_1:\\; \\theta = 0.51\\), we instead read the more recent empirical paper and decided \\(H_1:\\; \\theta = 0.508\\) was better? Then, following the same steps as before, the likelihood ratio statistic would be \\[\n\\LR(X) = \\left(\\frac{0.492}{0.5}\\right)^n \\left(\\frac{0.508}{0.492}\\right)^X.\n\\] Just as before, the likelihood ratio is increasing in \\(X\\), so the test rejects for large values of \\(X\\). And, just as before, we will set our threshold \\((c,\\gamma)\\) to control the Type I error at \\(\\alpha\\) under \\(H_0:\\; \\theta = 0.5\\). Notice that since we have the same test statistic and the same null, we are using exactly the same test. This would be true for any alternative value \\(\\theta_1 &gt; 0.5\\) (but if we used an alternative \\(\\theta_1 &lt; 0.5\\) we would reject for small \\(X\\)). So we actually have in \\(\\phi^*\\) a test of the point null \\(H_0:\\theta = 0.5\\) against the composite alternative \\(H_1:\\; \\theta &gt; 0.5\\), which is simultaneously optimal over all alternative values. A test that achieves maximal power for all values \\(\\theta \\in\\Theta_1\\) is called uniformly most powerful.\nNext we will turn this into a general condition for the same test to be optimal across the entire alternative.\n\n\n3.3 Uniformly most powerful tests\nDefinition: We say a test \\(\\phi^*\\) is a uniformly most powerful (UMP) level-\\(\\alpha\\) test of \\(H_0\\) against \\(H_1\\) if it is a valid level \\(\\alpha\\) test, and for any other valid test \\(\\phi\\), we have \\(\\beta_{\\phi^*}(\\theta) \\geq \\beta_\\phi(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\).\nIn the binomial example above, the reason we found a UMP test is that the likelihood ratio statistic was increasing in \\(X\\) no matter what alternative we chose, so the optimal thing to do was always to reject for large \\(X\\). We now generalize this condition:\nDefinition: We say \\(\\cP = \\{P_\\theta:\\; \\theta \\in \\Theta \\subseteq \\RR\\}\\) has monotone likelihood ratios (MLR) in the statistic \\(T(X)\\) if \\(p_{\\theta_2}(x)/p_{\\theta_1}(x)\\) is a non-decreasing function of \\(T(x)\\), for any pair of parameter values \\(\\theta_1 &lt; \\theta_2\\).\nIn a family with monotone likelihood ratios, one-tailed tests are UMP for one-sided testing problems:\nTheorem: Assume that \\(\\cP\\) has MLR in \\(T(X)\\), and consider testing \\(H_0:\\;\\theta \\leq \\theta_0\\) against \\(H_1:\\; \\theta &gt; \\theta_0\\), for some \\(\\theta_0 \\in \\Theta \\subseteq \\RR\\). If \\(\\phi^*(X)\\) rejects for large \\(T(X)\\), then \\(\\phi^*\\) is UMP at level \\(\\alpha = \\EE_{\\theta_0}\\phi^*(X)\\).\nProof: Consider any other level-\\(\\alpha\\) test \\(\\phi\\), and any \\(\\theta_1 &gt; \\theta_0\\). We know that \\(\\phi\\) is also a valid level-\\(\\alpha\\) test of the simple null \\(H_0:\\; \\theta=\\theta_0\\) vs the simple alternative \\(H_1:\\; \\theta=\\theta_1\\), and so is \\(\\phi^*(X)\\) by assumption. Because \\(p_{\\theta_1}(X)/p_{\\theta_0}(X)\\) is a non-decreasing function of \\(T(X)\\), \\(\\phi^*(X)\\) is a likelihood ratio test for this problem, so we have \\(\\beta_{\\phi^*}(\\theta_1) \\geq \\beta_{\\phi}(\\theta_1)\\).\nNote further that \\(\\beta_{\\phi^*}(\\theta_1) \\geq \\alpha\\) for \\(\\theta_1&gt;\\theta_0\\), which we can see by comparing it to the test \\(\\phi(X) \\equiv \\alpha\\) that ignores the data.\nIt remains only to show that \\(\\phi^*\\) is a valid level-\\(\\alpha\\) test; we only assumed it controls Type I error at the boundary. Define \\(\\bar{\\phi}(X) = 1-\\phi^*(X)\\), which rejects for small \\(T(X)\\) (or equivalently for large values of \\(-T(X)\\)). Note further that \\(\\bar{\\phi}(X)\\) is a level-\\((1-\\alpha)\\) LRT of \\(H_0:\\;\\theta = \\theta_0\\) vs \\(H_1:\\; \\theta = \\theta_1\\), for any \\(\\theta_1 &lt; \\theta_0\\). As a result, we can conclude that for \\(\\theta_1&lt;\\theta_0\\), we have \\[1-\\alpha \\leq \\beta_{\\bar{\\phi}}(\\theta_1) = 1-\\beta_{\\phi^*}(\\theta_1),\\] completing the proof.\nAn important group of examples of MLR families is all one-parameter exponential families.\nExample (One-parameter exponential family): Consider a size-\\(n\\) sample from the exponential family \\[\nX_1,\\ldots,X_n \\simiid p_\\eta(x) = e^{\\eta T(x) - A(\\eta)}h(x)\n\\] Then, for any \\(\\eta_1 &lt;\\eta_2\\), the likelihood ratio for the full sample is \\[\n\\frac{\\prod_i p_{\\eta_2}(x_i)}{\\prod_i p_{\\eta_1}(x_i)} = \\exp\\left\\{(\\eta_2-\\eta_1)\\sum_i T(x_i) - n(A(\\eta_2) - A(\\eta_1))\\right\\},\n\\] which is increasing in \\(\\sum_i T(x_i)\\) if \\(\\eta_2 &gt; \\eta_1\\) (and otherwise it is decreasing in the same statistic). As a result, any likelihood ratio test will reject for large values of \\(\\sum_i T(X_i)\\)."
  },
  {
    "objectID": "reader/hierarchical-bayes.html",
    "href": "reader/hierarchical-bayes.html",
    "title": "Hierarchical Bayes, Markov Chain Monte Carlo, and Empirical Bayes",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/hierarchical-bayes.html#hierarchical-bayes",
    "href": "reader/hierarchical-bayes.html#hierarchical-bayes",
    "title": "Hierarchical Bayes, Markov Chain Monte Carlo, and Empirical Bayes",
    "section": "1 Hierarchical Bayes",
    "text": "1 Hierarchical Bayes\nThe full power of Bayes is realized in large, complex problems with repeat structure, allowing us to pool information across many observations.\n\n1.1 Example: Predicting Batting Averages\nPredict a batter’s true batting average from \\(n_i\\) at-bats, \\(X_i\\) hits: \\(X_i|\\theta_i \\sim \\text{Binom}(n_i, \\theta_i)\\)\nPool info across players \\(i=1,\\ldots,m\\) via hierarchical model:\n\\[\n\\begin{aligned}\n\\alpha, \\beta &\\sim \\pi_0(\\alpha, \\beta) \\quad \\text{(hyperprior)} \\\\\n\\theta_i|\\alpha, \\beta &\\sim \\text{Beta}(\\alpha, \\beta), \\quad i=1,\\ldots,m \\\\\nX_i|\\theta_i, n_i &\\sim \\text{Binom}(n_i, \\theta_i), \\quad i=1,\\ldots,m\n\\end{aligned}\n\\]\n\\[\n\\mathbb{E}[\\theta_i|X] = \\mathbb{E}[\\mathbb{E}[\\theta_i|X, \\alpha, \\beta]|X] = \\mathbb{E}\\left[\\frac{\\alpha + X_i}{\\alpha + \\beta + n_i}|X\\right]\n\\]\nUse all \\(X_1,\\ldots,X_m\\) to learn a good prior on \\(\\theta_i\\).\nNote: There is always an equivalent model where we marginalize over \\(\\alpha, \\beta\\) and just write a more complicated prior on \\(\\theta\\). The hierarchical version may give better intuition or computational strategies.\n\n\n1.2 Gaussian Hierarchical Model\n\\[\n\\begin{aligned}\n\\theta_i &\\sim N(\\mu, \\tau^2), \\quad i=1,\\ldots,d \\\\\nX_i|\\theta_i &\\sim N(\\theta_i, \\sigma^2), \\quad i=1,\\ldots,d\n\\end{aligned}\n\\]\nPosterior mean:\n\\[\n\\mathbb{E}[\\theta_i|X] = \\mathbb{E}[\\mathbb{E}[\\theta_i|X, \\mu, \\tau^2]|X] = \\mathbb{E}\\left[\\frac{\\tau^2}{\\tau^2 + \\sigma^2}X_i + \\frac{\\sigma^2}{\\tau^2 + \\sigma^2}\\mu|X\\right]\n\\]\nLinear shrinkage estimator: Bayes optimal shrinkage estimated from data.\nLikelihood for \\(\\mu, \\tau^2\\) (marginalizing over \\(\\theta_i\\)):\n\\[\n\\begin{aligned}\nX_i|\\mu, \\tau^2 &\\sim N(\\mu, \\tau^2 + \\sigma^2) \\\\\n\\bar{X} &\\sim N(\\mu, \\frac{\\tau^2 + \\sigma^2}{n}) \\\\\nS^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 &\\sim \\frac{\\tau^2 + \\sigma^2}{n-1}\\chi^2_{n-1}\n\\end{aligned}\n\\]\nDefine \\(B = \\tau^2 + \\sigma^2\\) (amount of shrinkage):\n\\[\n\\delta(x) = \\mathbb{E}[\\mathbb{E}[\\theta_i|X, B]|X] = \\mathbb{E}\\left[\\frac{B - \\sigma^2}{B}X_i + \\frac{\\sigma^2}{B}\\bar{X}|X\\right]\n\\]\nEstimated from entire data set:\n\\[\n\\begin{aligned}\n\\bar{X} &\\sim N(\\mu, \\frac{B}{n}) \\\\\n(n-1)S^2 &\\sim B\\chi^2_{n-1}\n\\end{aligned}\n\\]\nConjugate prior (scale mixture):\n\\[\n\\pi(B|\\lambda, \\nu) \\propto B^{-\\nu/2-2}\\exp(-\\frac{\\lambda}{2B})\n\\]\n\\[\nB|X \\sim \\text{InvGamma}\\left(\\frac{n+\\nu}{2}, \\frac{\\lambda + (n-1)S^2}{2}\\right)\n\\]\n\\[\n\\mathbb{E}\\left[\\frac{1}{B}|X\\right] = \\frac{n+\\nu}{\\lambda + (n-1)S^2}\n\\]\n\\[\n\\delta_i(x) = \\frac{(n-3)S^2}{(n-1)S^2 + \\lambda}X_i + \\frac{\\lambda + 2S^2}{(n-1)S^2 + \\lambda}\\bar{X}\n\\]\nPseudo-data: \\(\\nu, \\lambda\\) with \\(\\nu \\approx 2, \\lambda \\approx \\nu\\sigma^2\\)\nMight want to truncate prior to \\([\\sigma^2, \\infty)\\) if \\(\\lambda\\) small.\n\n\n1.3 Graphical Form\nFor hyperparameters \\(\\alpha, \\beta\\):\n     α, β\n    /  |  \\\n   θ₁  θ₂  θ₃\n   |   |   |\n   X₁  X₂  X₃\nThese are the distributions associated with a factor for each vertex in a DAG \\((V,E)\\):\n\\[\np(z) = \\prod_{i=1}^{|V|} p(z_i|z_{\\text{pa}(i)})\n\\]\nFor this model:\n\\[\np(\\alpha, \\beta, \\theta_1,\\ldots,\\theta_m, X_1,\\ldots,X_m) = p(\\alpha, \\beta)\\prod_{i=1}^m p(\\theta_i|\\alpha, \\beta)p(X_i|\\theta_i)\n\\]"
  },
  {
    "objectID": "reader/hierarchical-bayes.html#markov-chain-monte-carlo-mcmc",
    "href": "reader/hierarchical-bayes.html#markov-chain-monte-carlo-mcmc",
    "title": "Hierarchical Bayes, Markov Chain Monte Carlo, and Empirical Bayes",
    "section": "2 Markov Chain Monte Carlo (MCMC)",
    "text": "2 Markov Chain Monte Carlo (MCMC)\nHierarchical models are very flexible, but often create big computational headaches:\n\\[\n\\pi(\\theta|x) = \\frac{p(x|\\theta)\\pi(\\theta)}{\\int p(x|\\theta)\\pi(\\theta)d\\theta}\n\\]\nNumerator is usually nice, denominator often intractable.\nComputational strategy: Set up a Markov chain with stationary distribution \\(\\pi(\\theta|x)\\), run it to get approximate samples from \\(\\pi(\\theta|x)\\).\n\n2.1 Definition of Markov Chain\nA stationary Markov chain with transition kernel \\(Q(y|x)\\) and initial distribution \\(\\pi_0(x)\\) is a sequence of r.v.’s \\(X_0, X_1, \\ldots\\) where \\(X_0 \\sim \\pi_0\\) and:\n\\[\nP(X_{t+1} \\in A | X_t, \\ldots, X_0) = P(X_{t+1} \\in A | X_t) = \\int_A Q(y|X_t) dy\n\\]\nMarginal distribution of \\(X_t\\):\n\\[\n\\pi_t(y) = P(X_t \\in A) = \\int Q(y|x)\\pi_{t-1}(x) dx\n\\]\nThis is a directed graphical model: \\(X_0 \\to X_1 \\to X_2 \\to \\cdots\\)\nIf \\(\\pi(y) = \\int Q(y|x)\\pi(x) dx\\), we say \\(\\pi\\) is a stationary distribution for \\(Q\\).\nA sufficient condition is detailed balance:\n\\[\n\\pi(x)Q(y|x) = \\pi(y)Q(x|y)\n\\]\n\\[\n\\int_y Q(y|x)\\pi(x) dx = \\int_y \\pi(y)Q(x|y) dy = \\pi(y)\n\\]\nA Markov chain with detailed balance is called reversible: \\(X_{t-1} | X_t \\sim X_{t+1} | X_t\\) if \\(\\pi_t = \\pi\\).\n\n\n2.2 Theory\nIf a Markov chain with stationary distribution \\(\\pi\\) is:\n\nIrreducible: \\(\\forall x,y, \\exists n: P(X_n = y | X_0 = x) &gt; 0\\)\nAperiodic: \\(\\forall x, \\gcd\\{n &gt; 0: P(X_n = x | X_0 = x) &gt; 0\\} = 1\\)\n\nThen \\(\\pi_t \\to \\pi\\) in TV distance, regardless of \\(\\pi_0\\) (chain “forgets” \\(\\pi_0\\)).\nStrategy: Find \\(Q\\) with stationary distribution \\(\\pi(\\theta|x)\\), start at any \\(X_0\\), run chain for a long time, then \\(X_t\\) is approximately a sample from posterior for large \\(t\\)."
  },
  {
    "objectID": "reader/hierarchical-bayes.html#gibbs-sampler",
    "href": "reader/hierarchical-bayes.html#gibbs-sampler",
    "title": "Hierarchical Bayes, Markov Chain Monte Carlo, and Empirical Bayes",
    "section": "3 Gibbs Sampler",
    "text": "3 Gibbs Sampler\nFor parameter vector \\(\\theta = (\\theta_1, \\ldots, \\theta_d)\\):\nAlgorithm: 1. Initialize \\(\\theta^{(0)}\\) 2. For \\(t = 1, \\ldots, T\\): For \\(j = 1, \\ldots, d\\): Sample \\(\\theta_j^{(t)} \\sim p(\\theta_j | \\theta_{-j}^{(t-1)}, X)\\) Record \\(\\theta^{(t)}\\)\nVariations: - Update one random coordinate \\(J \\sim \\text{Unif}(1,\\ldots,d)\\) - Update coordinates in random order\nAdvantage for hierarchical priors: only need to sample low-dimensional conditional distributions:\n\\[\np(\\theta_i | \\theta_{-i}, X, \\alpha) \\propto p(\\theta_i | \\theta_{\\text{pa}(i)}) \\prod_{j \\in \\text{ch}(i)} p(\\theta_j | \\theta_i)\n\\]\nEspecially easy if using conjugate priors at all levels; often can be parallelized.\n\n3.1 Gibbs Stationarity\nClaim: If \\(\\pi_t = \\pi(\\theta|X)\\), then \\(\\pi_{t+1} = \\pi(\\theta|X)\\).\nProof (sketch): Consider updating only one fixed coordinate \\(j\\): \\(\\theta_j^{(t+1)} \\sim p(\\theta_j | \\theta_{-j}^{(t)}, X)\\) If \\(\\theta^{(t)} \\sim \\pi(\\theta|X)\\), then \\(\\theta^{(t+1)} \\sim \\pi(\\theta|X)\\) Updating any coordinate preserves posterior distribution Updating coordinates in any order also does\nIn theory: Pick initialization and valid kernel \\(Q\\), sample long enough, get \\(\\theta^{(t)} \\sim \\pi(\\theta|X)\\) Do it again \\(N\\) more times, get \\(N\\) samples from \\(\\pi(\\theta|X)\\)\nIn practice: How do we know we’ve sampled long enough? - Plot \\(\\theta_i^{(t)}\\) vs \\(t\\), show how fast the MC mixes - Look for \\(\\hat{R} = \\frac{\\text{between-chain variance}}{\\text{within-chain variance}} \\approx 1\\)\nThese methods are GOOD, NOT GREAT. Can be deceived, especially for bimodal posteriors.\nBetter approach: Burn-in then convergence. Estimate posterior based on \\(\\theta^{(B+1)}, \\ldots, \\theta^{(B+N)}\\)\nFor function \\(f(\\theta)\\): \\(\\hat{\\mu}_f = \\frac{1}{N} \\sum_{t=B+1}^{B+N} f(\\theta^{(t)})\\)\n\n\n3.2 Implementation Details Matter\nExample: \\[\n\\begin{aligned}\n\\theta &\\sim N(0, 100) \\\\\nX_i | \\theta &\\sim N(\\theta, 0.1^2), \\quad i=1,\\ldots,n\n\\end{aligned}\n\\]\nPosterior: \\(\\theta | X \\sim N\\left(\\frac{\\bar{X}/0.01^2}{n/0.01^2 + 1/100}, \\frac{1}{n/0.01^2 + 1/100}\\right)\\)\nGibbs sampler: 1. \\(\\mathbb{E}[\\theta|X] = 8.9, \\text{SD}(\\theta|X) = 0.1\\) 2. \\(\\theta^{(t+1)} | X, \\theta^{(t)} \\sim N(8.9, 0.1^2)\\)\nGibbs takes a long time to mix.\nBetter parameterization: \\[\n\\begin{aligned}\nB &= \\theta - \\bar{X} \\\\\n\\theta &= B + \\bar{X}\n\\end{aligned}\n\\]\n\\(B|X \\sim N(0, 0.1^2)\\)\nGibbs: Directly sampling from posterior"
  },
  {
    "objectID": "reader/hierarchical-bayes.html#empirical-bayes",
    "href": "reader/hierarchical-bayes.html#empirical-bayes",
    "title": "Hierarchical Bayes, Markov Chain Monte Carlo, and Empirical Bayes",
    "section": "4 Empirical Bayes",
    "text": "4 Empirical Bayes\nBack to Gaussian hierarchical model:\n\\[\n\\begin{aligned}\n\\theta_i &\\sim N(\\mu, \\tau^2) \\\\\nX_i | \\theta_i &\\sim N(\\theta_i, \\sigma^2)\n\\end{aligned}\n\\]\n\\[\n\\mathbb{E}[\\theta_i | X, B] = \\frac{B - \\sigma^2}{B}X_i + \\frac{\\sigma^2}{B}\\bar{X}, \\quad B = \\tau^2 + \\sigma^2\n\\]\nFor any reasonable prior: \\(B | X \\approx \\frac{1}{N}\\sum_{i=1}^N (X_i - \\bar{X})^2\\)\nIf prior doesn’t matter much, why use one? Could just estimate \\(B\\) from data, however we want.\nA minimax estimator is \\(B = \\sigma^2 + \\frac{1}{N}\\sum_{i=1}^N (X_i - \\bar{X})^2\\)\nCalled Empirical Bayes: a hybrid approach in which hyperparameters are treated as fixed, others treated as random."
  },
  {
    "objectID": "reader/asymptotics.html",
    "href": "reader/asymptotics.html",
    "title": "Asymptotic Theory",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/asymptotics.html#introduction-to-asymptotic-theory",
    "href": "reader/asymptotics.html#introduction-to-asymptotic-theory",
    "title": "Asymptotic Theory",
    "section": "1 Introduction to Asymptotic Theory",
    "text": "1 Introduction to Asymptotic Theory\nSo far, everything we have seen in this class has dealt with the exact distributions of finite data sets. In many cases we have needed to exploit special properties of the model \\(\\cP\\) (e.g., that it has a complete sufficient statistic, it is an exponential family, it is a multivariate Gaussian distribution, etc.) to make exact calculations easier. In generic models, however, the tools we have learned so far may fail us.\nFor generic models, exact calculations may be intractable or impossible. However, we may be able to approximate our problem with a simpler problem in which calculations are easy. Typically, we approximate by Gaussian by taking the limit as observations \\(n \\to \\infty\\). This is only interesting if the approximation is good for reasonable sample sizes, but fortunately it very often is.\n\n1.1 Example: Logistic regression\nTo take a specific example, suppose that we modify the linear regression example from the last lecture to allow for a binary rather than a continuous response. We retain the feature vector \\(x_i \\in \\RR^d\\) and regression coefficients \\(\\beta \\in \\RR^d\\), but instead of observing outcome \\(y_i \\simind N(\\beta'x_i, \\sigma^2)\\), we now observe \\(y_i \\simind \\text{Bern}(\\mu(\\beta'x_i))\\), where\n\\[\n\\mu(\\eta) = \\frac{e^{\\eta}}{1+e^{\\eta}}.\n\\] Define the mean response \\(\\mu_i = \\mu(\\beta'x_i)\\) and linear predictor \\(\\eta_i = \\log \\frac{\\mu_i}{1-\\mu_i} = \\beta'x_i\\).\nUnlike the linear regression model, we cannot simply rotate the response vector into a canonical basis without destroying the structure of our model; the entries of the rotated response vector would be neither Bernoulli nor independent.\nWe might still hope to make progress by observing that we have an exponential family model, since \\[\n\\begin{aligned}\np_\\beta(y \\mid X)  &= \\prod_{i=1}^n \\mu_i^{y_i}(1-\\mu_i)^{1-y_i}\\\\\n&= \\prod_{i=1}^n \\exp\\left\\{\\beta'x_i y_i + \\log(1-\\mu(\\beta'x_i))\\right\\}\\\\\n&= \\exp\\left\\{\\beta'X'y - A(\\beta;X)\\right\\},\n\\end{aligned}\n\\] a \\(d\\)-parameter exponential family on \\(\\mathcal{Y} = \\{0,1\\}^n\\) with natural parameter \\(\\beta\\), complete sufficient statistic \\(X'y\\), and normalizing constant \\(A(\\beta; X) = - \\sum_i \\log (1+e^{\\beta'x_i})\\). Note \\(A\\) depends only on \\(\\beta\\) and the design matrix \\(X\\), which we typically treat as fixed (possibly after conditioning on it, if it is “really” random).\nIf we want to estimate \\(\\beta\\), we might hope for a UMVU estimator. Unfortunately, no unbiased estimator could possibly exist: any estimator \\(\\hat\\beta_j\\) would have some largest value that it can take on the finite sample space \\(\\mathcal{Y}\\), and its expectation can never be larger than that value, even though \\(\\beta_j\\) can range over the entire real line. We could also come up with a Bayes estimator, but we would have to specify a joint distribution over the parameter vector \\(\\beta \\in \\RR^d\\), which we might not wish to do.\nLikewise, we might want to test a hypothesis like \\(H_0:\\;\\beta_j = 0\\) vs \\(H_1:\\;\\beta_j \\neq 0\\). Again, we might think we are in good shape due to the exponential family form: all we have to do is condition on \\(X_{-j}'y\\) and then reject for conditionally large values of \\(X_j'y\\). Unfortunately, however, this is also typically a nonstarter: if one of the features \\(X_k\\) takes continuous values, then the entire vector \\(y\\) can be recovered from observing only \\(X_k'y = \\sum_{i:y_i=1} x_{i,k}\\) if all \\(2^n\\) such partial sums are distinct. In that case, the only unbiased test is the trivial constant test that ignores the response vector \\(y\\) and rejects with probability \\(\\alpha\\).\nFortunately, if \\(n\\) is “large” then we can apply general-purpose asymptotic methods to come up with very good estimators, tests, or confidence intervals in this problem. Define the maximum likelihood estimator \\[\n\\begin{aligned}\n\\hat\\beta_{\\text{MLE}} &= \\argmax_{\\beta \\in \\RR^d} p_\\beta(y \\mid X)\\\\\n&= \\argmax_{\\beta \\in \\RR^d} \\beta'X'y - A(\\beta; X),\n\\end{aligned}\n\\] which can be maximized efficiently because the log-likelihood is concave in \\(\\beta\\).\nWe’ll show in a future lecture that, under relatively mild conditions, we will have \\[\n\\hat\\beta_{\\text{MLE}} \\approx N_d(\\beta, J(\\beta)^{-1}),\n\\] for the Fisher information matrix \\[\nJ(\\beta) = \\Var_{\\beta}(\\nabla\\ell(\\beta; y, X))\n= -\\EE_\\beta \\nabla^2\\ell(\\beta; y, X)\n\\] That is, if \\(n\\) is large then \\(\\hat\\beta_{\\text{MLE}}\\) approximately follows a Gaussian distribution that is not only approximately unbiased, but its variance-covariance matrix \\(\\Sigma(\\beta) = J(\\beta)^{-1}\\) matches the Cramèr–Rao lower bound.\nWhat is more, the observed value of minus the Hessian matrix gives a good approximation to its expectation, the Fisher information matrix at the true \\(\\beta\\): \\[\n\\widehat{\\Sigma}(y,X) = (-\\nabla^2\\ell(\\hat\\beta_{\\text{MLE}}; y, X))^{-1} \\approx \\Sigma(\\beta) = J(\\beta)^{-1}\n\\] For a single coefficient \\(\\beta_j\\) we can say \\(\\hat\\beta_j - \\beta_j \\approx N(0, \\sigma^2_{j}(\\beta))\\), for \\(\\sigma^2_j(\\beta) = \\Sigma_{jj}(\\beta))\\), or \\[\nZ_j = \\frac{\\hat\\beta_j - \\beta_j}{\\hat\\sigma_j} \\approx N(0,1), \\quad \\text{ for } \\hat\\sigma^2_j = \\widehat\\Sigma_{jj},\n\\] leading to natural tests for hypotheses like \\(H_0:\\;\\beta_j = 0\\) or the confidence interval \\(\\hat\\beta_j \\pm z_{\\alpha/2} \\sqrt{\\hat\\Sigma_{jj}}\\) for \\(\\beta_j\\).\nThese results do not necessarily require \\(n\\) to be huge; for example, we can simulate the distribution of \\(\\hat\\beta_1\\) in the logistic regression model \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\) with an intercept and a single (uniformly distributed) covariate. For \\(\\beta_0 = -2\\), \\(\\beta_1 = 4\\), and \\(n = 100\\), the normal approximation appears reasonably good, though it depends on the parameters.\n\nset.seed(12345)\nn &lt;- 100\nbeta &lt;- c(-2,4)\n\nB &lt;- 1E4\nbeta.hat &lt;- matrix(NA, nrow=B, ncol=2)\nsigma.hat &lt;- matrix(NA, nrow=B, ncol=2)\nfor (b in 1:B) {\n  x &lt;- runif(n)\n  eta &lt;- beta[1] + x * beta[2]\n  mu &lt;- exp(eta) / (1+exp(eta))\n  y &lt;- 1*(runif(n) &lt; mu)\n  mod &lt;- glm(y ~ x, family=binomial)\n  ## Estimate is MLE\n  beta.hat[b,] &lt;- coef(mod)\n  ## Std. error estimate is sqrt(-diag(Hessian) at MLE)\n  sigma.hat[b,] &lt;- coef(summary(mod))[,\"Std. Error\"]\n}\n## \"True\" Fisher information\nsigma &lt;- sqrt(colMeans(sigma.hat^2))\nhist(beta.hat[,2], freq=FALSE, breaks=50, main = expression(paste(\"Simulated distribution of \", hat(beta)[1])), xlab=expression(hat(beta)[1]))\ncurve(dnorm(x, mean = beta[2], sd = sigma[2]), add=TRUE)\n\n\n\nhist((beta.hat[,2] - beta[2])/sigma.hat[,2], freq=FALSE, breaks=30, main = expression(paste(\"Simulated distribution of \", Z[1] == (hat(beta)[1]-beta[1])/hat(sigma)[1])), xlab=expression(Z[1]))\ncurve(dnorm(x), add=TRUE)\n\n\n\n\nThese results about the MLE apply far beyond logistic regression, to a wide variety of other models. To begin to prove them, however, we will need to be more precise about what we mean by \\(\\hat\\beta\\) approximately following a normal distribution, or by the estimator \\(\\hat\\Sigma\\) being approximately equal to the estimand \\(\\Sigma(\\beta)\\)."
  },
  {
    "objectID": "reader/asymptotics.html#convergence",
    "href": "reader/asymptotics.html#convergence",
    "title": "Asymptotic Theory",
    "section": "2 Convergence",
    "text": "2 Convergence\nThere are two main approximations we will be interested in making: approximating a random variable as a limiting constant, and approximating it as having some limiting distribution, usually a multivariate Gaussian distribution. To make these approximations precise, we need to introduce two types of convergence: convergence in probability, and convergence in distribution.\nLet \\(X_1, X_2, \\ldots\\) be a sequence of random variables on a sample space \\(\\cX\\), which we assume is endowed with a distance \\(d(x,y)\\). Because we are primarily interested in \\(\\cX \\subseteq \\RR^d\\), we will write \\(d(x,y)\\) as \\(\\|x-y\\|\\), and it will not matter which norm we choose, but\nWe say the sequence converges in probability to a limiting constant \\(c \\in \\cX\\), written as \\(X \\toProb c\\) if \\[\n\\mathbb{P}(\\|X_n - c\\| &gt; \\epsilon) \\to 0 \\quad \\forall \\epsilon &gt; 0.\n\\] In writing the norm we are implicitly assuming $\n\n2.1 Convergence in Probability\nWe say the sequence converges in probability to \\(c \\in \\mathbb{R}^d\\) (\\(X_n \\xrightarrow{p} c\\)) if:\n\\[\\mathbb{P}(\\|X_n - c\\| &gt; \\epsilon) \\to 0 \\quad \\forall \\epsilon &gt; 0\\]\n(Could really be any distance on any \\(\\cX\\))\nCan converge to a r.v. \\(X\\) too, but we don’t need this.\n\n\n2.2 Convergence in Distribution\nWe say the sequence converges in distribution to random variable \\(X\\) (\\(X_n \\xrightarrow{d} X\\)) if:\n\\[\\mathbb{E}[f(X_n)] \\to \\mathbb{E}[f(X)] \\text{ for all bounded continuous } f: \\cX \\to \\mathbb{R}\\]\nTheorem: \\(X_n, X \\in \\mathbb{R}\\). Fix \\(\\mathbb{P}(X = x) = 0\\). Let \\(F_n(x) = \\mathbb{P}(X_n \\leq x)\\), \\(F(x) = \\mathbb{P}(X \\leq x)\\). Then \\(X_n \\xrightarrow{d} X\\) iff \\(F_n(x) \\to F(x)\\) \\(\\forall x: F\\) is continuous at \\(x\\).\nAlso known as weak convergence.\n\n\n2.3 Example\nIf \\(X_n \\xrightarrow{d} X \\sim g\\), then \\(X_n \\xrightarrow{d} X\\):\n\\[F_n(x) = \\begin{cases}\n1 & \\text{if } x &gt; 0 \\\\\n1 - \\frac{1}{n} & \\text{if } x = 0 \\\\\n0 & \\text{if } x &lt; 0\n\\end{cases}\\]\n\\[F(x) = \\begin{cases}\n1 & \\text{if } x &gt; 0 \\\\\n0 & \\text{if } x \\leq 0\n\\end{cases}\\]\n\n\n2.4 Proof: \\(X_n \\xrightarrow{p} c \\implies X_n \\xrightarrow{d} c\\)\nLet \\(f_\\epsilon(x) = \\max\\{1 - \\frac{\\|x-c\\|}{\\epsilon}, 0\\}\\). Then \\(\\forall \\epsilon &gt; 0\\):\n\\[\\mathbb{P}(\\|X_n - c\\| &gt; \\epsilon) \\leq \\mathbb{E}[1 - f_\\epsilon(X_n)] \\to 0\\]\n\\(f\\) bounded continuous. Note \\(\\mathbb{E}[f(c)] = f(c)\\).\n\\(\\forall \\epsilon &gt; 0\\), \\(\\exists \\delta &gt; 0\\) s.t. \\(\\|x - c\\| &lt; \\delta \\implies |f(x) - f(c)| &lt; \\epsilon\\)\n\\[|\\mathbb{E}[f(X_n)] - f(c)| \\leq |\\mathbb{E}[f(X_n) - f(c)]1_{\\|X_n - c\\| &lt; \\delta}| + |\\mathbb{E}[(f(X_n) - f(c))1_{\\|X_n - c\\| \\geq \\delta}]|\\] \\[\\leq \\epsilon + 2\\sup |f| \\cdot \\mathbb{P}(\\|X_n - c\\| \\geq \\delta)\\]\nFor sufficiently large \\(n\\). \\(\\square\\)\nIn a sequence of statistical models \\(\\cP_n = \\{P_{n,\\theta}: \\theta \\in \\Theta\\}\\) with \\(X_n \\sim P_{n,\\theta}\\), we say \\(\\hat{\\theta}_n\\) is consistent for \\(g(\\theta)\\) if \\(\\hat{\\theta}_n \\xrightarrow{p} g(\\theta)\\), meaning:\n\\[\\mathbb{P}_\\theta(|\\hat{\\theta}_n - g(\\theta)| &gt; \\epsilon) \\to 0\\]\nUsually, we omit the index \\(n\\); sequence is implicit.\n\n\n2.5 Law of Large Numbers (LLN)\nLet \\(\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\)\nIf \\(\\mathbb{E}|X_i| &lt; \\infty\\), \\(\\mathbb{E}X_i = \\mu\\), then \\(\\bar{X}_n \\xrightarrow{p} \\mu\\)\n\n\n2.6 Central Limit Theorem (CLT)\nIf \\(\\text{Var}(X_i) = \\sigma^2 &lt; \\infty\\), then \\(\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)\\)\nThere are stronger versions of both the LLN and CLT, but this will generally be enough for us."
  },
  {
    "objectID": "reader/asymptotics.html#continuous-mapping-theorem",
    "href": "reader/asymptotics.html#continuous-mapping-theorem",
    "title": "Asymptotic Theory",
    "section": "3 Continuous Mapping Theorem",
    "text": "3 Continuous Mapping Theorem\nTheorem (Continuous Mapping): Let \\(g\\) be continuous, \\(X_n, X\\) r.v.’s.\n\nIf \\(X_n \\xrightarrow{d} X\\), then \\(g(X_n) \\xrightarrow{d} g(X)\\)\nIf \\(X_n \\xrightarrow{p} c\\), then \\(g(X_n) \\xrightarrow{p} g(c)\\)\n\nProof: \\(f\\) bounded continuous \\(\\implies f \\circ g\\) bounded continuous If \\(X_n \\xrightarrow{d} X\\), then \\(\\mathbb{E}[f(g(X_n))] \\to \\mathbb{E}[f(g(X))]\\) \\(X_n \\xrightarrow{p} c\\) special case with \\(X \\equiv c\\)"
  },
  {
    "objectID": "reader/asymptotics.html#slutskys-theorem",
    "href": "reader/asymptotics.html#slutskys-theorem",
    "title": "Asymptotic Theory",
    "section": "4 Slutsky’s Theorem",
    "text": "4 Slutsky’s Theorem\nTheorem (Slutsky): Assume \\(X_n \\xrightarrow{d} X\\), \\(Y_n \\xrightarrow{p} c\\). Then:\n\n\\(X_n + Y_n \\xrightarrow{d} X + c\\)\n\\(X_n Y_n \\xrightarrow{d} cX\\)\n\\(X_n / Y_n \\xrightarrow{d} X/c\\) if \\(c \\neq 0\\)\n\nProof: Show \\((X_n, Y_n) \\xrightarrow{d} (X, c)\\), apply continuous mapping.\nWouldn’t normally be true that \\(X_n \\xrightarrow{d} X\\), \\(Y_n \\xrightarrow{d} Y\\) implies \\(X_n + Y_n \\xrightarrow{d} X + Y\\) without specifying joint dist."
  },
  {
    "objectID": "reader/asymptotics.html#delta-method",
    "href": "reader/asymptotics.html#delta-method",
    "title": "Asymptotic Theory",
    "section": "5 Delta Method",
    "text": "5 Delta Method\nTheorem (Delta Method): If \\(\\sqrt{n}(X_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)\\) \\(f(x)\\) differentiable at \\(x = \\mu\\)\nThen \\(\\sqrt{n}(f(X_n) - f(\\mu)) \\xrightarrow{d} N(0, [f'(\\mu)]^2 \\sigma^2)\\)\nInstant: \\(X \\sim N(\\mu, \\sigma^2/n) \\implies f(X) \\sim N(f(\\mu), [f'(\\mu)]^2 \\sigma^2/n + o(1/n))\\)\nProof: \\(f(X_n) = f(\\mu) + f'(\\mu)(X_n - \\mu) + o(X_n - \\mu)\\) \\(\\sqrt{n}(f(X_n) - f(\\mu)) = f'(\\mu)\\sqrt{n}(X_n - \\mu) + \\sqrt{n}o(X_n - \\mu)\\) \\(N(0, \\sigma^2) + 0 \\xrightarrow{d} N(0, [f'(\\mu)]^2 \\sigma^2)\\)\nMultivariate: \\(\\sqrt{n}(X_n - \\mu) \\xrightarrow{d} N(0, \\Sigma)\\), \\(f: \\mathbb{R}^d \\to \\mathbb{R}^k\\) Derivative \\(Df(\\mu)\\) exists at \\(\\mu\\)\nThen \\(\\sqrt{n}(f(X_n) - f(\\mu)) \\xrightarrow{d} N(0, Df(\\mu) \\Sigma Df(\\mu)^T)\\)\n\\(N(f(\\mu), Df(\\mu) \\Sigma Df(\\mu)^T/n + o(1/n))\\) if \\(k=1\\)\n\n5.1 Example: Delta Method Application\n\\(X_1, \\ldots, X_n \\sim \\text{Unif}[0, \\theta]\\) iid \\(Y_1, \\ldots, Y_m \\sim \\text{Unif}[0, \\theta]\\) iid \\(X, Y\\) independent\nFor large \\(n, m\\), what is the distribution of \\(T = \\frac{\\bar{X}}{\\bar{Y}}\\)?\n\n\\(\\sqrt{n}(\\bar{X} - \\frac{\\theta}{2}) \\xrightarrow{d} N(0, \\frac{\\theta^2}{12})\\) as \\(n \\to \\infty\\)\n\\(\\sqrt{m}(\\bar{Y} - \\frac{\\theta}{2}) \\xrightarrow{d} N(0, \\frac{\\theta^2}{12})\\) as \\(m \\to \\infty\\)\n\n\\(T_n = \\frac{\\bar{X}}{\\bar{Y}} = \\frac{\\theta/2}{\\theta/2} = 1 + O_p(n^{-1/2} + m^{-1/2})\\)\nLet \\(f(x,y) = x/y\\)\n\\(f_x'(\\frac{\\theta}{2}, \\frac{\\theta}{2}) = \\frac{1}{\\theta/2} = \\frac{2}{\\theta}\\) \\(f_y'(\\frac{\\theta}{2}, \\frac{\\theta}{2}) = -\\frac{\\theta/2}{(\\theta/2)^2} = -\\frac{2}{\\theta}\\)\n\\(f'(\\frac{\\theta}{2}, \\frac{\\theta}{2}) = (\\frac{2}{\\theta}, -\\frac{2}{\\theta})\\)\n\\(\\sqrt{n}(T_n - 1) \\xrightarrow{d} N(0, \\frac{4}{\\theta^2} \\cdot \\frac{\\theta^2}{12} \\cdot \\frac{1}{n} + \\frac{4}{\\theta^2} \\cdot \\frac{\\theta^2}{12} \\cdot \\frac{n}{m})\\)\n\\(= N(0, \\frac{1}{3n} + \\frac{1}{3m})\\)\nMore accurate: \\(\\sqrt{n}(T_n - 1) \\xrightarrow{d} N(0, \\frac{4}{3}(1 + \\frac{n}{m}))\\)\n\n\n5.2 What if \\(\\mu = 0\\)?\n\nWhat if \\(\\mu_1 = \\mu_2 = 0\\)? Conclusion still holds: \\(T_n = \\frac{1 + O_p(n^{-1/2})}{1 + O_p(m^{-1/2})} = 1 + O_p(n^{-1/2} + m^{-1/2})\\)\n\nNote: \\(\\frac{1}{1 + n^{-1/2}} \\to 1\\) (continuous mapping) Not Slutsky\nSo \\(n(T_n - 1)^2 \\xrightarrow{d} \\chi^2_1\\) (continuous mapping) Why not delta method?\nIn general, can do higher-order Taylor expansions for delta method if derivatives \\(\\neq 0\\):\n\\(f(X_n) = f(\\mu) + f'(\\mu)(X_n - \\mu) + \\frac{1}{2}f''(\\mu)(X_n - \\mu)^2 + O_p(n^{-3/2})\\)\nIf \\(f'(\\mu) = 0\\), use second-order term: \\(n(f(X_n) - f(\\mu)) \\xrightarrow{d} \\frac{1}{2}f''(\\mu)\\chi^2_1\\)"
  },
  {
    "objectID": "reader/bayes-estimation.html",
    "href": "reader/bayes-estimation.html",
    "title": "Bayes Estimation",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/bayes-estimation.html#bayes-risk-and-bayes-estimator",
    "href": "reader/bayes-estimation.html#bayes-risk-and-bayes-estimator",
    "title": "Bayes Estimation",
    "section": "1 Bayes Risk and Bayes Estimator",
    "text": "1 Bayes Risk and Bayes Estimator\n\n1.1 Definitions\nThe Bayes risk is the average case risk:\n\\[\nr(\\pi, \\delta) = \\EE_\\pi[R(\\theta, \\delta)] = \\int R(\\theta, \\delta) \\,d\\pi(\\theta)\n\\]\nwhere \\(\\pi(\\theta)\\) is a probability measure (for now, we assume it’s proper; later we will allow it to be improper).\nNote: \\(\\pi\\) and \\(\\delta\\) are functionally equivalent for average risk, which makes sense even if we don’t believe \\(\\pi\\).\n\\[\nr(\\pi, \\delta) = \\EE_\\pi[\\EE_\\theta[L(\\theta, \\delta(X))]] = \\EE[L(\\theta, \\delta(X))]\n\\]\nwhere \\((\\theta, X) \\sim p(\\theta, x) = p(x|\\theta)\\pi(\\theta)\\)\nAn estimator \\(\\delta\\) minimizing \\(r_\\text{Bayes}(\\delta)\\) is called a Bayes estimator. It depends on \\(\\pi\\) and \\(L\\).\n\\[\n\\delta_\\pi = \\argmin_\\delta \\EE[L(\\theta, \\delta(X))]\n\\]\n\n\n1.2 Prior and Posterior\n\nThe usual interpretation of \\(\\pi\\) is the prior belief about \\(\\theta\\) before seeing the data.\nThe conditional distribution \\(\\pi(\\theta|X)\\) is called the posterior distribution (belief after seeing the data).\n\nDensities: - Prior: \\(\\pi(\\theta)\\) - Likelihood: \\(p(x|\\theta)\\) - Joint density: \\(p(\\theta, x) = \\pi(\\theta)p(x|\\theta)\\) - Marginal density: \\(q(x) = \\int p(\\theta, x) \\,d\\theta\\) - Posterior density: \\(\\pi(\\theta|x) = \\frac{p(\\theta, x)}{q(x)}\\)\nThe Bayes estimator depends on the posterior:\n\\[\n\\delta_\\pi(x) = \\argmin_d \\EE[L(\\theta, d)|X=x] = \\argmin_d \\int L(\\theta, d) \\pi(\\theta|x) \\,d\\theta\n\\]\n\n\n1.3 Theorem: Characterization of Bayes Estimators\nSuppose \\(X \\sim p_\\theta(x)\\) and \\(\\delta_\\pi(x) = \\delta(x)\\) for some function \\(\\delta\\). Then \\(\\delta\\) is Bayes with respect to \\(\\pi\\) if and only if \\(\\delta(x) \\in \\argmin_d \\EE[L(\\theta, d)|X=x]\\) for almost every \\(x\\).\nProof: 1. Let \\(\\delta'\\) be any other estimator. 2. \\(r(\\pi, \\delta') = \\int \\EE[L(\\theta, \\delta'(X))|X=x] q(x) \\,dx\\) 3. \\(r(\\pi, \\delta) = \\int \\EE[L(\\theta, \\delta(X))|X=x] q(x) \\,dx\\) 4. Define \\(E_x(d) = \\EE[L(\\theta, d)|X=x]\\) 5. If \\(\\delta(x) \\in \\argmin_d E_x(d)\\), then \\(E_x(\\delta(x)) \\leq E_x(\\delta'(x))\\) for all \\(x\\) 6. This implies \\(r(\\pi, \\delta) \\leq r(\\pi, \\delta')\\)"
  },
  {
    "objectID": "reader/bayes-estimation.html#special-cases-and-examples",
    "href": "reader/bayes-estimation.html#special-cases-and-examples",
    "title": "Bayes Estimation",
    "section": "2 Special Cases and Examples",
    "text": "2 Special Cases and Examples\n\n2.1 Squared Error Loss\nIf \\(L(\\theta, d) = (\\theta - d)^2\\), then the Bayes estimator is the posterior mean:\n\\[\n\\delta_\\pi(x) = \\EE[\\theta|X=x]\n\\]\nProof: \\[\n\\begin{aligned}\n\\EE[(\\theta - d)^2|X=x] &= \\EE[\\theta^2|X=x] - 2d\\EE[\\theta|X=x] + d^2 \\\\\n&= \\Var(\\theta|X=x) + (\\EE[\\theta|X=x] - d)^2 + \\EE[\\theta|X=x]^2 - 2d\\EE[\\theta|X=x] + d^2\n\\end{aligned}\n\\]\nThe minimum occurs when \\(d = \\EE[\\theta|X=x]\\).\n\n\n2.2 Weighted Squared Error\nFor \\(L(\\theta, d) = w(\\theta)(\\theta - d)^2\\) (e.g., squared relative error), the Bayes estimator is:\n\\[\n\\delta_\\pi(x) = \\frac{\\EE[w(\\theta)\\theta|X=x]}{\\EE[w(\\theta)|X=x]}\n\\]"
  },
  {
    "objectID": "reader/bayes-estimation.html#examples",
    "href": "reader/bayes-estimation.html#examples",
    "title": "Bayes Estimation",
    "section": "3 Examples",
    "text": "3 Examples\n\n3.1 Beta-Binomial\n\n\\(X|\\theta \\sim \\text{Binomial}(n, \\theta)\\), \\(\\theta \\in [0,1]\\)\n\\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\), \\(\\alpha, \\beta &gt; 0\\)\n\nThe marginal distribution of \\(X\\) is called Beta-Binomial.\nPosterior: \\[\n\\pi(\\theta|x) \\propto \\theta^x (1-\\theta)^{n-x} \\cdot \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} \\propto \\theta^{x+\\alpha-1}(1-\\theta)^{n-x+\\beta-1}\n\\]\nTherefore, \\(\\theta|X \\sim \\text{Beta}(x+\\alpha, n-x+\\beta)\\)\n\\[\n\\EE[\\theta|X] = \\frac{x+\\alpha}{n+\\alpha+\\beta}\n\\]\nInterpret \\(\\alpha+\\beta\\) as pseudo-trials and \\(\\alpha\\) as pseudo-successes.\n\n\n3.2 Normal Mean\n\n\\(X_i|\\theta \\sim N(\\theta, \\sigma^2)\\), \\(\\sigma^2\\) known\n\\(\\theta \\sim N(\\mu, \\tau^2)\\)\n\nPosterior: \\[\n\\pi(\\theta|x) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\theta)^2\\right) \\exp\\left(-\\frac{1}{2\\tau^2}(\\theta-\\mu)^2\\right)\n\\]\nComplete the square:\n\\[\n\\theta|X \\sim N\\left(\\frac{\\frac{n}{\\sigma^2}\\bar{x} + \\frac{1}{\\tau^2}\\mu}{\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}}, \\frac{1}{\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}}\\right)\n\\]\n\\[\n\\EE[\\theta|X] = \\frac{\\frac{n}{\\sigma^2}\\bar{x} + \\frac{1}{\\tau^2}\\mu}{\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}} = w\\bar{x} + (1-w)\\mu\n\\]\nwhere \\(w = \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}\\)\nIf \\(\\frac{1}{\\tau^2} = k\\), interpret as \\(k\\) pseudo-observations with mean \\(\\mu\\)."
  },
  {
    "objectID": "reader/bayes-estimation.html#conjugate-priors",
    "href": "reader/bayes-estimation.html#conjugate-priors",
    "title": "Bayes Estimation",
    "section": "4 Conjugate Priors",
    "text": "4 Conjugate Priors\nIn both examples, the prior and likelihood have a similar functional form, and the posterior comes from the same exponential family as the prior. When the posterior is from the same family as the prior, we say the prior is conjugate to the likelihood.\n\n4.1 Conjugate Priors for Exponential Families\nSuppose \\(p_\\theta(x) = h(x)\\exp(\\eta(\\theta)'T(x) - A(\\theta))\\) for carrier \\(h\\). Define:\n\\[\n\\pi(\\theta) = g(\\theta)\\exp(\\lambda'u(\\theta) - \\psi(\\lambda))\n\\]\nThen:\n\\[\n\\pi(\\theta|x) \\propto \\exp((\\lambda + T(x))'u(\\theta) - (A(\\theta) + \\psi(\\lambda)))\n\\]\nOften, \\(u(\\theta) = \\eta(\\theta)\\) and \\(\\lambda\\) is interpreted as pseudo-observations.\n\n\n4.2 Conjugate Prior Examples\n\nNormal-Normal:\n\n\\(X_i|\\theta \\sim N(\\theta, \\sigma^2)\\), \\(\\sigma^2\\) known\n\\(\\theta \\sim N(\\mu, \\tau^2)\\)\n\nPoisson-Gamma:\n\n\\(X|\\theta \\sim \\text{Poisson}(\\theta)\\), \\(\\theta &gt; 0\\)\n\\(\\theta \\sim \\text{Gamma}(\\alpha, \\beta)\\), \\(\\alpha, \\beta &gt; 0\\)\n\nPosterior: \\(\\theta|X \\sim \\text{Gamma}(\\alpha + \\sum x_i, \\beta + n)\\)\nInterpret \\(\\alpha\\) as pseudo-counts and \\(\\beta\\) as pseudo-exposure.\n\n\n\n4.3 Beta-binomial example\n\n```{ojs}\n// Import necessary libraries\nimport {Plotly} from '@observablehq/plotly'\nimport {jStat} from 'jstat'\n\n// Input sliders and text boxes\nviewof priorMean = Inputs.range([0.01, 0.99], {value: 0.5, step: 0.01, label: \"Prior Mean\"})\nviewof priorPseudoFlips = Inputs.range([0.1, 100], {value: 20, step: 0.1, label: \"Prior Pseudo-Flips\"})\nviewof n = Inputs.number({value: 500, min: 1, step: 1, label: \"n\"})\nviewof xVal = Inputs.number({value: 125, min: 0, max: n, step: 1, label: \"X\"})\n\n// Compute alpha and beta from prior mean and pseudo-flips\nalpha = priorMean * priorPseudoFlips\nbeta = (1 - priorMean) * priorPseudoFlips\n\n// Create theta values\ntheta = Array.from({length: 1000}, (_, i) =&gt; i / 999)\n\n// Calculate prior, likelihood, and posterior densities\npriorDensity = theta.map(t =&gt; jStat.beta.pdf(t, alpha, beta))\nposteriorDensity = theta.map(t =&gt; jStat.beta.pdf(t, alpha + xVal, beta + n - xVal))\n\n// Compute log-likelihood to prevent underflow issues\nlogLikelihood = theta.map(t =&gt; {\n  if (t &lt;= 0 || t &gt;= 1) return -Infinity\n  return xVal * Math.log(t) + (n - xVal) * Math.log(1 - t)\n})\nmaxLogLikelihood = Math.max(...logLikelihood)\nlikelihoodScaled = logLikelihood.map(l =&gt; Math.exp(l - maxLogLikelihood))\n\n// Scale likelihood to match plot dimensions\nmaxPriorDensity = Math.max(...priorDensity)\nmaxPosteriorDensity = Math.max(...posteriorDensity)\nymax = 1.1 * Math.max(maxPriorDensity, maxPosteriorDensity)\nlikelihoodScaled = likelihoodScaled.map(l =&gt; l * ymax / 1.1)\n\n// Prepare data for plotting\ndata = [\n  {\n    x: theta,\n    y: priorDensity,\n    name: 'Prior',\n    mode: 'lines',\n    line: {color: 'blue'}\n  },\n  {\n    x: theta,\n    y: likelihoodScaled,\n    name: 'Likelihood',\n    mode: 'lines',\n    line: {color: 'black'}\n  },\n  {\n    x: theta,\n    y: posteriorDensity,\n    name: 'Posterior',\n    mode: 'lines',\n    line: {color: 'red'}\n  }\n]\n\n// Compute estimates for vertical lines\nUMVU_estimate = xVal / n\nBayes_estimate = (alpha + xVal) / (alpha + beta + n)\n\n// Define layout with vertical lines and annotations\nlayout = {\n  title: 'Beta-Binomial',\n  xaxis: {title: 'theta', range: [0, 1]},\n  yaxis: {title: 'Density', range: [0, ymax]},\n  shapes: [\n    {\n      type: 'line',\n      x0: UMVU_estimate,\n      x1: UMVU_estimate,\n      y0: 0,\n      y1: ymax,\n      line: {color: 'black', dash: 'dot'}\n    },\n    {\n      type: 'line',\n      x0: Bayes_estimate,\n      x1: Bayes_estimate,\n      y0: 0,\n      y1: ymax,\n      line: {color: 'red', dash: 'dot'}\n    },\n    {\n      type: 'line',\n      x0: 0.5,\n      x1: 0.5,\n      y0: 0,\n      y1: ymax,\n      line: {color: 'gray', dash: 'solid'}\n    }\n  ],\n  annotations: [\n    {\n      x: UMVU_estimate,\n      y: ymax,\n      xref: 'x',\n      yref: 'y',\n      text: 'UMVU',\n      showarrow: true,\n      arrowhead: 2,\n      ax: 0,\n      ay: -40\n    },\n    {\n      x: Bayes_estimate,\n      y: ymax,\n      xref: 'x',\n      yref: 'y',\n      text: 'Bayes',\n      showarrow: true,\n      arrowhead: 2,\n      ax: 0,\n      ay: -40\n    }\n  ],\n  legend: {x: 0.8, y: 0.9},\n  margin: {t: 40}\n}\n\n// Create the plot\nPlotly.newPlot(plotDiv, data, layout)\n\n// Display the plot\nplotDiv = html`&lt;div id=\"plotDiv\"&gt;&lt;/div&gt;`\n```\n\n\n\n\n\n\n\nOJS Syntax Error (line 249, column 8)Unexpected token"
  },
  {
    "objectID": "reader/testing-one-parameter.html",
    "href": "reader/testing-one-parameter.html",
    "title": "Testing with One Real Parameter",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/testing-one-parameter.html#testing-with-one-real-parameter",
    "href": "reader/testing-one-parameter.html#testing-with-one-real-parameter",
    "title": "Testing with One Real Parameter",
    "section": "1 Testing with one real parameter",
    "text": "1 Testing with one real parameter\nThis lecture concerns the general problem of testing with one real parameter. We observe \\(X \\sim P_\\theta\\) for \\(\\theta \\in \\Theta \\subseteq \\RR\\), and we might want to test a one-sided alternative like \\(H_0:\\; \\theta \\leq \\theta_0\\) vs the one-sided alternative \\(H_1:\\; \\theta &gt; \\theta_0\\), or a point null hypothesis like \\(H_0:\\; \\theta = \\theta_0\\) against a two-sided alternative \\(H_1:\\; \\theta \\neq \\theta_0\\). Or, we could test an interval null \\(H_0:\\; |\\theta - \\theta_0| \\leq \\delta\\) vs the two-sided alternative \\(H_1:\\; |\\theta-\\theta_0|&gt;\\delta\\), for \\(\\delta \\geq 0\\) (which reduces to the point null if \\(\\delta = 0\\))."
  },
  {
    "objectID": "reader/testing-one-parameter.html#one-sided-testing",
    "href": "reader/testing-one-parameter.html#one-sided-testing",
    "title": "Testing with One Real Parameter",
    "section": "2 One-sided testing",
    "text": "2 One-sided testing\nLast time, we showed that if the family \\(\\cP\\) has MLR in the statistic \\(T(X)\\), then the one-sided test that rejects for large \\(T(X)\\) is UMP for testing \\(H_0:\\;\\theta\\leq \\theta_0\\) vs \\(H_1:\\;\\theta &gt; \\theta_0\\), because:\n\nit is simultaneously the likelihood ratio test for \\(H_0:\\;\\theta = \\theta_0\\) vs \\(H_1:\\;\\theta = \\theta_1\\), for every \\(\\theta_1 &gt; 0\\), and\nit controls the Type I error for all \\(\\theta &lt; \\theta_0\\).\n\nRecall that a test rejects for large \\(T(X)\\) if it is of the form \\[\\phi(X) = \\begin{cases} 1 &\\quad \\text{ if } T(X) &gt; c \\\\ 0 &\\quad \\text{ if } T(X) &lt; c\\\\ \\gamma &\\quad \\text{ if } T(X) = c\\end{cases},\\] where the critical threshold \\(c\\) is the upper-\\(\\alpha\\) quantile at the boundary \\[\nc_\\alpha = \\min \\{c \\in \\RR:\\; \\PP_{\\theta_0}(T(X) &gt; c) \\leq \\alpha\\}\n\\] and the randomization parameter \\(\\gamma\\) is used to “top off” the Type I error rate if \\(T(X)\\) is discrete and \\(\\PP_{\\theta_0}(T(X) &gt; c_\\alpha) &lt; \\alpha\\). In the rest of this section we will ignore randomization and assume that we just accept a conservative test in case \\(\\PP_{\\theta_0}(T(X) &gt; c_\\alpha) &lt; \\alpha\\) (as is generally done in practice).\nA generic one-parameter model \\(\\cP\\) does not have MLR in any statistic \\(T(X)\\); e.g. the LRT for testing \\(\\theta_0\\) vs \\(\\theta_1=\\theta_0 + 1\\) does not coincide with the LRT for testing \\(\\theta_0\\) vs \\(\\theta_1=\\theta_0 + 2\\). Then we cannot maximize power for both alternative values \\(\\theta_0+1\\) and \\(\\theta_0+2\\) simultaneously.\nIn such cases, we could still come up with a test that rejects for large values of some other test statistic \\(T(X)\\), that tends to be larger when \\(\\theta\\) is larger. Formally, we say that \\(T(X)\\) is stochastically increasing in \\(\\theta\\) if \\(\\PP_\\theta(T(X) &gt; c)\\) is non-decreasing in \\(\\theta\\), for every \\(c \\in \\RR\\). The power function of \\(\\phi(X) = 1\\{T(X) &gt; c_\\alpha\\}\\), then, is also non-decreasing in \\(\\theta\\), and \\(\\phi(X)\\) is a valid test of \\(H_0:\\;\\theta\\leq \\theta_0\\) vs \\(H_1:\\;\\theta &gt; \\theta_0\\).\n\n2.1 Score test\nSuppose we observe \\(X_1,\\ldots,X_n \\simiid P_\\theta\\) for large \\(n\\), and we want to test \\(H_0:\\;\\theta\\leq \\theta_0\\) vs \\(H_1:\\;\\theta &gt; \\theta_0\\), but \\(\\cP\\) does not have MLR so we cannot maximize the power over the entirety of \\(H_1\\). One idea is to use the heuristic of maximizing the power for alternatives near \\(\\theta_0\\); if \\(n\\) is large, then we have a lot of information about \\(\\theta\\) so our power will be close to \\(1\\) no matter what we do. So we might prioritize maximizing the power at \\(\\theta_0 + \\varepsilon\\) for small \\(\\varepsilon\\).\nThe LRT for \\(\\theta_0\\) vs \\(\\theta_0 + \\varepsilon\\) rejects for large values of \\[ \\frac{p_{\\theta_0+\\varepsilon}(X)}{p_{\\theta_0}(X)} =\\exp\\{\\ell(\\theta_0+\\varepsilon; X) - \\ell(\\theta_0; X)\\} \\approx e^{\\varepsilon \\dot\\ell(\\theta_0;X)},\\] which is equivalent to rejecting for large values of \\(\\dot\\ell(\\theta_0;X)\\). Using the score statistic can give simple and appealing tests in certain situations.\nExample: Laplace\nSuppose \\(X_1,\\ldots,X_n \\simiid \\text{Laplace}(\\theta) = \\frac{1}{2}e^{-|x-\\theta|}\\) and we want to test \\(H_0:\\theta \\leq 0\\) vs \\(H_1:\\;\\theta &gt; 0\\). We can calculate the likelihood ratio test for a given fixed alternative \\(\\theta_1 &gt; 0\\) as\n\\[\n\\log \\frac{p_{\\theta_1}(X)}{p_0(X)} = \\sum_i |X_i| - |X_i-\\theta_1| = \\theta_1\\sum_i T_{\\theta_1}(X_i),\n\\] so the optimal test rejects for large \\(\\sum_i T_{\\theta_1}(X_i)\\), where \\[\nT_{\\theta}(x) = \\begin{cases} -1 & \\text{ if } x \\leq 0 \\\\ \\frac{2x}{\\theta} -1 & \\text{ if } 0 \\leq x \\leq \\theta \\\\ +1 &\\text{ if } x \\geq \\theta \\end{cases}.\n\\] We can visualize the univariate version of the test statistic \\(T_{\\theta}(x)\\) for several different values of \\(\\theta&gt;0\\):\n\n\n\n\n\nNote that this test implicitly caps the influence of any single observation \\(X_i\\). Once \\(X_i &gt; \\theta_1\\), it gives the same evidence in favor of \\(\\theta_1\\) and against \\(\\theta_0\\) regardless of how much it exceeds \\(\\theta_1\\). Compare this with the sample mean, where the influence of a single observation \\(X_i\\) is unbounded. It is easy to see that \\(f(X_i)\\) is stochastically increasing in \\(\\theta\\) for any non-decreasing function \\(f\\), so any LRT gives a valid level-\\(\\alpha\\) test on the entire null distribution.\nIf we take \\(\\theta_1\\downarrow 0\\), the univariate test statistic approaches \\[\nT_0(x) = \\begin{cases} -1 & \\text{ if } x \\leq 0\\\\ +1 &\\text{ if } x &gt; 0\\end{cases},\n\\] which gives the score test since \\[\n\\dot{\\ell}(\\theta;X) = \\frac{d}{d\\theta} \\sum_i -|X_i-\\theta| = \\sum_i T_0(X_i)\n\\] This is equivalent to rejecting for large values of \\(S(X) = \\#\\{X_i &gt; 0\\}\\), simply the number of positive \\(X_i\\) values.\nWe can also plot the power curves for \\(n = 100\\) and \\(\\alpha = 0.1\\), for these tests and for the test that rejects for large values of \\(\\sum_i X_i\\). As we see, the score test performs noticeably better than the test that rejects for large values of the sample mean \\(\\overline{X}\\). But the LRT for \\(\\theta_1 = 0.2\\) seems to do best for this value of \\(n\\), since \\(\\theta_1 = 0.2\\) is a moderately hard alternative value for which the power is intermediate.\n\n\n\n\n\n\n\n2.2 The sign test as a nonparametric test\nThe test based on the statistic \\(S(X) = \\#\\{X_i &gt; 0\\}\\) is called the sign test, and is generally an appealing test for a nonparametric testing problem. Suppose \\(X_1,\\ldots,X_n \\simiid F\\), where \\(F\\) represents an unknown cdf for their distribution. Assume for simplicity that \\(F\\) is continuous and strictly increasing on its support, so that the median \\(\\theta(F) = F^{-1}(1/2)\\) is well-defined, and consider testing \\(H_0:\\; \\theta(F) \\leq 0\\) vs \\(H_1:\\; \\theta(F) &gt; 0\\).\nThen \\(S(X) \\sim \\text{Binom}(n, 1-F(0))\\), where the probability parameter \\(1-F(0)\\) is no more than \\(1/2\\) if \\(H_0\\) is true, but strictly greater than \\(1/2\\) if \\(H_1\\) is true. Then the test that rejects when \\(S(X)\\) is above the upper \\(1-\\alpha\\) quantile of the \\(\\text{Binom}(n,1/2)\\) distribution (randomizing at the boundary if desired) is level-\\(\\alpha\\) on \\(H_0\\)."
  },
  {
    "objectID": "reader/testing-one-parameter.html#two-sided-alternatives",
    "href": "reader/testing-one-parameter.html#two-sided-alternatives",
    "title": "Testing with One Real Parameter",
    "section": "3 Two-sided alternatives",
    "text": "3 Two-sided alternatives\nOften we want to test a null hypothesis against the alternative that the parameter is larger or smaller than a null value, or range of values. This section will consider a null hypothesis of the form \\(H_0:\\; |\\theta - \\theta_0| \\leq \\delta\\) against the alternative \\(H_1:\\; |\\theta - \\theta_0| &gt; \\delta\\), for some tolerance \\(\\delta \\geq 0\\). In the important special case \\(\\delta = 0\\) we will call \\(H_0\\) a point null, and if \\(\\delta &gt; 0\\) we will call \\(H_0\\) an interval null.\n\n3.1 Two-tailed tests\nTo test a two-sided alternative, we will generally employ a two-tailed test based on some test statistic \\(T(X)\\). We will say that \\(\\phi(X)\\) rejects for extreme \\(T(X)\\) (i.e., for large or small values of \\(T(X)\\)) if \\[\n\\phi(X) = \\begin{cases} 1 & \\quad \\text{ if } T(X) &lt; c_1 \\text{ or } T(X) &gt; c_2\\\\\n0 & \\quad \\text{ if } c_1 &lt; T(X) &lt; c_2\\\\\n\\gamma_i &\\quad \\text{ if } T(X) = c_i, \\; i = 1,2\n\\end{cases}\n\\]\nWhen we test with a two-sided alternative we will generally not be able to optimize power everywhere. For example, if we test \\(H_0:\\; \\theta = 0\\) vs \\(H_1:\\; \\theta \\neq 0\\) in the \\(z\\)-test problem \\(X \\sim N(\\theta,1)\\), we can choose any test of the form \\[\n\\phi_{\\alpha_1}(x) = 1\\{x &lt; -z_{\\alpha_1}\\} + 1\\{x &gt; z_{\\alpha - \\alpha_1}\\},\n\\] for any \\(\\alpha_1 \\in [0,\\alpha]\\). We obtain the right- and left-tailed tests in the limit where \\(\\alpha\\) is \\(0\\) or \\(\\alpha\\) respectively, and the usual symmetric two-tailed test when \\(\\alpha_1 = \\alpha/2\\).\n\n\n\n\n\nNote that two of the three tests shown above have the undesirable property that the power falls below \\(\\alpha\\) on part of the alternative: that is, there are alternative values of \\(\\theta\\) for which our chance of rejecting the null is even less than it would be if the null were true.\nNone of the tests plotted above is as powerful for \\(\\theta &gt; 0\\) as the right-tailed test (\\(\\alpha_1 = 0\\)), and none is as powerful for \\(\\theta &lt; 0\\) as the left-tailed test (\\(\\alpha_1 = \\alpha\\)), and intuitively it is clear that we cannot hope to find a test that maximizes power on both parts of the alternative.\nAs in the case with estimation, one way that we can proceed when there is no UMP test is to impose a constraint that rules out all but one test. In the \\(z\\)-test above, the test with \\(\\alpha_1 = \\alpha/2\\) is symmetric in two respects:\n\nIt is equal-tailed, meaning that we have dedicated an equal portion of our total Type I error budget to the left and right lobes of the rejection region, and\nIt is unbiased, meaning that the power is at least \\(\\alpha\\) everywhere on the alternative.\n\nThe idea of an equal-tailed test makes sense when \\(H_0\\) is simple, but it is not obvious how it extends to the more common situation where \\(H_0\\) is composite. We will focus on the latter condition, unbiasedness.\n\n\n3.2 Exponential example\nConsider testing \\(H_0:\\;\\theta = 1\\) vs \\(H_1:\\;\\theta \\neq 1\\) in the model where \\(X \\sim \\text{Exp}(\\theta)\\), with cdf \\[F_\\theta(t) = \\PP_\\theta(X \\leq t) = 1-e^{-t/\\theta}.\\] To solve for the equal-tailed test cutoffs we set \\(c_1^{\\text{ET}}=F_1^{-1}(1-\\alpha/2) = -\\log(1-\\alpha/2)\\) and \\(c_2^{\\text{ET}}= F_1^{-1}(\\alpha/2) = -\\log(\\alpha/2)\\). Then the power function of the equal-tailed test \\(\\phi^{\\text{ET}}\\) is\n\\[\n\\begin{aligned}\n\\beta_{\\phi^{\\text{ET}}}(\\theta)\n&= \\PP_\\theta(X &lt; c_1^{\\text{ET}}) + \\PP_\\theta(X &gt; c_2^{\\text{ET}})\\\\\n&= 1 - e^{-c_1^{\\text{ET}}/\\theta} +e^{-c_2^{\\text{ET}}/\\theta}\\\\\n&= 1 - (1-\\alpha/2)^{1/\\theta} + (\\alpha/2)^{1/\\theta}\n\\end{aligned}\n\\] This test does indeed have power equal to \\(\\alpha\\) at \\(\\theta = 1\\), but its power is also \\(\\alpha\\) at \\(\\theta = 1/2\\), and the power is actually below \\(\\alpha\\) on \\((1/2,1)\\). So this is not an unbiased test. If we want an unbiased test, we need to set the derivative of the power equal to \\(0\\) at \\(\\theta = 1\\). We can solve this numerically in terms of the left-lobe rejection probability \\(\\alpha_1\\), taking \\[\n\\begin{aligned}\nc_1(\\alpha_1) &=-\\log(1-\\alpha_1), \\quad\\text{ and }\\\\\nc_2(\\alpha_1) &= -\\log(\\alpha_2)=-\\log(\\alpha-\\alpha_1).\n\\end{aligned}\n\\] If \\(\\alpha = 0.1\\) we obtain \\(\\alpha_1 = 0.080\\), \\(c_1 = 0.083\\), and \\(c_2 = 3.9\\) for the unbiased test, vs \\(c_1 = 0.051\\) and \\(c_2 = 3.0\\) for the equal-tailed test. The unbiased test is not as powerful for \\(\\theta &gt; 1\\), but it is more powerful for \\(\\theta &lt; 1\\), and its power is minimized at \\(\\alpha\\) when \\(\\theta = 1\\). We plot both power curves below.\n\n\n\n\n\n\n\n3.3 Optimal unbiased tests\nIf we are testing a point null against a two-sided alternative, we can take our choice between the equal-tailed and unbiased test, but the unbiasedness criterion is conceptually appealing for more general testing problems because the definition naturally extends to the case where \\(H_0\\) is composite. For example, if we want to test an interval null against a two-sided alternative, it is not clear what it means to set \\(\\PP_{H_0}(T(X) &lt; c_1) = \\alpha/2\\), because that probability varies over the null parameter space \\(\\Theta_0\\). By contrast, the unbiased criterion is well-defined for any hypothesis testing problem.\nIf the power function is differentiable in \\(\\theta\\), and \\(\\theta_0\\) is in \\(\\Theta^{\\circ}\\), the interior of the parameter space, then any unbiased test \\(\\phi\\) must have \\(\\beta_\\phi(\\theta_0) = \\alpha\\) and \\(\\dot{\\beta}_{\\phi}(\\theta_0) = 0\\). Otherwise, the power would be strictly less than \\(\\alpha\\) at either \\(\\theta_0 +\\varepsilon\\) or \\(\\theta_0- \\varepsilon\\), for sufficiently small \\(\\varepsilon&gt;0\\).\nIn exponential family models, we can use these facts to obtain a simple characterization of the criterion that the power function has zero derivative at \\(\\theta_0\\), as. Let \\(X \\sim p_\\theta(x) = e^{\\theta T(x) - A(\\theta)}h(x)\\), and differentiate the power function to obtain \\[\n\\begin{aligned}\n\\dot{\\beta}_{\\phi}(\\theta_0)\n&= \\frac{d}{d\\theta} \\left. \\int \\phi(x)e^{\\theta T(x) - A(\\theta)}h(x)\\,d\\mu(x)\\right|_{\\theta=\\theta_0} \\\\\n&= \\int \\phi(x)(T(x)-\\dot{A}(\\theta_0))e^{\\theta_0 T(x) - A(\\theta_0)}h(x)\\,d\\mu(x)\\\\\n&= \\EE_{\\theta_0}\\left[\\phi(X)(T(X) - \\EE_{\\theta_0}T(X))\\right]\\\\[5pt]\n&= \\text{Cov}_{\\theta_0}(T(X), \\phi(X))\\\\[5pt]\n&= \\EE_{\\theta_0}\\left[(\\phi(X)-\\alpha)T(X)\\right].\n\\end{aligned}\n\\] Setting the last expression to 0 and massaging the equation a bit, we obtain \\[\n\\EE_{\\theta_0}T(X) = \\frac{\\EE_{\\theta_0}[\\phi(X)T(X)]}{\\alpha} = \\EE_{\\theta_0}[T(X) \\mid \\phi(X) \\text{ rejects } H_0].\n\\] Thus, the conditional expectation of \\(T(X)\\) under the null, given that it falls in the rejection region, is the same as the marginal expectation. For instance, both the acceptance region and the rejection region for our unbiased test of \\(H_0:\\;\\theta=1\\) in the exponential model share the same “balance point” at \\(\\EE_{1}X = 1\\). Because the right lobe is farther out from 1, it has only about 1/4 as much probability mass as the right lobe.\n\n\n\n\n\nRecall that when we restricted our attention to unbiased estimators, we were able to find a unique best unbiased estimator. Likewise, we can sometimes find an optimal two-sided test if we restrict our attention to unbiased tests. We say that \\(\\phi^*\\) is UMP unbiased (UMPU) if, for any other unbiased level \\(\\alpha\\) test \\(\\phi\\), we have \\(\\beta_{\\phi^*}(\\theta) \\geq \\beta_{\\phi}(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\). UMPU tests exist, at least, for one-parameter exponential family models, as we show below.\nTheorem (UMP Unbiased tests): Assume we want to test \\(H_0:\\;|\\theta -\\theta_0| \\leq \\delta\\) vs \\(H_1:\\;|\\theta - \\theta_0| &gt; \\delta\\) in the model \\(X \\sim e^{\\theta T(x)-A(\\theta)}h(x)\\), for \\(\\delta \\geq 0\\) and \\(\\theta_0-\\delta, \\theta+\\delta \\in \\Theta^\\circ\\), the interior of the parameter space. Suppose that the test \\(\\phi^*(X)\\) rejects for extreme values of \\(T(X)\\), with the cutoffs \\(c_1,c_2,\\gamma_1,\\gamma_2\\) chosen so that\n\n\\(\\phi^*\\) attains power \\(\\alpha\\) at the boundary of the null, i.e. \\(\\beta_{\\phi^*}(\\theta_0 - \\delta) = \\beta_{\\phi^*}(\\theta_0 + \\delta) = \\alpha\\), and\nif \\(\\delta&gt; 0\\), the power function is flat at \\(\\theta_0\\), i.e. \\(\\dot{\\beta}_{\\phi^*}(\\theta_0) = 0\\).\n\nThen \\(\\phi^*\\) is UMPU.\nProof: Assume without loss of generality that \\(\\theta_0 = 0\\), and first consider the case \\(\\delta = 0\\). Our proof will proceed much as it did for the Neyman-Pearson lemma. For \\(\\theta \\neq 0\\), we want to solve the problem \\[\n\\begin{aligned}\n\\maxz_\\phi &\\int \\phi(x)p_{\\theta}(x)\\,d\\mu(x)\\\\\n\\text{ subject to } &\\int \\phi(x)p_0(x)\\,d\\mu(x) = \\alpha, \\quad\\text{ and }\\\\\n&\\int \\phi(x)(T(x)-\\nu_0)p_0(x)\\,d\\mu(x) = 0,\n\\end{aligned}\n\\] where \\(\\nu_0 = \\EE_0 T(X)\\). Note that we have an equality constraint for the Type I error, because any unbiased test must have power exactly equal to \\(\\alpha\\) at \\(\\theta_0\\). The Lagrangian is \\[\n\\begin{aligned}\n&\\int \\phi p_{\\theta}\\,d\\mu - \\lambda_1\\int \\phi p_0\\,d\\mu - \\lambda_2\\int \\phi (T-\\nu_0)p_0 \\,d\\mu \\\\\n&\\quad = \\int \\phi\\left(p_\\theta -\\lambda_1 p_0 - \\lambda_2(T-\\nu_0)p_0\\right)\\,d\\mu \\\\\n&\\quad = \\int \\phi\\left(\\frac{p_\\theta}{p_0} - \\lambda_1 - \\lambda_2(T-\\nu_0)\\right)\\,dP_0.\n\\end{aligned}\n\\] Since \\(\\frac{p_\\theta}{p_0}(x) = e^{\\theta T(x) - A(\\theta)+A(0)}\\), the test that maximizes this Lagrangian has \\[\n\\phi^*(x) = \\begin{cases} 1 &\\quad \\text{ if } e^{\\theta T(x)} &gt; a_0 + a_1 T(x)\\\\\n0 &\\quad \\text{ if } e^{\\theta T(x)} &lt; a_0 + a_1 T(x)\\\\\n\\text{anything} &\\quad \\text{ if } e^{\\theta T(x)} = a_0 + a_1 T(x)\n\\end{cases}\n\\] for \\(a_0 = (\\lambda_1 - \\lambda_2\\nu_0)e^{A(0)-A(\\theta)}\\) and \\(a_1 = \\lambda_2 e^{A(0)-A(\\theta)}\\).\nFor any \\(c_1,c_2\\) we can find \\(a_1 &gt; 0\\) and \\(a_0\\in \\RR\\) for which \\(e^{\\theta t} = a_0 + a_1 t\\) at \\(t = c_1,c_2\\), in which case \\(e^{t\\theta} &gt; a_0 + a_1 t\\) for \\(t &lt; c_1\\) and \\(t &gt; c_2\\) and \\(e^{t\\theta} &lt; a_0 + a_1 t\\) otherwise; then we can solve for \\(\\lambda_1,\\lambda_2 \\in \\RR\\) for which our \\(\\phi^*\\) maximizes the Lagrangian.\nNow, for any other test \\(\\phi\\) that satisfies the unbiasedness constraints we can write \\[\n\\begin{aligned}\n\\beta_{\\phi}(\\theta)  \n&= \\beta_{\\phi}(\\theta) - \\lambda_1\\left(\\beta_{\\phi}(0) - \\alpha\\right) -\\lambda_2 \\dot\\beta_{\\phi}(0)\\\\\n&\\leq \\beta_{\\phi^*}(\\theta) - \\lambda_1\\left(\\beta_{\\phi^*}(0) - \\alpha\\right) -\\lambda_2 \\dot\\beta_{\\phi^*}(0)\\\\\n&= \\beta_{\\phi^*}(\\theta).\n\\end{aligned}\n\\] Since \\(\\theta\\) was arbitrary, we have the result.\nThe proof for \\(\\delta &gt; 0\\) is similar, with the constraints \\(\\beta_{\\phi}(0) = \\alpha\\) and \\(\\dot{\\beta}_{\\phi}(0) = 0\\) replaced by \\(\\beta_{\\phi}(-\\delta) = \\beta_{\\phi}(\\delta) =\\alpha\\)."
  },
  {
    "objectID": "reader/probability.html",
    "href": "reader/probability.html",
    "title": "Probability",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/probability.html#what-is-a-probability",
    "href": "reader/probability.html#what-is-a-probability",
    "title": "Probability",
    "section": "What is a probability?",
    "text": "What is a probability?\n\nThere is some philosophical controversy about what probability is. Roughly speaking, there are two common answers:\n\n\nFrequentist answer: Probability represents a relative frequency of an outcome when an experiment is repeated many times.\nBayesian answer: Probability represents a degree of belief that something is true, or that something will happen.\n\nPeople who give the first answer tend to be doubtful that we can meaningfully assign probabilities to everything. There is no sense in which the 2024 presidential election is an experiment that we can repeat. Below is a list of outcomes that seem more and more difficult to assign probabilities to. Think about where you would stop agreeing that probability is a meaningful construct:\n\nThe probability that a (roughly) physically symmetrical die will roll 4 on the next toss: most people, regardless of their metaphysical views about probability, would probably agree on \\(1/6\\), whether it is because they can imagine the die being tossed many times, because they believe the die has an equal physical propensity to land on any given face, or because they think we should be subjectively indifferent between any of the six outcomes.\nThe probability that a radiation treatment will be successful for a given cancer patient: we could probably give some answer based on the fraction of patients for whom the treatment is successful, but we have to be careful about what comparison group to use: we should probably only include patients whose cancer is at a similar stage and who are roughly the same age, but perhaps we should take into account other factors like the patient’s family history, other aspects of their health profile, the cancer’s genetic profile, etc. By the time we get done conditioning on everything that might matter, the patient may be the only person left in our comparison class.\nThe probability that the Democratic candidate will win the next presidential election: every presidential election is a truly one-off event, and anyone whose opinion is worth listening to can probably list a few important aspects of this election that are more or less unprecedented.\nThe probability that a given subatomic particle has the mass predicted by a certain physical theory: this is not the probability that something will happen but a probability that something is true about the world. Whether it is true or false, it has been so since the creation of the universe.\nThe probability that P = NP (as a statement about computational complexity theory): whether or not this is true is a purely mathematical question, but it is one that may or may not be resolved in our lifetime.\nThe probability that the 20th digit of \\(\\sqrt{2}\\) is 5: this is another mathematical question that you could probably work out for yourself in a few minutes with just a pencil and paper and no new data about the world, so it seems perverse to think of it as random. Still, if you were forced to place a bet and didn’t have time to work it out you might assign a \\(10\\%\\) probability.\n\nThe two great statistical frameworks of frequentist and Bayesian statistics loosely correspond to the two views about probability above. Generally speaking, Bayesians get a lot of mileage out of being willing to assign probabilities to everything, while frequentists try to take a more conservative course where possible, allowing some quantities to simply be unknown.\nA great deal of ink has been spilled about whether one approach is superior to another. Fifty years ago, it was more common for statisticians to be dogmatically committed to one or the other view, but nowadays the mainstream view is that both approaches have strengths and weaknesses, and which framework to use is a pragmatic choice that should be made on a case-by-case basis.\nFortunately, the philosophical and methodological disagreements between frequentists and Bayesians are not disagreements about the mathematical construct of probability, only questions about how and when these mathematical formalisms should be connected to real-world questions. There is very little controversy about how we should conceive of probability mathematically.\n\nMathematical answer: a probability is a function \\(P\\) mapping (non-pathological1) subsets of a sample space \\(\\cX\\) to a number in \\([0,1]\\), for which \\(P(\\cX) = 1\\) and which is additive over disjoint subsets, meaning\n\n\\[\nP\\left(\\bigcup_{i=1}^\\infty A_i\\right) = \\sum_{i=1}^\\infty P(A_i), \\quad \\text{ if } A_i \\cap A_j = \\emptyset \\text{ for all } i \\neq j.\n\\]"
  },
  {
    "objectID": "reader/probability.html#probability-as-a-measure",
    "href": "reader/probability.html#probability-as-a-measure",
    "title": "Probability",
    "section": "Probability as a measure",
    "text": "Probability as a measure\nMeasure theory is an area of mathematics concerned with measuring the “size” of subsets of a certain set. Soon after it was developed in the the early twentieth century, the great Soviet mathematician Kolmogorov realized it could be applied to give a rigorous grounding to probability theory, it was a major advance in understanding and resolving certain paradoxes in probability theory. David Aldous gives a nice discussion of this history.\nThis is not a course on measure-theoretic probability and we will not rigorously develop the subject. However, it will be useful to draw on some of the basics of measure theory, because the subject clarifies the meaning of conditioning and allows us to simplify our notation; for example, the measure-theoretic idea of a density includes both probability density functions and probability mass functions, allowing us to cleanly state results that apply both to discrete and continuous random variables, as well as to discrete-continuous mixtures, random graphs, and even random variables defined on more exotic sample spaces like manifolds."
  },
  {
    "objectID": "reader/probability.html#measures",
    "href": "reader/probability.html#measures",
    "title": "Probability",
    "section": "Measures",
    "text": "Measures\nGiven a set \\(\\cX\\), a measure\\(\\mu\\) is a certain kind of function mapping “nice enough” subsets \\(A \\subseteq \\cX\\) to non-negative numbers \\(\\mu(A) \\in [0,\\infty]\\).\nExample 1 (Counting measure): If \\(\\cX\\) is countable, e.g. \\(\\cX = \\mathbb{Z}\\), then a natural measure is the counting measure \\(\\#(A)\\), which simply counts the number of points in a subset \\(A\\). That is, \\(\\#(\\{0,1\\}) = 2\\), and \\(\\#(\\{2,4,6,8,\\ldots\\}) = \\infty\\) .\nExample 2 (Lebesgue measure): If \\(\\cX = \\RR^n\\) for some integer \\(n\\), a natural measure is the Lebesgue measure \\(\\lambda(A)\\), which returns the volume of a subset \\(A\\). Roughly speaking, we can write\n\\[\n\\lambda(A) = \\int \\cdots \\int_A \\,d x_1\\,d x_2\\cdots \\,d x_n.\n\\]\nExample 3 (Gaussian measure): Now taking \\(\\cX = \\RR\\), we might instead want to define the “size” of a set as the probability that a standard Gaussian random variable \\(Z \\sim \\cN(0,1)\\) is observed to be in the set \\(A\\). That is, we can define the measure:\n\\[\nP_Z(A) = \\PP(Z \\in A) = \\int_A \\phi(x)\\,d x, \\quad \\text{ where } \\;\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\n\\]\nis the probability density function of \\(Z\\).\nAs it turns out, it is not so obvious how to define what exactly we mean by taking an integral when the set is sufficiently pathological: some sets are just called non-measurable and we can’t hope to meaningfully assign them a measure. Sets of this kind are important to consider when building a rigorous theory about measures but they are not the sort of thing you would stumble upon unless you went out looking for them.\nOne of the original motivations for measure theory was to provide a framework for excluding these pathological sets and rigorously defining integrals over the other, nicer sets. In general, the domain of a measure is not all subsets of \\(\\cX\\) (called the power set and notated \\(2^{\\cX}\\)), but rather a collection of “nice enough” subsets \\(\\cF \\subseteq 2^{\\cX}\\).\nFormally, the collection \\(\\cF\\) must be a \\(\\sigma\\)-field, meaning that it satisfies certain closure properties. We say \\(\\cF\\) is a \\(\\sigma\\)-field (or \\(\\sigma\\)-algebra) if\n\nThe full set \\(\\cX\\) is in \\(\\cF\\).\nIf \\(A\\) is in \\(\\cF\\) then its complement \\(\\cX \\setminus A\\) is also in \\(\\cF\\) (i.e., \\(\\cF\\) is closed under complementation)\nIf \\(A_1,A_2,\\ldots \\in \\cF\\) then \\(\\bigcup_{i=1}^\\infty A_i\\) is also in \\(\\cF\\) (i.e. \\(\\cF\\) is closed under countable unions)\n\nThe details of this definition are not important for purposes of this course.\nExample: If \\(\\cX\\) is countable we can take \\(\\cF\\) to be the entire power set.\nExample: If \\(\\cX = \\mathbb{R}^n\\) we will typically use the Borel \\(\\sigma\\)-field \\(\\cB\\), defined as the smallest \\(\\sigma\\)-field that includes all open rectangles \\((a_1,b_1)\\times (a_2,b_2) \\times \\cdots \\times (a_n, b_n)\\), where \\(a_i &lt; b_i\\) for all \\(i\\). That is, we start with the open rectangles and recursively apply the closure properties to obtain a very large collection of sets, which informally we can think of as containing all non-pathological subsets of \\(\\mathbb{R}^n\\).\nWe are now ready to define a measure. We call a pair of a set \\(\\cX\\) and an associated \\(\\sigma\\)-field \\(\\cF \\subseteq 2^{\\cX}\\) a measurable space. Given a measurable space \\((\\cX, \\cF)\\), a measure is a function \\(\\mu: \\cF \\to [0,\\infty]\\) (inclusive of \\(+\\infty\\)) satisfying three properties:\n\nNon-negativity: \\(\\mu(A) \\geq 0\\) for all \\(A\\in \\cF\\).\nEmpty set maps to zero: \\(\\mu(\\emptyset) = 0\\)\nCountable additivity: If \\(A_1,A_2,\\ldots\\in \\cF\\) are all disjoint, then\n\n\\[\n\\mu\\left(\\bigcup_{i=1}^\\infty A_i\\right) = \\sum_{i=1}^\\infty \\mu(A_i)\n\\]\nIf \\(\\mu\\) is a measure on \\((\\cX, \\cF)\\) we call \\((\\cX, \\cF, \\mu)\\) a measure space. The second requirement, that \\(\\mu(\\emptyset) = 0\\), may seem redundant given the countable additivity condition, but it is just there to prevent us from saying the measure of every set is infinite (but we could still say the measure of every set except \\(\\emptyset\\) is infinite).\nIn the special case \\(\\mu(\\cX) = 1\\), we call \\(\\mu\\) a probability measure and \\((\\cX, \\cF, \\mu)\\) is called a probability space."
  },
  {
    "objectID": "reader/probability.html#integrals",
    "href": "reader/probability.html#integrals",
    "title": "Probability",
    "section": "Integrals",
    "text": "Integrals\nOne very nice thing about measures is that they let us define integrals of (nice enough) real-valued functions on \\(\\cX\\) with respect to the measure \\(\\mu\\), meaning the integral is “weighted” in a way that assigns total weight \\(\\mu(A)\\) to each set \\(A\\). We will use the notation \\(\\int f(x)\\,\\,d\\mu(x)\\), or just \\(\\int f \\,d\\mu\\).\nTo construct this integral, we begin by defining it for indicator functions, and then extend to more general functions by linearity and limits, in a few steps:\nFirst, for an indicator function \\(1_A(x) = 1\\{x \\in A\\}\\) of a set \\(A \\in \\cF\\), it is straightforward to define the integral as \\(\\int 1_A \\,d\\mu = \\mu(A)\\) (note if \\(A \\notin \\cF\\) this does not work, but we are only defining the integral for a class of “nice” functions determined by our \\(\\sigma\\)-field)\nNext, consider a simple function \\(f(x) = \\sum_{i=1}^\\infty c_i 1_{A_i}(x)\\), with all \\(c_i \\geq 0\\) and \\(A_i \\in \\cF\\). Because the integral should be linear, we should have\n\\[\n\\int f\\,d\\mu = \\sum_{i=1}^\\infty c_i \\int 1_{A_i}\\,d\\mu = \\sum_{i=1}^\\infty c_i \\mu(A_i)\n\\]\nThird, we can extend to all sufficiently nice non-negative functions by approximating them from below with a series of simple functions:\n\\[\n\\int f\\,d\\mu = \\lim_{n=1}^\\infty \\int f_n\\,d\\mu.\n\\]\nFor example, we could take the sequence of simple functions \\[f_n(x) = \\sum_{k=0}^{\\infty} k 2^{-n} 1_{A_{n,k}}(x),\\quad \\text{ where } A_{n,k} = \\left\\{x:\\; f(x) \\in \\left[k 2^{-n}, (k+1) 2^{-n}\\right)\\right\\},\\] as illustrated in the picture below.\n\nPlot = require(\"@observablehq/plot\")\nd3 = require(\"d3\")\n\n// Define the Gaussian function\nfunction gaussian(x, mu = 0, sigma = 1) {\n  return (1 / (sigma * Math.sqrt(2 * Math.PI))) * \n    Math.exp(-0.5 * Math.pow((x - mu) / sigma, 2));\n}\n\n// Define our function f(x) = 3g(x-4) + 5g(x-7)\nfunction f(x) {\n  return 3 * gaussian(x - 4, 0, 1) + 4.8 * gaussian(x - 7, 0, 1);\n}\n\n// Create a slider for 'c'\nviewof c = Inputs.range([0, 4], {step: 1, label: \"n\"})\n\n// Generate x values\nxValues = d3.range(-2, 13, 0.01)\n\n// Calculate y values\ndata = xValues.map(x =&gt; ({x, y: f(x)}))\n\n// Calculate Lebesgue sets with multiple intervals\nfunction calculateLebesgueSets(data, c) {\n  const maxY = d3.max(data, d =&gt; d.y);\n  const sets = d3.range(0, Math.ceil(maxY * Math.pow(2, c))).map(k =&gt; {\n    const lower = k * Math.pow(2, -c);\n    const upper = (k + 1) * Math.pow(2, -c);\n    const points = data.filter(d =&gt; d.y &gt; lower && d.y &lt;= upper);\n    \n    if (points.length &gt; 0) {\n      // Identify distinct intervals\n      const intervals = [];\n      let currentInterval = [points[0]];\n      for (let i = 1; i &lt; points.length; i++) {\n        if (points[i].x - points[i-1].x &lt;= 0.011) { // Allow small gaps\n          currentInterval.push(points[i]);\n        } else {\n          intervals.push(currentInterval);\n          currentInterval = [points[i]];\n        }\n      }\n      intervals.push(currentInterval);\n\n      return {\n        k,\n        lower,\n        upper,\n        intervals: intervals.map(interval =&gt; ({\n          xMin: d3.min(interval, d =&gt; d.x),\n          xMax: d3.max(interval, d =&gt; d.x),\n          points: interval\n        }))\n      };\n    }\n    return null;\n  }).filter(set =&gt; set !== null);\n  \n  return sets;\n}\n\nlebesgueSets = calculateLebesgueSets(data, c)\n\n// Create the plot\nplot = Plot.plot({\n  marks: [\n    Plot.line(data, {x: \"x\", y: \"y\", stroke: \"gray\", strokeWidth: 1}),\n    Plot.dot(lebesgueSets.flatMap(set =&gt; set.intervals.flatMap(int =&gt; int.points)), {\n      x: \"x\", \n      y: \"y\", \n      fill: d =&gt; d3.schemeCategory10[lebesgueSets.findIndex(s =&gt; s.intervals.some(int =&gt; int.points.includes(d))) % 10],\n      r: 2\n    }),\n    Plot.rectY(lebesgueSets.flatMap(set =&gt; \n      set.intervals.map(int =&gt; ({\n        x1: int.xMin,\n        x2: int.xMax,\n        y1: 0,\n        y2: set.lower,\n        color: set.k\n      }))\n    ), {\n      x1: \"x1\",\n      x2: \"x2\",\n      y1: \"y1\",\n      y2: \"y2\",\n      fill: d =&gt; d3.schemeCategory10[d.color % 10],\n      fillOpacity: 0.2,\n      stroke: d =&gt; d3.schemeCategory10[d.color % 10],\n      strokeOpacity: 0.5\n    })\n  ],\n  y: {\n    domain: [0, d3.max(data, d =&gt; d.y) * 1.1],\n    label: \"f(x)\"\n  },\n  x: {\n    domain: [-2, 13],\n    label: \"x\"\n  },\n  style: {\n    backgroundColor: \"white\"\n  },\n  width: 800,\n  height: 500\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we can write any real-valued function as the sum of its positive and negative parts, \\(f(x) = f^+(x) - f^-(x)\\), where \\(f^+(x) = \\max\\{f(x), 0\\}\\) and \\(f^-(x) = \\max\\{-f(x), 0\\}\\). Then both \\(f^+\\) and \\(f^-\\) have non-negative (possibly infinite) integrals. Then we simply take\n\\[\n\\int f\\,d\\mu = \\int f^+\\,d\\mu - \\int f^-\\,d\\mu \\in [-\\infty, \\infty],\n\\]\ncalling the difference undefined if the integrals of both \\(f^+\\) and \\(f^-\\) are infinite.\nAs a result, we have \\(\\int f\\,d\\mu\\) for any function \\(f\\) whose positive and negative parts can both be approximated from below by simple functions. Note that we have left out some important details in this presentation (for example we have not characterized which functions \\(f\\) are nice enough to be approximated well by simple functions) but these details are unimportant for this class. The important thing to know is that to any measure \\(\\mu\\) there corresponds a well-defined integral \\(\\int \\cdot \\,d\\mu\\), which behaves as we would expect it to.\nWe can now return to our previous examples of measures and ask what the corresponding integrals are:\nExample 1, continued (Counting measure): An integral with respect to \\(\\#\\) just adds up all the values of \\(f(x)\\):\n\\[\n\\int f\\,d\\# = \\sum_{x\\in \\cX} f(x)\n\\]\nExample 2, continued (Lebesgue measure): An integral with respect to the Lebesgue measure is called a Lebesgue integral, which is essentially just the usual integral you are used to from calculus class:\n\\[\n\\int f\\,d\\lambda = \\int\\cdots \\int f(x) \\,d x_1 \\cdots \\,d x_n.\n\\]\nThe Lebesgue integral extends the Riemann integral to a more general class of functions, in the sense that if the Riemann integral of \\(f\\) is defined then the Lebesgue integral is also well defined and the two integrals coincide. But the Lebesgue integral is also well-defined for functions like \\(f(x) = 1\\{x \\in \\mathbb{Q}\\}\\), for which the Riemann integral is not well-defined (Exercise: what is the Lebesgue integral of \\(1\\{x \\in \\mathbb{Q}\\}\\)?)\nExample 3, continued (Gaussian measure): Note that \\(P_Z(A)\\) is defined as the (Lebesgue) integral of \\(1_A(x)\\phi(x)\\). By extension, the integral of \\(f\\) with respect to \\(P_Z\\) is the Lebesgue integral of \\(f(x) \\phi(x)\\), which is nothing more than the expectation of \\(f(Z)\\):\n\\[\n\\int f\\,d P_Z = \\int_{-\\infty}^\\infty f(x) \\phi(x) \\,d x = \\EE[f(Z)].\n\\]"
  },
  {
    "objectID": "reader/probability.html#densities",
    "href": "reader/probability.html#densities",
    "title": "Probability",
    "section": "Densities",
    "text": "Densities\nWe have just seen in the last two examples that there is a special relationship between the Lebesgue measure \\(\\lambda\\) on \\(\\RR\\) and the Gaussian measure \\(P_Z\\), allowing us to evaluate integrals with respect to \\(P_Z\\) by turning them into integrals with respect to \\(\\lambda\\), namely \\(\\int f(x)\\,d P_Z(x) = \\int f(x)\\phi(x) \\,d\\lambda(x)\\).\nThis is a happy fact, since mathematicians have gone to a lot of trouble figuring out how to calculate integrals with respect to the usual (Lebesgue) measure. Most of the expectations we want to calculate in statistics are integrals with respect to some joint probability measure over random variables, and we certainly wouldn’t want to have to reinvent the wheel of integration every time we want to do calculations with respect to a new random variable.\nNote that we can’t turn integrals for every random variable into Lebesgue integrals. If \\(Y\\) follows a binomial distribution, for example, we can just as well define \\(P_Y(A) = \\PP(Y \\in A)\\), but there is no counterpart to \\(\\phi\\) that would let us turn \\(P_Y\\) integrals into Lebesgue integrals in the same way.\nFormally, consider a measurable space \\((\\cX, \\cF)\\), with two measures \\(P\\) and \\(\\mu\\). We say \\(P\\) is absolutely continuous with respect to \\(\\mu\\) if \\(P(A) = 0\\) whenever \\(\\mu(A) = 0\\). In notation, we write \\(P \\ll \\mu\\).\nIf \\(P \\ll \\mu\\) then, under mild conditions, we can always define a density function \\(p:\\cX \\mapsto [0,\\infty)\\) such that\n\\[\nP(A) = \\int 1_A(x) p(x)\\,d\\mu(x), \\quad \\text{ for all } A \\in \\cF,\n\\]\nand by extension \\(\\int f(x)\\,d P(x) = \\int f(x) p(x) \\,d\\mu(x)\\).\nThe function \\(p\\) is called the density function or Radon-Nikodym derivative of \\(P\\) with respect to \\(\\mu\\). It is sometimes written using the suggestive notation \\(\\frac{\\,d P}{\\,d\\mu}(x)\\). Whenever we have a density function we can turn integrals with respect to \\(P\\) into integrals with respect to \\(\\mu\\) simply by multiplying the density \\(p\\) into the integrand.\nIf we do not specify what \\(\\mu\\) is, it is assumed to be the Lebesgue measure; that is, if we say \\(P\\) is absolutely continuous with no further elaboration, we mean \\(P \\ll \\lambda\\).\nIf \\(P\\) is a probability measure and \\(\\mu\\) is the Lebesgue measure, then \\(p(x)\\) is a probability density function (pdf). If \\(\\mu\\) is a counting measure, we call \\(p(x)\\) the probability mass function (pmf)."
  },
  {
    "objectID": "reader/probability.html#probability-spaces-and-random-variables",
    "href": "reader/probability.html#probability-spaces-and-random-variables",
    "title": "Probability",
    "section": "Probability spaces and random variables",
    "text": "Probability spaces and random variables\nIn a typical statistics problem we may have multiple random variables, some continuous and some discrete, some random variables may be functions of others, and we want to have good notation for the probability that “something happens,” like \\(\\PP(X^2 &lt; (Y+Z)/ W)\\), or the expectation of a random variable, like \\(\\EE[(XY-ZW)^2]\\). What does this kind of expression have to do with the measures and integrals we have been discussing so far?\nIf we defined a probability measure \\(P\\) as the joint distribution of \\((X, Y, Z, W)\\) then we could evaluate \\(P\\) on the corresponding subset of the sample space, e.g. \\(P(\\{x,y,z,w :\\; x^2 &lt; (y+z)/w})\\). Or we could write an appropriate integral like \\(\\int (xy-zw)^2\\,dP(x,y,z,w)\\). But it is convenient not to have to make things so explicit. To this end, we can introduce the idea of an abstract outcome \\(\\omega\\) in an outcome space \\(\\Omega\\), which informally represents “all of the information required to evaluate every random variable in the problem.” Then a random variable is simply any function of \\(\\omega\\), and we can think of \\(\\PP\\) as a measure on \\(\\Omega\\) and \\(\\EE\\) as an integral with respect to \\(\\PP\\). Then our usual way of writing probabilities and expectations become shorthand for the corresponding measure and integral over \\(\\Omega\\); e.g. \\[\\PP(X^2 &lt; (Y+Z)/W) = \\PP\\left(\\{\\omega \\in \\Omega:\\; X(\\omega)^2 &lt; (Y(\\omega) + Z(\\omega))/W(\\omega)\\}\\right),\\] and \\[\\EE((XY-ZW)^2) = \\int_\\Omega (X(\\omega)Y(\\omega) - Z(\\omega)W(\\omega))^2 \\,d\\PP(\\omega),\\] and it is understood that we will never bother to make explicit what kind of object \\(\\omega\\) is. A subset of \\(\\Omega\\) to which \\(\\PP\\) assigns a probability is called an event and any function of \\(\\omega\\) is a random variable. If \\(\\PP(A) = 1\\), we say \\(A\\) occurs almost surely.\nTo do actual calculations of probabilities and expectations we use the distributions of the random variables involved in that particular calculation. The distribution of a random variable \\(X(\\omega)\\) is given by the push-forward measure defined as \\(Q = \\PP \\circ X^{-1}\\). That is, if \\(X\\) has realizations in the sample space \\(\\cX\\), and \\(B\\) is a (measurable) subset of \\(\\cX\\), then \\[Q(B) = \\PP(X^{-1}(B)) = \\PP(\\{\\omega \\in \\Omega:\\; X(\\omega) \\in B \\}).\\]"
  },
  {
    "objectID": "reader/probability.html#conditional-probability",
    "href": "reader/probability.html#conditional-probability",
    "title": "Probability",
    "section": "Conditional probability",
    "text": "Conditional probability\nWhile it is beyond the scope of this course, measure theory also allows us to patch the definition of conditional probability and conditional expectation. Given two events \\(A\\) and \\(B\\), if \\(\\PP(B) &gt; 0\\), we can unproblematically define the conditional probability of \\(A\\) given \\(B\\) as \\(\\PP(A \\mid B) = \\PP(A \\cap B) / \\PP(B)\\), but this definition obviously fails when \\(\\PP(B) = 0\\).\nGenerally speaking, we cannot necessarily define \\(\\PP(A \\mid B)\\) for measure zero events \\(B\\) (Homework 0 includes a problem illustrating the inherent ambiguity of this definition). But, for example, if \\(X\\) and \\(Y\\) are both continuous random variables with some dependence between them we would like to be able to discuss, e.g., the distribution or expectation of \\(Y\\) given that \\(X\\) takes on some specific value \\(x\\). We can do this by defining the conditional expectation \\(\\EE(Y \\mid X)\\) as a random variable \\(g(X)\\), which has the property \\(\\EE[(Y - g(X)) 1_A(X)] = 0\\) for all (nice) subsets \\(A\\). By evaluating this function \\(g\\) at \\(x\\) we can answer the question we asked earlier. However, note this explanation is informal and brushes many important points under the rug; for a more complete explanation, take Stat 205A.\nHaving defined the conditional expectation, we can also ask about the conditional distribution of \\(Y\\) by evaluating the conditional expectation on new random variables defined with indicator functions: \\(\\PP(Y \\in A \\mid X) = \\EE[1_A(Y) \\mid X]\\) ."
  },
  {
    "objectID": "reader/probability.html#footnotes",
    "href": "reader/probability.html#footnotes",
    "title": "Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA problem in the first homework will guide you step-by-step in constructing the kind of pathological set that makes the parenthetical caveat necessary when we are dealing with continuous spaces.↩︎"
  },
  {
    "objectID": "reader/likelihood-inference.html",
    "href": "reader/likelihood-inference.html",
    "title": "Likelihood-Based Inference: Wald, Score, and Generalized Likelihood Ratio Tests",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/likelihood-inference.html#likelihood-based-inference",
    "href": "reader/likelihood-inference.html#likelihood-based-inference",
    "title": "Likelihood-Based Inference: Wald, Score, and Generalized Likelihood Ratio Tests",
    "section": "1 Likelihood-Based Inference",
    "text": "1 Likelihood-Based Inference\n\n1.1 Setting\n\\(X_1, \\ldots, X_n \\stackrel{\\text{iid}}{\\sim} p_\\theta(x)\\), \\(p_\\theta \\in \\cP\\), smooth in \\(\\theta\\)\nAssume: - \\(\\mathbb{E}_\\theta[\\nabla \\ell_\\theta(X)] = 0\\) - \\(\\text{Var}_\\theta[\\nabla \\ell_\\theta(X)] = \\mathbb{E}_\\theta[-\\nabla^2 \\ell_\\theta(X)] = J(\\theta) &gt; 0\\) - MLE \\(\\hat{\\theta}\\) Consistent\nThen if \\(\\theta = \\theta_0\\): - \\(\\nabla \\ell_n(\\theta_0; X) \\sim N(0, nJ(\\theta_0))\\) - \\(-\\nabla^2 \\ell_n(\\theta_0; X) \\xrightarrow{p} nJ(\\theta_0)\\)\nUsed \\(\\theta = \\hat{\\theta} + J^{-1}(\\theta_0) \\nabla \\ell_n(\\theta_0; X)/n + o_p(n^{-1/2})\\) to get \\(\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\sim N(0, J^{-1}(\\theta_0))\\)\nCan use this for inference on \\(\\theta_0\\)"
  },
  {
    "objectID": "reader/likelihood-inference.html#wald-type-confidence-regions",
    "href": "reader/likelihood-inference.html#wald-type-confidence-regions",
    "title": "Likelihood-Based Inference: Wald, Score, and Generalized Likelihood Ratio Tests",
    "section": "2 Wald-Type Confidence Regions",
    "text": "2 Wald-Type Confidence Regions\nAssume we have some estimator \\(\\hat{\\theta}_n\\) s.t. \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\xrightarrow{d} N(0, J^{-1}(\\theta_0))\\). Then we can plug in:\nIf \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\sim N_d(0, J^{-1}(\\theta_0))\\), then \\(nJ(\\theta_0)(\\hat{\\theta}_n - \\theta_0) \\sim N_d(0, I_d)\\)\nSo \\(n(\\hat{\\theta}_n - \\theta_0)^T J(\\theta_0)(\\hat{\\theta}_n - \\theta_0) \\sim \\chi^2_d\\) (Slutsky)\nLeads to test of \\(H_0: \\theta = \\theta_0\\) \\(H_1: \\theta \\neq \\theta_0\\): Reject if \\(n(\\hat{\\theta}_n - \\theta_0)^T J(\\theta_0)(\\hat{\\theta}_n - \\theta_0) &gt; \\chi^2_{d,1-\\alpha}\\)\nSo \\(\\mathbb{P}_{\\theta_0}(n(\\hat{\\theta}_n - \\theta_0)^T J(\\theta_0)(\\hat{\\theta}_n - \\theta_0) \\leq \\chi^2_{d,1-\\alpha}) = 1-\\alpha\\)\nNote: we reject \\(\\theta_0\\) iff \\(\\{\\theta_n: n(\\hat{\\theta}_n - \\theta)^T J(\\theta)(\\hat{\\theta}_n - \\theta) \\leq \\chi^2_{d,1-\\alpha}\\}\\) reject \\(\\theta_0\\) iff \\(\\theta_0 \\notin \\{\\theta: n(\\hat{\\theta}_n - \\theta)^T J(\\theta)(\\hat{\\theta}_n - \\theta) \\leq \\chi^2_{d,1-\\alpha}\\}\\)\nRegion \\(\\{\\theta: n(\\hat{\\theta}_n - \\theta)^T J(\\theta)(\\hat{\\theta}_n - \\theta) \\leq \\chi^2_{d,1-\\alpha}\\}\\) is confidence ellipsoid\nMore info = smaller ellipse (shrinks like \\(\\sqrt{n}\\))\n\n2.1 Estimating \\(J(\\theta)\\)\nTwo options is to plug-in the MLE: 1. MLE for \\(J_n(\\theta)\\): \\(J_n(\\hat{\\theta}_n) = -\\frac{1}{n}\\nabla^2 \\ell_n(\\hat{\\theta}_n; X)\\) 2. \\(\\hat{J}_n(\\theta) = \\frac{1}{n}\\text{Var}_\\theta[\\nabla \\ell_n(\\theta; X)] = \\frac{1}{n}\\sum_{i=1}^n \\nabla \\ell_\\theta(X_i) \\nabla \\ell_\\theta(X_i)^T\\)\nNB: \\(\\text{Var}_\\theta[\\nabla \\ell_n(\\theta; X)] = n\\text{Var}_\\theta[\\nabla \\ell_\\theta(X)] = 0\\)\nOr \\(\\hat{J}_n = \\mathbb{E}_{\\hat{\\theta}_n}[-\\nabla^2 \\ell_{\\hat{\\theta}_n}(X)]\\)\n\n2.1.1 Remarks\n\nBoth have \\(\\hat{J}_n \\xrightarrow{p} J(\\theta_0)\\) in nice iid sampling setting\nBoth make sense outside of iid setting\nHeuristically: plug-in measures info about \\(\\theta\\) in typical data set, but obs info measures info about \\(\\theta\\) in this data set\n\n\n\n\n2.2 Wald Interval for \\(\\theta_j\\)\nIf \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\sim N_d(\\theta_0, J_n^{-1}(\\theta_0))\\) then \\(\\hat{\\theta}_n \\sim N_d(\\theta_0, J_n^{-1}(\\theta_0)/n)\\)\nLeads to univariate interval: \\(s.e.(\\hat{\\theta}_{n,j}) = \\sqrt{[J_n^{-1}(\\hat{\\theta}_n)]_{jj}/n}\\)\n\\(C_j = [\\hat{\\theta}_{n,j} \\pm z_{1-\\alpha/2} \\cdot s.e.(\\hat{\\theta}_{n,j})]\\)\nglm function in R uses these intervals/p-values with \\(\\hat{J}_n = J_n(\\hat{\\theta}_n)\\)\nConf ellipsoid for \\(\\theta_0\\): \\(\\{\\theta: n(\\hat{\\theta}_n - \\theta)^T \\hat{J}_n(\\hat{\\theta}_n)(\\hat{\\theta}_n - \\theta) \\leq \\chi^2_{d,1-\\alpha}\\}\\)\nMore generally, if \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\xrightarrow{d} N(0, \\Sigma(\\theta_0))\\) and \\(\\hat{\\Sigma}_n(\\theta) \\xrightarrow{p} \\Sigma(\\theta_0)\\) (not nec. MLE) then we can do the same things\n\n\n2.3 Example: Generalized Linear Model with Fixed Design\n\\(X_1, \\ldots, X_n \\in \\mathbb{R}^d\\) fixed \\(Y_1, \\ldots, Y_n \\sim p_{\\eta_i}(y)\\) indep, \\(Y_i | X_i \\sim p_{\\eta_i}(y)\\) \\(\\eta_i = \\beta^T X_i\\) (canonical form)\nLet \\(\\mu_i(\\beta) = \\mathbb{E}_\\beta[Y_i] = \\psi'(\\eta_i)\\)\nMore general: \\(\\eta_i = f(\\beta^T X_i)\\) for \\(f\\) monotone\nMost common examples include: - Logistic regression: \\(Y_i \\sim \\text{Bern}(e^{\\eta_i}/(1+e^{\\eta_i}))\\) - Poisson log-linear model: \\(Y_i \\sim \\text{Pois}(e^{\\eta_i})\\)\n\\(\\ell_n(\\beta; Y) = \\sum_{i=1}^n [Y_i \\eta_i - \\psi(\\eta_i) + \\log h(Y_i)]\\)\n\\(\\nabla \\ell_n(\\beta; Y) = \\sum_{i=1}^n (Y_i - \\mu_i(\\beta)) X_i\\)\n\\(\\mathbb{E}_\\beta[Y_i] = \\mu_i(\\beta) = \\psi'(\\eta_i)\\)\n\\(\\nabla^2 \\ell_n(\\beta; Y) = -\\sum_{i=1}^n \\psi''(\\eta_i) X_i X_i^T\\)\n\\(\\text{Var}_\\beta(Y_i) = \\psi''(\\eta_i)\\) (not random)\n\\(\\hat{\\beta} \\stackrel{\\cdot}{\\sim} N(\\beta, J_n^{-1}(\\beta))\\) in finite samples \\(\\xrightarrow{d} N(0, J^{-1})\\)\nUnder regularity cond. on \\(X\\): Taylor expansion of \\(\\ell_n\\) leads to \\(\\sqrt{n}(\\hat{\\beta}_n - \\beta) \\xrightarrow{d} N(0, J^{-1})\\)\n\n2.3.1 Advantages of Wald Test\n\nEasy to invert, simple conf regions\nAsymptotically correct\n\n\n\n2.3.2 Disadvantages\n\nHave to compute MLE\nDepends on parameterization\nRelies on two approximations: \\(\\ell_n\\) Normal and \\(\\ell_n\\) quadratic\nNeed MLE to be consistent\nConfidence interval/ellipsoid might go outside \\(\\Theta\\)"
  },
  {
    "objectID": "reader/likelihood-inference.html#score-test",
    "href": "reader/likelihood-inference.html#score-test",
    "title": "Likelihood-Based Inference: Wald, Score, and Generalized Likelihood Ratio Tests",
    "section": "3 Score Test",
    "text": "3 Score Test\nTest \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta \\neq \\theta_0\\)\nWe can bypass quadratic approximation entirely by using score as test stat:\n\\(\\nabla \\ell_n(\\theta_0; X) \\sim N(0, nJ(\\theta_0))\\) or \\(J_n^{-1/2}(\\theta_0)\\nabla \\ell_n(\\theta_0; X) \\stackrel{\\cdot}{\\sim} N_d(0, I_d)\\)\nSo we can reject \\(H_0: \\theta = \\theta_0\\) if \\(\\|\\hat{J}_n^{-1/2}(\\theta_0)\\nabla \\ell_n(\\theta_0; X)\\|^2 &gt; \\chi^2_{d,1-\\alpha}\\)\n\\(\\nabla \\ell_n(\\theta_0; X)^T \\hat{J}_n^{-1}(\\theta_0) \\nabla \\ell_n(\\theta_0; X) \\sim \\chi^2_d\\)\nCan do 1-sided tests\n\n3.1 Remarks\n\nNo quadratic approx, no MLE\nNo need to estimate Fisher info at \\(\\theta_0\\)\nCan be generalized to case with nuisance params\nTypically estimate via MLE on \\(\\Theta_0\\)\n\n\n\n3.2 Score Test is Invariant to Reparameterization\nAssume \\(\\Theta \\subset \\mathbb{R}^d\\), \\(\\eta = g(\\theta)\\), \\(\\Psi = g(\\Theta)\\)\n\\(q_\\eta(x) = p_{g^{-1}(\\eta)}(x)\\)\n\\(\\ell_\\eta(x) = \\log q_\\eta(x) = \\ell_\\theta(x)\\)\n\\(\\nabla_\\eta \\ell_\\eta(x) = \\nabla_\\theta \\ell_\\theta(x) \\cdot \\nabla g^{-1}(\\eta)\\)\n\\(J_\\eta(\\eta) = J_\\theta(g^{-1}(\\eta)) \\cdot \\nabla g^{-1}(\\eta) \\cdot \\nabla g^{-1}(\\eta)^T\\)\nSo \\(\\nabla_\\eta \\ell_\\eta(x)^T J_\\eta^{-1}(\\eta) \\nabla_\\eta \\ell_\\eta(x) = \\nabla_\\theta \\ell_\\theta(x)^T J_\\theta^{-1}(\\theta) \\nabla_\\theta \\ell_\\theta(x)\\)\nif \\(\\eta_0 = g(\\theta_0)\\)\n\n\n3.3 Example: 1-Parameter Exponential Family\n\\(X_1, \\ldots, X_n \\stackrel{\\text{iid}}{\\sim} e^{\\eta T(x) - A(\\eta)} h(x)\\)\n\\(\\nabla \\ell_n(\\eta; X) = \\sum T(X_i) - n\\mu(\\eta)\\)\n\\(\\ell_n''(\\eta; X) = -n\\text{Var}_\\eta[T(X)]\\)\n\\(\\hat{\\eta}_n = \\text{MLE} = A'^{-1}(\\bar{T})\\)\n\\(\\frac{\\sum T(X_i) - n\\mu(\\eta_0)}{\\sqrt{n\\text{Var}_{\\eta_0}[T(X)]}} \\sim N(0,1)\\)\n\n\n3.4 Example: \\(X_1, \\ldots, X_n \\stackrel{\\text{iid}}{\\sim} \\text{Laplace}(\\theta, 2\\sqrt{2})\\)\nTest \\(H_0: \\theta = 0\\) vs \\(H_1: \\theta \\neq 0\\) (two-tailed)\n\\(\\ell_n(\\theta; X) = \\sum |X_i - \\theta| - n\\log(4\\sqrt{2})\\)\n\\(\\nabla \\ell_n(\\theta; X) = \\sum \\text{sgn}(\\theta - X_i) = \\sum [\\mathbb{I}(X_i &lt; \\theta) - \\mathbb{I}(X_i &gt; \\theta)]\\)\n\\(\\nabla \\ell_n(0; X) = \\sum [\\mathbb{I}(X_i &lt; 0) - \\mathbb{I}(X_i &gt; 0)] = \\sum \\text{sgn}(-X_i)\\)\n\\(J_n(0) = n/2\\)\n\\(\\sqrt{2/n} \\sum \\text{sgn}(-X_i) \\sim N(0,1)\\) (sign test)\nNote: this test is the exact NP/UMP test for \\(H_0: \\theta = 0\\) vs \\(H_0: |\\theta| = \\epsilon\\) for \\(\\epsilon &gt; 0\\)\nIntuition: Maximize power for nearby alternatives since we’ll have power for \\(\\theta \\gg 0\\)\nMore generally, one-sided score test is almost UMP for nearby alternatives\n\\(p_\\theta(x) \\approx p_0(x)[1 + \\epsilon \\ell'_0(X)]\\) for small \\(\\epsilon &gt; 0\\)\n\n\n3.5 Example: Pearson’s \\(\\chi^2\\) Test (Goodness of Fit)\n\\(N = (N_1, \\ldots, N_d) \\sim \\text{Multi}(n, \\pi)\\), \\(\\pi_i \\geq 0\\), \\(\\sum \\pi_i = 1\\)\n\\(\\ell_n(\\pi; N) = \\sum N_i \\log \\pi_i\\)\nNote \\(\\mathbb{E}[\\pi] = 1\\) so this is a full rank \\(d-1\\) parameter exp family e.g. \\(T_j = \\mathbb{I}(\\text{category} = j)\\), \\(j=1,\\ldots,d-1\\)\n\\(\\nabla \\ell_n(\\pi; N) = (N_1/\\pi_1, \\ldots, N_d/\\pi_d)^T - n1_d\\)\n\\(\\hat{\\pi} = \\text{MLE} = (N_1/n, \\ldots, N_d/n)\\)\n$J_n() = n[(_1^{-1}, , _d^{-1})"
  },
  {
    "objectID": "reader/maximum-likelihood.html",
    "href": "reader/maximum-likelihood.html",
    "title": "Maximum Likelihood Estimation: Theory and Asymptotics",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/maximum-likelihood.html#maximum-likelihood-estimation",
    "href": "reader/maximum-likelihood.html#maximum-likelihood-estimation",
    "title": "Maximum Likelihood Estimation: Theory and Asymptotics",
    "section": "1 Maximum Likelihood Estimation",
    "text": "1 Maximum Likelihood Estimation\nFor a generic dominated family \\(\\cP = \\{P_\\theta: \\theta \\in \\Theta\\}\\) with densities \\(f_\\theta\\), a simple estimator for \\(\\theta\\) is:\n\\[\\hat{\\theta}_{\\text{MLE}}(X) = \\arg\\max_{\\theta \\in \\Theta} p_\\theta(X) = \\arg\\max_{\\theta \\in \\Theta} \\prod_{i=1}^n f_\\theta(X_i) = \\arg\\max_{\\theta \\in \\Theta} \\ell_n(\\theta; X)\\]\nRemarks: 1. \\(\\arg\\max\\) may not exist, be unique, or be computable 2. Doesn’t depend on parameterization or base measure; MLE for \\(g(\\theta)\\) is \\(g(\\hat{\\theta}_{\\text{MLE}})\\)\n\n1.1 Example: Exponential Family\n\\[\\ell(\\eta; X) = \\eta^T T(X) - A(\\eta) + \\log h(X)\\]\n\\(T(\\bar{X}) = \\mathbb{E}_\\eta[T(X)]\\) if such \\(\\eta\\) exists\nBecause \\(\\ell''(\\eta; X) = -\\text{Var}_\\eta[T(X)]\\) is negative definite unless \\(\\eta \\to T(\\eta)\\) constant, in which case param redundant\nAt most 1 solution exists\nLet \\(m(X) = \\mathbb{E}_\\eta[T(X)] = \\nabla A(\\eta)\\)\n\n\n1.2 Example: Normal Distribution\n\\(X_i \\sim \\text{iid } N(\\theta, \\sigma^2)\\), \\(h(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-x^2/(2\\sigma^2)}\\), \\(\\eta \\in \\mathbb{R}\\)\n\\(T(X) = \\frac{X}{\\sigma^2}\\), \\(A(\\eta) = \\frac{\\eta^2}{2\\sigma^2}\\)\nAssume \\(\\eta = \\theta/\\sigma^2\\), \\(\\sigma^2\\) known\n\\(m(\\eta) = \\eta\\sigma^2 = \\theta\\), \\(m^{-1}(\\theta) = \\theta/\\sigma^2\\)\nConsistency: \\(\\bar{X} \\xrightarrow{p} \\theta\\) (LLN)\nCts mapping: \\(\\hat{\\eta} = m^{-1}(\\bar{X}) \\xrightarrow{p} \\theta/\\sigma^2\\)\nSince \\(\\sqrt{n}(\\bar{X} - \\theta) \\xrightarrow{d} N(0, \\text{Var}_\\eta[T(X)])\\):\n\\(N(0, \\sigma^2)\\)\nRecall: \\(J(\\eta) = \\text{Var}_\\eta[T(X)]\\)\nDelta method: \\(\\sqrt{n}(\\hat{\\eta} - \\eta) \\xrightarrow{d} N(0, [m^{-1'}(\\theta)]^2 \\sigma^2)\\)\n\\(N(0, 1/\\sigma^2)\\)\nRecall: \\(J(\\eta) = \\text{Var}_\\eta[T(X)] = \\sigma^2\\)\n\\(N(0, J^{-1})\\)\nAsymptotically unbiased Gaussian, achieves CRLB\n\n\n1.3 Example: Poisson Distribution\n\\(X_i \\sim \\text{iid Poisson}(\\theta)\\), \\(\\eta = \\log \\theta\\)\n\\(T(X) = X\\), \\(\\mathbb{E}[X] = \\theta\\), \\(N(0, \\theta)\\)\n\\(\\hat{\\eta}_n = \\log \\bar{X}\\), \\(\\sqrt{n}(\\log \\bar{X} - \\log \\theta) \\xrightarrow{d} N(0, \\theta^{-1})\\) (Delta method)\n\\(N(0, \\theta^{-1})\\)\nBut for finite \\(n\\), \\(\\mathbb{P}(\\bar{X} = 0) = \\mathbb{P}(X_1 = 0)^n = e^{-n\\theta} &gt; 0\\)\nMLE can have embarrassing finite sample performance despite being asymptotically optimal\n\n\n1.4 Proof: Convergence in Distribution with Probability Approaching 1\nIf \\(\\mathbb{P}(B_n) \\to 1\\), \\(X_n \\xrightarrow{d} X\\), \\(Z_n\\) arbitrary, then \\(X_n 1_{B_n} + Z_n 1_{B_n^c} \\xrightarrow{d} X\\)\nProof: \\(\\mathbb{P}(\\|Z_n 1_{B_n^c}\\| &gt; \\epsilon) \\leq \\mathbb{P}(B_n^c) \\to 0\\), so \\(Z_n 1_{B_n^c} \\xrightarrow{p} 0\\) Also, \\(1_{B_n} \\xrightarrow{p} 1\\), apply Slutsky\nAny zany behavior has no effect on convergence in distribution"
  },
  {
    "objectID": "reader/maximum-likelihood.html#asymptotic-efficiency",
    "href": "reader/maximum-likelihood.html#asymptotic-efficiency",
    "title": "Maximum Likelihood Estimation: Theory and Asymptotics",
    "section": "2 Asymptotic Efficiency",
    "text": "2 Asymptotic Efficiency\nIn the exponential family case, generalizes to a much broader class of models\nSetting: \\(X_1, \\ldots, X_n \\stackrel{\\text{iid}}{\\sim} p_\\theta(x)\\), \\(\\theta \\in \\mathbb{R}^d\\)\n\\(p_\\theta\\) smooth in \\(\\theta\\) (e.g., 2 cts integrable derives, can be relaxed)\nLet \\(\\ell_i(\\theta; X) = \\log p_\\theta(X_i)\\), \\(\\ell_n(\\theta; X) = \\sum_{i=1}^n \\ell_i(\\theta; X)\\)\n\\(S_n(\\theta) = \\nabla_\\theta \\ell_n(\\theta; X)\\), \\(J_n(\\theta) = \\text{Var}_\\theta[\\nabla_\\theta \\ell_n(\\theta; X)] = nJ_1(\\theta)\\)\nWe say an estimator is asymptotically efficient if \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\xrightarrow{d} N(0, J_1^{-1}(\\theta))\\)\nDelta method for differentiable estimand \\(g(\\theta)\\):\n\\(\\sqrt{n}(g(\\hat{\\theta}_n) - g(\\theta)) \\xrightarrow{d} N(0, \\nabla g(\\theta)^T J_1^{-1}(\\theta) \\nabla g(\\theta))\\)\nAlso achieves CRLB if \\(\\hat{\\theta}_n\\) does, \\(g\\) diff"
  },
  {
    "objectID": "reader/maximum-likelihood.html#asymptotic-distribution-of-mle",
    "href": "reader/maximum-likelihood.html#asymptotic-distribution-of-mle",
    "title": "Maximum Likelihood Estimation: Theory and Asymptotics",
    "section": "3 Asymptotic Distribution of MLE",
    "text": "3 Asymptotic Distribution of MLE\nUnder mild conditions, \\(\\hat{\\theta}_n\\) is asymptotically Gaussian efficient\nWe will be interested in \\(\\ell_n(\\theta; X)\\) as a function of \\(\\theta\\) Notate true value as \\(\\theta_0\\): \\(X \\sim P_{\\theta_0}\\)\nDerivatives of \\(\\ell_n\\) at \\(\\theta_0\\): \\(S_0\\), \\(S_1\\)\n\\(S_n(\\theta_0; X) = \\sum_{i=1}^n \\nabla \\ell_i(\\theta_0; X_i) \\sim N(0, J_n(\\theta_0))\\)\n\\(\\mathbb{E}[S_n(\\theta; X)] = n\\mathbb{E}[\\nabla \\ell_i(\\theta_0; X_i)] = 0\\)\n\\(\\mathbb{E}[-\\nabla^2 \\ell_n(\\theta; X)] = \\mathbb{E}[\\sum_{i=1}^n -\\nabla^2 \\ell_i(\\theta_0; X_i)] = J_n(\\theta_0)\\)\n\n3.1 Informal Proof\nTaylor expansion between \\(\\theta_0\\), \\(\\hat{\\theta}_n\\):\n\\(S_n(\\hat{\\theta}_n; X) = S_n(\\theta_0; X) + S_n'(\\theta_0; X)(\\hat{\\theta}_n - \\theta_0)\\)\n\\(\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) = J_n^{-1}(\\theta_0) S_n(\\theta_0; X)/\\sqrt{n}\\)\n\\(\\xrightarrow{d} N(0, J_1^{-1}(\\theta_0))\\)\nMore rigorous proof later, but note we need consistency of \\(\\hat{\\theta}_n\\) first to even justify Taylor expansion\n\n\n3.2 Quadratic Approximation\nQuadratic approximation near \\(\\theta_0\\):\n\\(\\ell_n(\\theta) \\approx \\ell_n(\\theta_0) + (\\theta - \\theta_0)^T S_n(\\theta_0) - \\frac{1}{2}(\\theta - \\theta_0)^T J_n(\\theta_0)(\\theta - \\theta_0)\\)\n\\(N(J_n^{-1}(\\theta_0)S_n(\\theta_0), J_n^{-1}(\\theta_0))\\)\nGaussian linear term + Deterministic curvature\n\\(\\ell_n(\\theta) - \\ell_n(\\theta_0) \\approx -\\frac{n}{2}(\\theta - \\hat{\\theta}_n)^T J_1(\\theta_0)(\\theta - \\hat{\\theta}_n) + \\text{const}\\)"
  },
  {
    "objectID": "reader/maximum-likelihood.html#consistency-of-mle",
    "href": "reader/maximum-likelihood.html#consistency-of-mle",
    "title": "Maximum Likelihood Estimation: Theory and Asymptotics",
    "section": "4 Consistency of MLE",
    "text": "4 Consistency of MLE\n\\(X_1, \\ldots, X_n \\stackrel{\\text{iid}}{\\sim} f_{\\theta_0}\\), \\(\\theta_n \\in \\arg\\max_{\\theta \\in \\Theta} \\ell_n(\\theta; X)\\)\nWill be ok if \\(\\theta_n\\) comes close to maximizing \\(\\ell_n\\)\nQuestion: When does \\(\\ell_n \\to \\ell\\)?\nAssume model identifiable: \\(P_\\theta = P_{\\theta_0}\\) for \\(\\theta \\neq \\theta_0\\)\nRecall KL Divergence:\n\\(D(g\\|f) = \\mathbb{E}_g[\\log \\frac{g(X)}{f(X)}] = \\int g(x) \\log \\frac{g(x)}{f(x)} dx\\) (note switch)\n\\(\\log \\frac{g}{f} \\geq 1 - \\frac{f}{g}\\) (strict ineq unless const, i.e., unless \\(f=g\\))\nLet \\(W_i(\\theta) = \\ell_i(\\theta; X_i) - \\ell_i(\\theta_0; X_i)\\), \\(W(\\theta) = \\mathbb{E}_{\\theta_0}[W_i(\\theta)]\\)\nNote: \\(\\theta_0 = \\arg\\max_{\\theta \\in \\Theta} W(\\theta)\\)\n\\(W(\\theta_0) = 0\\)\n\\(W(\\theta) = -\\mathbb{E}_{\\theta_0}[\\log \\frac{f_{\\theta_0}(X)}{f_\\theta(X)}] = -D(f_{\\theta_0}\\|f_\\theta) \\leq 0\\)\n\\(= 0\\) iff \\(\\theta = \\theta_0\\)\nBut not enough: 1. MLE \\(\\hat{\\theta}_n\\) depends on entire function \\(\\ell_n\\) 2. Need uniform convergence in \\(\\theta\\)\n\n4.1 Definition: Compact Convergence\nFor compact \\(K\\), let \\(C(K) = \\{f: K \\to \\mathbb{R} \\text{ cts}\\}\\)\nFor \\(f \\in C(K)\\), let \\(\\|f\\| = \\sup_{x \\in K} |f(x)|\\)\n\\(f_n \\to f\\) in this norm if \\(\\|f_n - f\\| \\to 0\\)\n\n\n4.2 Theorem: LLN for Random Functions\nAssume \\(K\\) compact, \\(W_i, W_2, \\ldots \\in C(K)\\) iid \\(\\mathbb{E}[\\|W_i\\|] &lt; \\infty\\), \\(\\mathbb{E}[W_i(\\theta)] = W(\\theta)\\)\nThen \\(\\bar{W}_n \\in C(K)\\) and \\(\\mathbb{P}(\\|\\bar{W}_n - W\\| &gt; \\epsilon) \\to 0\\)\ni.e., \\(\\ell_n \\to \\ell\\) in \\(\\|\\cdot\\|_\\infty\\) norm\n\n\n4.3 Theorem (Keener 9.4)\nLet \\(G_n, G\\) random functions in \\(K\\) compact 1. \\(G_n \\to g\\) in \\(\\|\\cdot\\|_\\infty\\), some fixed \\(g \\in C(K)\\). Then: a. If \\(t^* \\in K\\) fixed, then \\(G_n(t^*) \\xrightarrow{p} g(t^*)\\) b. If \\(g\\) maximized at unique value \\(t^*\\) and \\(G_n(t_n) = \\max_t G_n(t)\\), then \\(t_n \\xrightarrow{p} t^*\\) 2. If \\(K \\subset \\mathbb{R}\\), \\(g'(t) = 0\\) has unique sol. \\(t^*\\) and \\(t_n\\) solve \\(G_n'(t) = 0\\), then \\(t_n \\xrightarrow{p} t^*\\)\n(Sketch of proof in purple) If \\(G_n'(t_n) = 0\\), get \\(G_n'(t^*) = g'(t^*) + (g'(t_n) - g'(t^*)) + (G_n'(t^*) - g'(t^*))\\) \\(\\to 0 + 0 + 0\\) by assumptions + by cts mapping\nFix \\(\\epsilon &gt; 0\\), let \\(B_\\epsilon = \\{t: \\|t - t^*\\| &lt; \\epsilon\\}\\) Let \\(K_\\epsilon = K \\setminus B_\\epsilon(t^*)\\), \\(K \\setminus B_\\epsilon\\) compact \\(\\delta = g(t^*) - \\max_{t \\in K_\\epsilon} g(t) &gt; 0\\)\nIf \\(t_n \\notin K_\\epsilon\\), then \\(G_n(t_n) &gt; G_n(t^*) - \\frac{\\delta}{2} &gt; g(t^*) - \\delta = \\max_{t \\in K_\\epsilon} g(t)\\)\n\\(\\mathbb{P}(t_n \\notin K_\\epsilon) \\leq \\mathbb{P}(\\|G_n - g\\|_\\infty &gt; \\frac{\\delta}{2}) \\to 0\\)\nAnalogous to \\(\\bar{X}_n \\xrightarrow{p} \\mu\\)\n\n\n4.4 Theorem: Consistency of MLE for Compact \\(\\Theta\\)\n\\(X_1, \\ldots, X_n \\stackrel{\\text{iid}}{\\sim} f_{\\theta_0}\\), \\(\\cP\\) has densities \\(p_\\theta\\), \\(\\theta \\in \\Theta\\)\nAssume: 1. \\(p_\\theta\\) cts in \\(\\theta\\) 2. \\(\\Theta\\) compact 3. \\(\\mathbb{E}_{\\theta_0}[\\sup_\\theta |f_\\theta(X)|/f_{\\theta_0}(X)] &lt; \\infty\\) 4. \\(\\mathbb{E}_{\\theta_0}[\\sup_\\theta |W_i(\\theta)|] &lt; \\infty\\) 5. Model identifiable\nThen \\(\\hat{\\theta}_n \\xrightarrow{p} \\theta_0\\) if $"
  },
  {
    "objectID": "reader/testing-nuisance.html",
    "href": "reader/testing-nuisance.html",
    "title": "Testing with Nuisance Parameters",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/testing-nuisance.html#nuisance-parameters",
    "href": "reader/testing-nuisance.html#nuisance-parameters",
    "title": "Testing with Nuisance Parameters",
    "section": "1 Nuisance Parameters",
    "text": "1 Nuisance Parameters\n\n1.1 Common Setup\nExtra unknown parameters which are not of direct interest:\n\\(\\cP = \\{P_{\\theta, \\lambda}: \\theta \\in \\Theta, \\lambda \\in \\Lambda\\}\\)\n\\(H_0: \\theta \\in \\Theta_0\\) vs \\(H_1: \\theta \\in \\Theta_1\\)\n\n\\(\\theta\\): parameter of interest\n\\(\\lambda\\): nuisance parameter\n\nIssue: \\(\\lambda\\) unknown but might affect type I error or power of a given test\n\n\n1.2 Examples\n\n\\(X_1, \\ldots, X_n \\sim \\text{iid } N(\\mu, \\sigma^2)\\), \\(Y_1, \\ldots, Y_m \\sim \\text{iid } N(\\nu, \\sigma^2)\\) \\(\\mu, \\nu, \\sigma^2\\) unknown \\(H_0: \\mu = \\nu\\) vs \\(H_1: \\mu \\neq \\nu\\) \\(\\theta = \\mu - \\nu\\), \\(\\lambda = (\\mu + \\nu, \\sigma^2)\\) or \\((\\mu, \\sigma^2)\\)\n\\(X \\sim \\text{Binom}(n_1, \\pi_1)\\), \\(X_2 \\sim \\text{Binom}(n_2, \\pi_2)\\) \\(n_1, n_2\\) known (not nuisance parameters) \\(H_0: \\pi_1 = \\pi_2\\) vs \\(H_1: \\pi_1 \\neq \\pi_2\\)\n\\(X \\sim N(\\mu, \\sigma^2)\\), \\(\\theta \\in \\mathbb{R}\\), \\(\\lambda \\in \\mathbb{R}\\), both unknown How to test \\(H_0: \\theta = 0\\) vs \\(H_1: \\theta \\neq 0\\)?\n\n\n\n1.3 Idea: Condition on Sufficient Statistic for \\(\\lambda\\)\nCondition on \\(U(X)\\) to eliminate dependence on \\(\\lambda\\)\n\\[p_{\\theta, \\lambda}(t|u) = \\frac{p_{\\theta, \\lambda}(t, u)}{p_{\\lambda}(u)} = \\frac{e^{\\theta \\cdot t} g_\\lambda(t, u)}{\\int e^{\\theta \\cdot s} g_\\lambda(s, u) ds}\\]\nEvaluate \\(H_0: \\theta \\in \\Theta_0\\) vs \\(H_1: \\theta \\in \\Theta_1\\) in s-parameter model \\(\\{p_\\theta(\\cdot|u): \\theta \\in \\Theta\\}\\)\nNote: If \\(s=1\\), this family has MLR in \\(T\\). Even if \\(s&gt;1\\), we have still gotten rid of \\(\\lambda\\)."
  },
  {
    "objectID": "reader/testing-nuisance.html#theorem-informal",
    "href": "reader/testing-nuisance.html#theorem-informal",
    "title": "Testing with Nuisance Parameters",
    "section": "2 Theorem (Informal)",
    "text": "2 Theorem (Informal)\nLet \\(\\cP\\) be full rank exp. fam. with densities \\(p_{\\theta, \\lambda}(x) = e^{\\theta \\cdot T(x) + \\lambda \\cdot U(x) - A(\\theta, \\lambda)}h(x)\\)\n\\(\\theta \\in \\mathbb{R}^s\\), \\(\\lambda \\in \\mathbb{R}^r\\), \\((\\theta_0, \\lambda_0)\\) possible\n\nTo test \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta \\neq \\theta_0\\), there is a UMPU test \\(\\phi(x) = \\psi(T(x), U(x))\\) where\n\\[\\psi(t, u) = \\begin{cases}\n1 & \\text{if } t &gt; c_2(u) \\\\\n\\gamma_2(u) & \\text{if } t = c_2(u) \\\\\n0 & \\text{if } c_1(u) &lt; t &lt; c_2(u) \\\\\n\\gamma_1(u) & \\text{if } t = c_1(u) \\\\\n1 & \\text{if } t &lt; c_1(u)\n\\end{cases}\\]\nwith \\(\\gamma_1, \\gamma_2, c_1, c_2\\) chosen to make \\(\\mathbb{E}_{\\theta_0}[\\phi] = \\alpha\\) and \\(\\mathbb{E}_{\\theta_0}[T\\phi] = \\theta_0\\)\nTo test \\(H_0: \\theta \\leq \\theta_0\\) vs \\(H_1: \\theta &gt; \\theta_0\\), there is a UMPU test \\(\\phi(x) = \\psi(T(x), U(x))\\) where\n\\[\\psi(t, u) = \\begin{cases}\n1 & \\text{if } t &gt; c(u) \\\\\n\\gamma(u) & \\text{if } t = c(u) \\\\\n0 & \\text{if } t &lt; c(u)\n\\end{cases}\\]\nwith \\(\\gamma, c\\) chosen to make \\(\\mathbb{E}_{\\theta_0}[\\phi] = \\alpha\\)\n\nNote: \\(h\\) has disappeared from the problem.\n\n2.1 Example: Poisson Ratio\n\\(X_i \\sim \\text{iid Poisson}(\\mu_i)\\), \\(i=1,2\\)\n\\(H_0: \\mu_1 = \\mu_2\\) vs \\(H_1: \\mu_1 \\neq \\mu_2\\)\n\\[p(x) = \\frac{\\mu_1^{x_1} e^{-\\mu_1}}{x_1!} \\cdot \\frac{\\mu_2^{x_2} e^{-\\mu_2}}{x_2!} = e^{x_1 \\log \\mu_1 + x_2 \\log \\mu_2 - \\mu_1 - \\mu_2}\\]\nLet \\(\\theta = \\log \\frac{\\mu_1}{\\mu_2}\\), \\(\\lambda = \\log \\mu_2\\)\n\\(H_0: \\theta = 0\\) vs \\(H_1: \\theta \\neq 0\\)\nReject for conditionally large values of \\(X_1\\) given \\(X_1 + X_2 = u\\):\n\\[P_\\theta(X_1 = x_1 | X_1 + X_2 = u) = \\frac{e^{\\theta x_1}}{\\sum_{i=0}^u e^{\\theta i}} = \\binom{u}{x_1} \\left(\\frac{e^\\theta}{1+e^\\theta}\\right)^{x_1} \\left(\\frac{1}{1+e^\\theta}\\right)^{u-x_1}\\]\n\\(X_1 | X_1 + X_2 \\sim \\text{Binom}(u, \\frac{e^\\theta}{1+e^\\theta})\\)\nSo in the end, we do a Binomial test."
  },
  {
    "objectID": "reader/testing-nuisance.html#proof-sketch",
    "href": "reader/testing-nuisance.html#proof-sketch",
    "title": "Testing with Nuisance Parameters",
    "section": "3 Proof Sketch",
    "text": "3 Proof Sketch\n\nAny unbiased test has \\(\\mathbb{P}(\\phi=1) \\leq h(t,u)\\) (continuity)\nPower = 0 on boundary \\(\\implies \\mathbb{E}_{\\theta_0}[T\\phi] = \\theta_0\\) (UK complete sufficient on boundary sub-model)\n\\(\\phi\\) optimal among all tests with conditional level \\(\\alpha\\) by reduction to univariate model\n\n\n3.1 Detailed Proof\nAssume \\(\\phi\\) any unbiased test:\n\n\\(\\mathbb{E}_\\theta[\\phi] = \\alpha + f(\\theta)\\), \\(f(\\theta_0) = 0\\), \\(f'(\\theta_0) = 0\\) Keener Thm 12.4\n\\(\\mathbb{E}_\\theta[\\phi]\\) infinitely diff on \\(\\mathbb{R}^s\\), can diff under \\(\\int\\)\n\\(\\phi\\) unbiased \\(\\implies \\mathbb{E}_\\theta[T\\phi] = \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_\\theta[\\phi] = \\theta\\)\n\nStep 1: Boundary sub-model \\(\\cP_0 = \\{p_{\\theta_0, \\lambda}: \\lambda \\in \\mathbb{R}^r\\}\\)\n\\(p_{\\theta_0, \\lambda}(x) = e^{\\lambda \\cdot U(x) - A(\\theta_0, \\lambda)}h(x)\\)\n\\(\\cP_0\\) is full rank \\(r\\)-param exp fam, \\(U(X)\\) complete suff\nLet \\(f(\\lambda) = \\mathbb{E}_{\\theta_0, \\lambda}[\\phi(X) | U(X)] = \\alpha\\)\n\\(\\mathbb{E}_{\\theta_0, \\lambda}[\\phi(X) T(X) | U(X)] = \\theta_0\\) a.s.\n\\(\\mathbb{E}_{\\theta_0, \\lambda}[\\phi(X) | U(X)] = \\alpha\\) a.s.\nTwo-sided: \\(\\mathbb{E}_{\\theta_0, \\lambda}[g(U(X))\\phi(X)] = \\mathbb{E}_{\\theta_0, \\lambda}[T(X)\\phi(X)] = \\theta_0\\)\n\\(\\mathbb{E}_{\\theta_0, \\lambda}[g(U)\\mathbb{E}_{\\theta_0, \\lambda}[\\phi|U]] = \\theta_0\\)\n\\(\\mathbb{E}_{\\theta_0, \\lambda}[g(U) \\alpha] = \\theta_0\\)\n\\(\\mathbb{E}_{\\theta_0, \\lambda}[g(U)] = \\frac{\\theta_0}{\\alpha}\\)\nOne-sided: \\(\\mathbb{E}_{\\theta_0, \\lambda}[\\phi] = \\alpha\\) \\(\\forall \\lambda\\)\nSteps 2-3: For any value \\(u\\), the conditional model is:\n\\[p_\\theta(t|u) = e^{\\theta \\cdot t} g(t,u)\\]\n1-param exp fam.\nIn one/two-sided case, we have shown \\(\\psi(t,u)\\) is UMP/UMPU in \\(\\{p_\\theta(\\cdot|u)\\}\\)\nLet \\(g(t,u) = \\mathbb{E}_{\\theta_0}[\\phi(X) | T(X)=t, U(X)=u] \\leq 1\\)\n\\(\\mathbb{E}_{\\theta_0}[\\psi(T,U) | U] = \\mathbb{E}_{\\theta_0}[\\phi(X) | U(X)=u]\\)\n\\(\\psi\\) if \\(\\theta &gt; \\theta_0\\) \\(\\phi(X)\\) is a conditional test of \\(H_0\\) vs \\(H_1\\) in \\(\\{p_\\theta(\\cdot|u)\\}\\) with power \\(\\leq \\alpha\\) at boundary\nOne-sided case: For \\(\\theta &gt; \\theta_0\\) \\(\\psi(t,u)\\) is the UMP test of \\(\\theta=\\theta_0\\) vs \\(\\theta&gt;\\theta_0\\) in \\(\\{p_\\theta(\\cdot|u)\\}\\), which is a 1-param exp fam\nTwo-sided: \\(\\psi(t,u)\\) is the UMP test of \\(\\theta=\\theta_0\\) vs \\(\\theta \\neq \\theta_0\\) among tests with power \\(\\alpha\\) over \\(\\theta=\\theta_0\\) Keener Thm 12.22 (main thm for two-sided tests)\nIn either case, \\(\\psi\\) has higher cond. power than \\(\\phi\\) a.s.\nFor \\(\\theta \\neq \\theta_0\\):\n\\[\\mathbb{E}_\\theta[\\phi] = \\mathbb{E}_\\theta[\\mathbb{E}[\\phi(X) | T(X), U(X)]]\\] \\[\\leq \\mathbb{E}_\\theta[\\mathbb{E}[\\psi(T(X), U(X)) | T(X), U(X)]]\\] \\[= \\mathbb{E}_\\theta[\\psi]\\]\n\n\n3.2 Example: Normal Mean with Unknown Variance\n\\(X \\sim N(\\mu, \\sigma^2)\\), \\(\\sigma^2&gt;0\\) unknown \\(H_0: \\mu=0\\) vs \\(H_1: \\mu \\neq 0\\)\n\\(T = \\frac{\\bar{X}}{\\|X\\|}\\), \\(U = \\|X\\|^2\\)\nOptimal test rejects when \\(\\bar{X}\\) is extreme given \\(\\|X\\|^2\\)\nIf \\(\\mu=0\\), \\(\\frac{X}{\\|X\\|}\\) is rotationally symmetric \\(\\frac{X}{\\|X\\|} \\sim \\text{Unif}(S^{n-1})\\), \\(\\frac{X}{\\|X\\|}\\) indep of \\(\\|X\\|\\)\nOptimal test rejects when \\(\\frac{\\bar{X}}{\\|X\\|}\\) extreme (marginally)\nCould stop here & simulate\n\n3.2.1 Geometric Picture (n=2)\n[Insert geometric picture here]\nAbove test rejects for: - conditionally extreme \\(\\bar{X}\\) given \\(\\|X\\|^2\\) OR - marginally extreme \\(\\frac{\\bar{X}}{\\|X\\|}\\)\nFact: reject for marginally extreme \\(T\\) where\n\\[T^2 = \\frac{(\\sum X_i)^2}{\\sum X_i^2 - \\frac{1}{n}(\\sum X_i)^2} = \\frac{n\\bar{X}^2}{\\|X\\|^2 - n\\bar{X}^2} = \\frac{n\\bar{X}^2}{S^2}\\]\nand \\(S^2 = \\frac{1}{n-1}\\sum (X_i - \\bar{X})^2\\)\n\n\n3.2.2 Geometric Picture\n[Insert second geometric picture here]\n\\(T^2 = \\frac{\\|\\text{Proj}_\\mathbf{1}X\\|^2}{\\|\\text{Proj}_{\\mathbf{1}^\\perp}X\\|^2} \\cdot \\frac{n-1}{n}\\)\nNext major theme: ratios of projections"
  },
  {
    "objectID": "reader/testing-nuisance.html#permutation-tests",
    "href": "reader/testing-nuisance.html#permutation-tests",
    "title": "Testing with Nuisance Parameters",
    "section": "4 Permutation Tests",
    "text": "4 Permutation Tests\nEven if we don’t get a UMPU test at the end, conditioning on null suff. stat. still helps\nExample: \\(X_1, \\ldots, X_n \\sim \\text{iid } P\\), \\(Y_1, \\ldots, Y_m \\sim \\text{iid } Q\\) \\(H_0: P=Q\\) vs \\(H_1: P \\neq Q\\)\nUnder \\(H_0\\): \\(P=Q\\), \\(X_1, \\ldots, X_n, Y_1, \\ldots, Y_m \\sim P\\)\nLet \\(Z = (Z_1, \\ldots, Z_{n+m}) = (X_1, \\ldots, X_n, Y_1, \\ldots, Y_m)\\)\nUnder \\(H_0\\), \\(U = Z\\) is complete sufficient\nLet \\(S_{n+m}\\) = Permutations on \\(n+m\\) elements\n\\((X, Y) = (U_{\\pi(1)}, \\ldots, U_{\\pi(n+m)})\\) for \\(\\pi \\in S_{n+m}\\)\nThus for test stat \\(T\\), if \\(P=Q\\):\n\\[\\mathbb{P}(T \\geq t | U) = \\frac{1}{(n+m)!}\\sum_{\\pi \\in S_{n+m}} 1\\{T(Z_{\\pi(1)}, \\ldots, Z_{\\pi(n+m)}) \\geq t\\}\\]\nMonte Carlo test: In practice, we sample $_1, , _B"
  },
  {
    "objectID": "reader/unbiased-estimation.html",
    "href": "reader/unbiased-estimation.html",
    "title": "Unbiased Estimation",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/unbiased-estimation.html#outline",
    "href": "reader/unbiased-estimation.html#outline",
    "title": "Unbiased Estimation",
    "section": "1 Outline",
    "text": "1 Outline\n\nConvex Loss\nRao-Blackwell Theorem\nUMVU Estimators\nExamples"
  },
  {
    "objectID": "reader/unbiased-estimation.html#unbiased-estimation",
    "href": "reader/unbiased-estimation.html#unbiased-estimation",
    "title": "Unbiased Estimation",
    "section": "2 Unbiased Estimation",
    "text": "2 Unbiased Estimation\nRecall from Lecture 2 that we had two primary strategies to choose an estimator:\n\nSummarize the risk function by a scalar (average or supremum)\nRestrict attention to a smaller class of estimators\n\nToday we’ll discuss unbiased estimation, which is an example of the second strategy. That is, if \\(g(\\theta)\\) is our estimand, we will require that \\(\\EE_\\theta \\delta = g(\\theta)\\) for all \\(\\theta\\)\nUnbiased estimation is especially convenient in models with a complete sufficient statistic \\(T(X)\\). In that case:\n\nThere is at most one unbiased \\(\\delta(T(X))\\) (if \\(\\delta_1, \\delta_2(T)\\) are both unbiased, then \\(\\delta_1 \\eqas \\delta_2\\))\nIf an unbiased estimator exists, it uniformly minimizes risk for any convex loss function"
  },
  {
    "objectID": "reader/unbiased-estimation.html#convex-loss-functions",
    "href": "reader/unbiased-estimation.html#convex-loss-functions",
    "title": "Unbiased Estimation",
    "section": "3 Convex Loss Functions",
    "text": "3 Convex Loss Functions\nRecall \\(f(y)\\) is convex if for all \\(x_1, x_2\\) and all \\(\\gamma \\in [0,1]\\):\n\\[f(\\gamma x_1 + (1-\\gamma)x_2) \\leq \\gamma f(x_1) + (1-\\gamma) f(x_2)\\] \\(f\\) is strictly convex if the inequality is strict unless \\(x_1 = x_2\\).\nAn key fact about convex functions is Jensen’s Inequality: If \\(f\\) is convex, then for any random variable \\(X\\), we have\n\\[f(\\EE[X]) \\leq \\EE[f(X)]\\] If \\(f\\) is strictly convex, then the inequality is strict unless \\(X\\) is constant.\nWe say a loss function \\(L(\\theta, d)\\) is (strictly) convex if is (strictly) convex as a function of the estimate \\(d\\), its second argument, holding the parameter \\(\\theta\\) fixed.\nExample: The best-known example of a convex loss function is the squared error loss. Recall that the corresponding risk, the MSE, can be decomposed as the sum of the bias squared and the variance:\n\\[\n\\begin{aligned}\n\\text{MSE}_\\theta(\\delta) &= \\EE_\\theta[(\\delta(X) - g(\\theta))^2] \\\\[5pt]\n&= \\text{Bias}_\\theta(\\delta)^2 + \\text{Var}_\\theta(\\delta(X))\n\\end{aligned}\n\\] If \\(\\delta(X)\\) is unbiased, then its MSE is exactly its variance, so minimizing the risk among unbiased estimators just amounts to finding one with the least variance."
  },
  {
    "objectID": "reader/unbiased-estimation.html#the-rao-blackwell-theorem",
    "href": "reader/unbiased-estimation.html#the-rao-blackwell-theorem",
    "title": "Unbiased Estimation",
    "section": "4 The Rao-Blackwell Theorem",
    "text": "4 The Rao-Blackwell Theorem\nIntuitively, convex losses punish us for using noisy estimators: we would always improve the risk if we could replace an estimator \\(\\delta(X)\\) with its expectation: \\[L(\\theta, \\EE_\\theta [\\delta(X)]) \\leq \\EE_\\theta \\left[L(\\theta, \\delta(X))\\right].\\]Generally, this is not feasible in real problems because \\(\\EE_\\theta [\\delta(X)]\\) depends on \\(\\theta\\), so it is not an estimator. But for any sufficient statistic \\(T(X)\\), the conditional expectation of \\(\\delta(X)\\) given \\(T(X)\\) really is an estimator, and it is always at least as good as \\(\\delta(X\\)) if the loss is convex.\nThe next result formalizes this fact, and thereby gives decision-theoretic teeth to the sufficiency principle:\nTheorem (Rao-Blackwell): Let \\(T(X)\\) be sufficient for \\(\\cP = \\{P_\\theta:\\;\\theta\\in\\Theta\\}\\), and let \\(\\delta(X)\\) be any estimator for \\(g(\\theta)\\). Define the new estimator:\n\\[\\bar{\\delta}(T(X)) = \\EE[\\delta(X) \\mid T(X)]\\]\nThen for any convex loss \\(L(\\theta, d)\\), we have \\(R(\\theta, \\bar{\\delta}) \\leq R(\\theta, \\delta)\\) for all \\(\\theta\\). If \\(L\\) is strictly convex, \\(\\bar{\\delta}\\) strictly dominates \\(\\delta\\) as an estimator unless \\(\\delta(X) \\eqPas \\bar{\\delta}(T(X))\\).\nProof:\n\\[\\begin{aligned}\nR(\\theta, \\bar{\\delta}) &= \\EE_\\theta\\left[\\,L(\\theta, \\;\\EE[\\delta \\mid T])\\,\\right]\\\\[5pt]\n&\\leq \\EE_\\theta\\left[\\,\\EE[L(\\theta, \\delta) \\mid T]\\,\\right]\\\\[5pt]\n&= \\EE_\\theta[\\,L(\\theta, \\delta)\\,] \\\\[5pt]\n&= R(\\theta, \\bar{\\delta})\n\\end{aligned}\\]\n\\(\\bar{\\delta}\\) is called the Rao-Blackwellization of \\(\\delta\\). Note that the condition $\\delta(X) \\eqPas \\bar{\\delta}(T(X))$ is equivalent to the condition that \\(\\delta\\) depends only on \\(X\\) through \\(T(X)\\).\nWhenever we are dealing with a convex loss, the Rao-Blackwell theorem lets us restrict our attention only to estimators that run through \\(T(X)\\), because any other estimator could be improved (or at least not worsened) by Rao-Blackwellization. The theorem even gives us a recipe for **constructing** the improved estimator."
  },
  {
    "objectID": "reader/unbiased-estimation.html#umvu-estimators",
    "href": "reader/unbiased-estimation.html#umvu-estimators",
    "title": "Unbiased Estimation",
    "section": "5 UMVU estimators",
    "text": "5 UMVU estimators\nIn this section we will combine two key facts from this lecture and last concerning unbiased estimation of any estimand \\(g(\\theta)\\).\n\nIf \\(T(X)\\) is complete sufficient, there can be at most one unbiased estimator based on \\(T(X)\\).\nIf the loss is convex, then we can restrict our attention only to estimators that are based on \\(T(X)\\).\n\nTogether these facts imply that, if any unbiased estimator exists at all, then there is a unique best unbiased estimator.\nNot all estimands have unbiased estimators. We say \\(g(\\theta)\\) is U-estimable if there exists any \\(\\delta(X)\\) with \\(\\EE_\\theta \\delta(X) = g(\\theta)\\) for all \\(\\theta\\). This leads to the following theorem:\nTheorem: For model \\(\\mathcal{P} = \\{P_\\theta : \\theta \\in \\Theta\\}\\), assume \\(T(X)\\) is a complete sufficient statistic. Then\n\nFor any U-estimable \\(g(\\theta)\\) there exists a unique unbiased estimator of the form \\(\\delta(T(X))\\).\nFor a (strictly) convex loss, that estimator (strictly) dominates any other unbiased estimator \\(\\tilde{\\delta}(X)\\) unless \\(\\tilde{\\delta}(X) \\eqas \\delta(T(X))\\).\n\nAs usual the “uniqueness” here is only up to \\(\\eqPas\\).\nProof:\n(1) Since \\(g(\\theta)\\) is U-estimable, there exists some unbiased estimator \\(\\delta_0(X)\\). Then its Rao-Blackwellization \\(\\delta(T(X)) = \\EE[\\delta_0 \\mid T]\\) is also unbiased, since\n\\[\n\\EE_\\theta \\delta(T) = \\EE_\\theta[\\EE[\\delta_0 | T]] = \\EE_\\theta \\delta_0 = g(\\theta).\n\\]\nAny other estimator of the form \\(\\tilde\\delta(T)\\) must be almost surely equal to \\(\\delta(T)\\), by completeness: if \\(f(t) = \\delta(t)-\\tilde\\delta(t)\\), then both estimators being unbiased means \\(\\EE_\\theta f(T) = g(\\theta)-g(\\theta) = 0\\), so \\(f(T(X)) \\eqas 0\\). Thus, \\(\\delta(T)\\) is unique.\n(2) The first result implies that every unbiased estimator has the same Rao-Blackwellization, namely \\(\\delta(T)\\). Thus, by the Rao-Blackwell theorem, \\(\\delta(T)\\) (strictly) dominates every other unbiased estimator for any (strictly) convex loss function, unless the estimator is almost surely identical to \\(\\delta\\). \\(\\blacksquare\\)\nThe estimator from this theorem is usually called the UMVU (Uniformly Minimum Variance Unbiased) Estimator. We say \\(\\delta(X)\\) is UMVU if:\n\n\\(\\delta(X)\\) is unbiased\n\\(\\text{Var}_\\theta \\,\\delta(X) \\leq \\text{Var}_\\theta \\,\\tilde{\\delta}(X)\\) for all \\(\\theta\\) and all unbiased \\(\\tilde{\\delta}(X)\\)\n\nSince \\(\\text{MSE}(\\theta; \\delta) = \\text{Var}_\\theta(\\delta(X))\\) for any unbiased estimator, and the squared error loss is strictly convex, Theorem XXX immediately implies the existence of a unique UMVU estimator for any U-estimable \\(g(\\theta)\\), whenever we have a complete sufficient statistic.\nNote that in problems where no complete sufficient statistic exists, there can be multiple unbiased estimators based on the minimal sufficient statistic; for example, both the mean and the median are unbiased for the Laplace location parameter, but they are not almost surely equal to each other, and they do not have the same risk function."
  },
  {
    "objectID": "reader/unbiased-estimation.html#finding-the-umvue",
    "href": "reader/unbiased-estimation.html#finding-the-umvue",
    "title": "Unbiased Estimation",
    "section": "6 Finding the UMVUE",
    "text": "6 Finding the UMVUE\nTheorem XXX suggests two strategies for finding the UMVUE:\n\nSolve directly for an unbiased estimator based on \\(T\\)\nFind any unbiased estimator at all, then Rao-Blackwellize it\n\nWe give examples of both strategies below:\nExample (Poisson): Let \\(X_1, \\ldots, X_n \\sim \\text{Pois}(\\theta)\\), \\(g(\\theta) = e^{-\\theta}\\) and consider unbiased estimation for \\(g(\\theta) = \\theta^2\\).\nThe complete sufficient statistic for the model is\n\\[T(X) = \\sum X_i \\sim \\text{Pois}(n\\theta),\\] and its probability mass function for \\(t \\geq 0\\) is \\[\np_\\theta(t) = \\frac{e^{-n\\theta} (n\\theta)^t}{t!}\n\\] Strategy 1\nIf there is some unbiased estimator \\(\\delta(t)\\), we can try to solve for it by setting its expectation equal to \\(\\theta^2\\): \\[\n\\theta^2 = \\EE_\\theta \\delta(T) = \\sum_{t=0}^\\infty \\delta(t) \\frac{e^{-n\\theta} (n\\theta)^t}{t!}.\n\\] Rearranging factors, we obtain matching power series: \\[\n\\sum_{t=0}^\\infty \\delta(t) \\frac{n^t \\theta^t}{t!} = e^{n\\theta}\\theta^2 =  \\sum_{k=0}^\\infty \\frac{n^k\\theta^{k+2}}{k!}.\n\\] We will choose the coefficients on the left-hand side to match terms. First, change the index for the left-hand sum to \\(t = k+2\\): \\[\n\\sum_{t=0}^\\infty \\delta(t) \\frac{n^t \\theta^t}{t!} = e^{n\\theta}\\theta^2 =  \\sum_{t=2}^\\infty \\frac{n^{t-2}\\theta^{t}}{(t-2)!}.\n\\] To match the terms, we can set \\(\\delta(0)=\\delta(1)=0\\), and for \\(t\\geq 2\\), set \\(\\delta(t)=\\frac{t!}{n^2(t-2)!}=\\frac{t(t-1)}{n^2}\\). The same expression works for both, so we obtain the estimator \\[\n\\delta(T) = \\frac{T(T-1)}{n^2}\n\\]\nStrategy 2:\nAlternatively, we can find an unbiased estimator and Rao-Blackwellize it. If $n$, we can use the fact that\n\\[\n\\EE_\\theta [X_1 X_2] = \\EE_\\theta [X_1] \\;\\cdot\\; \\EE_\\theta [X_2] = \\theta^2\n\\] to obtain an initial unbiased estimator \\(\\delta_0(X) = X_1X_2\\), which we will Rao-Blackwellize.\nConditional on $\nto match the terms\n\\(\\delta(T) = (1 - 1/n)^T\\) unbiased:\n\\[\\begin{aligned}\n\\EE_\\theta \\delta(T) &= \\sum_{t=0}^\\infty (1-1/n)^t e^{-n\\theta} (n\\theta)^t / t! \\\\\n&= e^{-n\\theta} \\sum_{t=0}^\\infty ((n-1)\\theta)^t / t! \\\\\n&= e^{-n\\theta} e^{(n-1)\\theta} = e^{-\\theta}\n\\end{aligned}\\]\nAlternatively, we could Rao-Blackwellize \\(\\delta_0(X) = I(X_1 = 0)\\):\n\\[\\begin{aligned}\n\\EE[I(X_1 = 0) | T] &= \\PP(X_1 = 0 | T) \\\\\n&= \\frac{\\PP(X_1 = 0, X_2 + \\cdots + X_n = T)}{\\PP(X_2 + \\cdots + X_n = T-1) + \\PP(X_2 + \\cdots + X_n = T)} \\\\\n&= \\frac{\\binom{n-1}{T} (1/n)^0 (1-1/n)^T}{\\binom{n-1}{T-1} (1/n) (1-1/n)^{T-1} + \\binom{n-1}{T} (1-1/n)^T} \\\\\n&= \\frac{(1-1/n)^T}{T/n + (1-1/n)^T} \\\\\n&= (1-1/n)^T\n\\end{aligned}\\]\nExample: \\(X_1, \\ldots, X_n \\sim U[0, \\theta]\\), \\(\\theta &gt; 0\\)\n\\(T = X_{(n)}\\) complete sufficient\n\\(p_\\theta(t) = n t^{n-1} / \\theta^n \\cdot I(0 &lt; t &lt; \\theta)\\)\n\\(\\EE_\\theta[T] = \\frac{n}{n+1} \\theta\\)\n\\(T \\cdot \\frac{n+1}{n}\\) is UMVUE\nAlternatively, \\(2X_1\\) is unbiased:\n\\[\\EE[2X_1 | T] = 2T \\cdot \\frac{n+1}{2n} = T \\cdot \\frac{n+1}{n}\\]\nActually, \\(T\\) is inadmissible too! Keener shows \\(\\frac{n-1}{n} T\\) has better MSE for any estimator \\(c \\cdot T\\).\nThis raises the question: why do we require zero bias?\nThe UMVUE is often inefficient, inadmissible, or just dumb in cases where another approach makes much more sense.\nExample: \\(X \\sim \\text{Bin}(1000, \\theta)\\)\nEstimate \\(g(\\theta) = I(\\theta &gt; 0.5)\\)\nUMVUE is \\(I(X &gt; 500)\\). Why?\n\n\\(X = 500\\): Conclude \\(g(\\theta) = 1\\)\n\\(X = 499\\): Conclude \\(g(\\theta) = 0\\)\n\nThis is not epistemically reasonable. Could do much better with e.g. MLE or a Bayes estimator.\nIn fact, our theorem should make us suspicious of UMVUEs: every idiotic function of \\(T\\) is a UMVUE of its own expectation!\nExample: \\(X_1, \\ldots, X_n \\sim N(\\mu, 1)\\), estimate \\(g(\\mu) = \\|\\mu\\|\\)\n\\(\\bar{X}\\) is complete sufficient\n\\(\\|\\bar{X}\\|\\) is unbiased: \\(\\EE[\\|\\bar{X}\\|] = \\EE[\\|N(\\mu, 1/n)\\|] = \\|\\mu\\|\\)\nSo \\(\\|\\bar{X}\\|\\) is UMVUE\nIf \\(\\mu = 0\\), \\(\\delta(\\bar{X}) = 0\\) about half the time\n\\(\\|\\bar{X}\\| + d \\cdot \\max(0, \\|\\bar{X}\\| - d)\\) strictly dominates UMVUE"
  },
  {
    "objectID": "reader/jamesstein.html",
    "href": "reader/jamesstein.html",
    "title": "The James-Stein Estimator",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/jamesstein.html#gaussian-sequence-model",
    "href": "reader/jamesstein.html#gaussian-sequence-model",
    "title": "The James-Stein Estimator",
    "section": "1 Gaussian sequence model",
    "text": "1 Gaussian sequence model\nRecall that we have discussed a variety of estimators for \\(\\theta \\in \\RR^d\\) in the Gaussian sequence model\n\\[X \\sim N_d(\\theta, I_d)\\]\nNote that this model is somewhat more general than it appears. If \\(X_1,\\ldots,X_n \\simiid N_d(\\theta, \\sigma^2 I_d)\\) for known \\(\\sigma^2&gt; 0\\), we could make a sufficiency reduction to obtain\n\\[Z = \\frac{1}{\\sigma\\sqrt{n}} \\sum_i X_i \\sim N_d(\\theta, I_d).\\] For simplicity we will discuss the ``vanilla’’ version here, but we can always translate our results to the more general setting via this transformation.\nWe’ll generally assume in what follows that the loss we care about is the squared error loss, summed over the coordinates: \\[L(\\theta, d) =\n\\|\\delta(X) - \\theta\\|^2 = \\sum_j (\\delta_j(X) - \\theta_j)^2\\]\nThe most obvious estimator is \\(\\delta_0(X) = X\\) itself, which we could justify in a variety of ways: we’ve shown that it is the UMVU estimator for \\(\\theta\\) and also the objective Bayes estimator, since the flat prior on \\(\\theta\\) coincides with the Jeffreys prior (as it does for any location model). It also happens to be the maximum likelihood estimator (MLE), which we’ll discuss later in the course.\n\n1.1 Bayes estimators\nIf we introduce the Bayesian prior \\(\\theta_i \\simiid N(0,\\tau^2)\\) then we have seen that we arrive at the Bayes estimator \\(\\frac{\\tau^2}{1+\\tau^2}X\\).\nWe can think of this as a tuning parameter for a generic linear shrinkage estimator \\[\\delta_\\zeta(X) = (1-\\zeta)X,\\] where \\(\\zeta \\in [0,1]\\) is in effect a tuning parameter we will call the shrinkage parameter. Taking \\(\\zeta = 0\\) corresponds to using \\(X\\) as our estimator for \\(\\theta\\), and taking \\(\\zeta = 1/(1+\\tau^2)\\) corresponds to the Bayes estimator where \\(\\tau^2\\) is known.\nIf we aren’t sure which \\(\\zeta\\) to use, for example because we have some a priori uncertainty about \\(\\tau^2\\), we can try to estimate it from the data using hierarchical Bayes, which we’ve seen would give the final estimator \\[\\delta(X) = (1 - \\EE[\\zeta \\mid X]) X = \\delta_{\\hat\\zeta_{\\text{Bayes}}(X)}(X),\\] so we are in effect estimating \\(\\zeta\\) from the whole data set and then plugging it in as a data-adaptive tuning parameter.\nThe hierarchical Bayes estimator uses a Bayes estimator for \\(\\zeta\\), but if we take an empirical Bayes approach we could try other estimators, such as the MLE or UMVU. If \\(d \\geq 3\\) then the UMVU estimator for \\(\\zeta\\) is \\[\\hat{\\zeta}_{\\text{UMVU}}(X) = \\frac{d-2}{\\|X\\|^2},\\] which we can verify using the identity \\[\\EE[1/Y] = \\frac{1}{d-2}, \\quad \\text{ if } Y \\sim \\chi_d^2 = \\text{Gamma}(d/2, 2), \\text{ for } d &gt; 2,\\] which is proved in the handwritten notes. Plugging in \\(\\hat\\zeta_\\text{UMVU}\\) results in an estimator called the James-Stein estimator, \\[ \\delta_{\\text{JS}}(X)= \\left(1 - \\frac{d-2}{\\|X\\|^2}\\right)X = \\delta_{\\hat\\zeta_{\\text{UMVU}}}(X) \\]\n\n\n1.2 James-Stein Paradox\nWhile the James-Stein estimator can be motivated as an empirical Bayes estimator, it is surprisingly good even without making any Bayesian assumptions at all.\nFor \\(d \\geq 3\\), the estimator \\(X\\) is actually inadmissible as an estimator of \\(\\theta\\) under squared error loss:\n\\[\\text{MSE}(\\theta, \\delta_{JS}) &lt; \\text{MSE}(\\theta, X) \\quad \\text{ for all } \\theta \\in \\mathbb{R}^d.\\]\nIt is not surprising for a Bayes estimator to beat the UMVU estimator an average with respect to some prior, but this result holds for every fixed value of the parameter \\(\\theta\\).\nIn fact, since there is nothing special about shrinking towards \\(0\\). We could use a version of the estimator that shrinks toward any other \\(\\theta_0 \\in \\RR^d\\), i.e. \\[ \\tilde delta(X) = \\theta_0 + \\left(1 - \\frac{d-2}{\\|X - \\theta_0\\|^2}\\right) (X - \\theta_0)\\]. This also dominates \\(\\delta_0\\) because it is just the James-Stein estimator we’d get if we made the substitution \\[Y = X - \\theta_0 \\sim N_d(\\mu, I_d), \\quad \\text{ for } \\mu = \\theta - \\theta_0.\\] The translation-invariance of the Gaussian location model means that the James-Stein estimator for \\(\\mu\\) using \\(Y\\) also dominates the estimator \\(\\hat\\mu_0(Y) = Y\\), which corresponds to the estimator \\(\\delta_0(X) = \\hat\\mu_0 + \\theta_0 = X\\) for \\(\\theta\\).\nThis result was received as a shock in the 1950s when it first came out. It was regarded for a long time as a curiosity, but it was eventually understood to carry the deep implication that shrinkage makes sense, especially in higher-dimensional problems, even when we don’t have a Bayes justification for it.\n\n\n1.3 Linear shrinkage estimators\nEven without introducing a Bayesian prior for \\(\\theta\\), we can motivate our linear shrinkage estimator purely from the perspective of trading a bit of bias for a reduction in variance.\nWe can start by calculating the MSE (considered as a purely frequentist risk function) for a single coordinate, using the bias-variance tradeoff: \\[\n\\begin{aligned}\n\\EE_\\theta[(\\theta - \\delta_i(X))^2]\n&= (\\theta_i - \\EE_\\theta (1-\\zeta)X_i)^2 + \\text{Var}_\\theta (1-\\zeta)X_i\\\\\n&= (\\zeta\\theta_i)^2 + (1-\\zeta)^2\n\\end{aligned}\n\\] Summing over the \\(d\\) coordinates gives \\[\\text{MSE}(\\theta; \\delta) = \\zeta^2\\|\\theta\\|^2 + d(1-\\zeta)^2,\\] where the first term represents the squared bias and the second is the variance.\nNote that the risk is a quadratic in \\(\\zeta\\) with positive second derivative, so we can minimize it by setting \\[0 = \\frac{d}{d\\zeta}\\text{MSE}(\\theta) = 2\\zeta\\|\\theta\\|^2 - 2(1-\\zeta)d,\\] leading to \\[\\zeta^*(\\theta) = \\frac{d}{d+\\|\\theta\\|^2} = \\frac{1}{1+\\|\\theta\\|^2/d},\\] which looks remarkably similar to \\(\\frac{1}{1+\\tau^2}\\), which is the Bayes-optimal \\(\\zeta\\) under the Gaussian prior from the last section.\nOne thing to notice is that \\(\\zeta^*(\\theta) &gt; 0\\), so a small amount of shrinkage helps. But the correct amount of shrinkage depends on \\(\\|\\theta\\|^2\\): if \\(\\|\\theta\\|^2 \\to \\infty\\), the correct amount of shrinkage goes to \\(0\\), so any fixed \\(\\zeta\\) would overshoot for some \\(\\theta\\) parameters.\nIt turns out the James-Stein estimator manages to estimate the correct amount of shrinkage from the data, in such a way that we avoid overshooting most of the time, and thereby improve on the MSE for any \\(\\theta\\).\nTo understand why, we need a general way to calculate the MSE for an estimator with an adaptive \\(\\hat\\zeta(X)\\). Stein’s unbiased risk estimator will give us that."
  },
  {
    "objectID": "reader/jamesstein.html#steins-unbiased-risk-estimator",
    "href": "reader/jamesstein.html#steins-unbiased-risk-estimator",
    "title": "The James-Stein Estimator",
    "section": "2 Stein’s Unbiased Risk Estimator",
    "text": "2 Stein’s Unbiased Risk Estimator\n\n2.1 Stein’s Lemma\nThe first ingredient in finding the MSE of \\(\\delta_\\text{JS}\\) is a lemma called Stein’s Lemma:\nTheorem (Stein’s lemma, univariate): Suppose \\(X \\sim N(\\theta, \\sigma^2)\\), and that \\(h:\\; \\RR \\to \\RR\\) is differentiable, with \\(\\EE|\\dot{h}(X)| &lt; \\infty\\). Then we have \\[\\Cov(X, h(X)) = \\EE[(X-\\theta)h(X)] = \\sigma^2\\EE[\\dot{h}(X)].\\]\nProof:\nNext, we will do the calculation for \\(\\theta = 0\\) and \\(\\sigma^2 = 1\\). Then \\[\\EE[Xh(X)] = \\int_{-\\infty}^\\infty xh(x)\\phi(x)\\,dx = \\int_{-\\infty}^\\infty \\dot{h}(x)\\phi(x)\\,dx,\\] where we’ve used integration by parts: \\[\\dot{\\phi}(x) = \\frac{d}{dx} \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2} = -x \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2} = -x\\phi(x).\\] (A slightly more rigorous version is in the handwritten notes: we can assume wlog \\(h(0) = 0\\) because otherwise we could center it by using \\(k(X) = h(X) - h(0)\\), which has the same covariance with \\(X\\). Then we can break up the integral into an integral from \\(0\\) to \\(\\infty\\) and another from \\(-\\infty\\) to \\(0\\), and do integration by parts a bit more carefully for each.)\nFor more general \\(\\theta\\), we can write \\(X = \\theta + \\sigma Z\\), where \\(Z \\sim N(0,1)\\). Then applying the result for \\(k(z) = h(\\theta + \\sigma z)\\), we have\n\\[\\EE[(X-\\theta) h(X)] = \\sigma\\EE[Zh(\\theta + \\sigma Z)] = \\sigma^2\\EE[\\dot{h}(\\theta + \\sigma Z)] = \\sigma^2\\EE[\\dot{h}(X)],\\] giving the general result.\nWe will need the multivariate version of Stein’s lemma. For a function \\(h:\\; \\RR^d \\to \\RR^d\\), define the Jacobian matrix \\(Dh \\in \\RR^{d\\times d}\\) by \\[(Dh(x))_{ij} = \\frac{\\partial h_i}{\\partial x_j}(x).\\]\nDefine the Frobenius norm \\(A \\in \\RR^{d\\times d}\\) as \\[\\|A\\|_F = \\left(\\sum_{ij} A_{ij}^2\\right)^{1/2}.\\]\nNow we can state our theorem:\nTheorem (Stein’s lemma, multivariate): Assume \\(X \\sim N_d(\\theta; \\sigma^2 I_d)\\), and \\(h:\\;\\RR^d \\to \\RR^d\\) is differentiable with \\(\\EE\\|Dh(X)\\|_F &lt; \\infty\\). Then \\[ \\EE[(X-\\theta)'h(X)] = \\sigma^2 \\EE \\text{tr}(Dh(X)) = \\sigma^2 \\sum_i \\EE \\frac{\\partial h_i}{\\partial x_i} (X).\\]\nProof: The proof follows from the proof of the univariate version if we observe that the distribution of \\(X_i\\) conditional on the other coordinates \\(X_{-i}\\) is \\(N(\\theta, \\sigma^2)\\). Then we have \\[ \\EE\\left[(X_i - \\theta_i)h_i(X) \\mid X_{-i}\\right]\n= \\sigma^2 \\EE\\left[\\frac{\\partial h_i}{\\partial x_i}(X_i) \\mid X_{-i} \\right].\\] Taking expectations gives \\[ \\EE\\left[(X_i - \\theta_i)h_i(X)\\right]\n= \\sigma^2 \\EE\\left[\\frac{\\partial h_i}{\\partial x_i}(X_i) \\right],\\] and summing over \\(i\\) gives the result.\n\n\n2.2 Stein’s unbaised risk estimator (SURE)\nWe can obtain an unbiased estimator of the MSE for almost any differentiable estimator \\(\\delta(X)\\) in the Gaussian sequence model, if we apply Stein’s lemma to the function \\(h(x) = x - \\delta(x)\\). We only need the derivative of \\(h\\) to satisfy the condition of Stein’s lemma, which it does for most differentiable estimators.\nUsing the identity \\(\\|a - b\\|^2 = \\|a\\|^2 + \\|b\\|^2 - 2a'b\\), we have for \\(h(x) = x - \\delta(x)\\), \\[\n\\begin{aligned}\n\\text{MSE}(\\theta; \\delta)\n&= \\EE_\\theta\\|\\delta(X) - \\theta\\|^2\\\\\n&= \\EE_\\theta\\|X - h(X) - \\theta\\|^2\\\\\n&= \\EE_\\theta\\|X - \\theta\\|^2 + \\EE_\\theta\\|h(X)\\|^2 - 2\\EE_\\theta \\left[(X - \\theta)'h(X)\\right]\\\\\n&= \\sigma^2 d + \\EE_\\theta\\|h(X)\\|^2 - 2\\sigma^2 \\EE_\\theta \\text{tr}(Dh(X)).\n\\end{aligned}\n\\] Thus, if \\(\\sigma^2\\) is known, we obtain the unbiased estimator \\[ \\widehat{\\text{MSE}}(X) = \\sigma^2 d +  \\|h(X)\\|^2 - 2 \\sigma^2 \\text{tr}(Dh(X)).\\]\n\n\n2.3 Example: shrinking toward \\(\\overline{X}\\)\nAs an example, we can estimate the MSE of an estimator that shrinks \\(X_i\\) partway toward the average estimate across the \\(d\\) coordinates, \\(\\overline{X} = \\frac{1}{d}\\sum_i X_i\\): \\[\n\\delta_i^\\gamma(X) = (1-\\gamma) X_i  + \\gamma \\overline{X}.\n\\] We can think of this as making a bet that most of the \\(\\theta_i\\) values are close to \\(\\bar{\\theta} = \\frac{1}{d}\\sum_i \\theta_i\\). Then \\[\nh(X) = X - \\delta^\\gamma(X) = \\gamma(X - \\overline{X} 1_d),\n\\] and \\[\nDh(X)_{ii} = \\frac{\\partial}{\\partial X_i} \\gamma(X_i - \\overline{X}) = \\gamma (1-1/d) \\Rightarrow \\text{tr}(Dh(X)) = (d-1)\\gamma\n\\] An unbiased estimator for the MSE of \\(\\delta^\\gamma\\) is then \\[\n\\widehat{\\text{MSE}}^\\gamma(X) = \\sigma^2 d +  \\gamma^2(d-1)V^2 - 2(d-1) \\gamma \\sigma^2,\n\\] where \\(V^2 = \\frac{1}{d-1}\\sum_i (X_i-\\overline{X})^2\\) is the sample variance. Take note that the \\(X_i\\) values are not assumed to be i.i.d. here, since their means are generically different.\nWe can use this estimator in two different ways. The simplest way would be to calculate the actual MSE by taking the estimator’s expectation, which we know is the actual MSE of \\(\\delta^\\gamma\\). The only random variable is \\(V^2\\). Write \\(X_i = \\theta_i + Z_i\\) where \\(Z_i \\sim N(0,\\sigma^2)\\). Then we have \\[\n\\frac{1}{d-1}\\sum_i\\EE_\\theta(X_i-\\overline{X})^2 = \\frac{1}{d-1}\\sum_i (\\theta_i - \\bar{\\theta})^2 + \\frac{1}{d-1}\\sum_i \\EE(Z_i - \\overline{Z})^2 = \\beta^2 + \\sigma^2,\n\\] where \\(\\beta^2 = \\frac{1}{d-1}\\sum_i(\\theta_i - \\bar{\\theta})^2\\) is the sample variance of the \\(\\theta_i\\) values. Plugging in this expectation and collecting terms, we obtain the MSE \\[\n\\text{MSE}^\\gamma(\\theta) = \\EE_\\theta\\left[\\widehat{\\text{MSE}}^\\gamma(X)\\right] = \\sigma^2 + (d-1)(1-\\gamma)^2\\sigma^2 + (d-1)\\gamma^2\\beta^2.\n\\] We can solve for the optimal value \\(\\gamma^*(\\beta) = \\frac{\\sigma^2}{\\beta^2+\\sigma^2}\\), which unsurprisingly depends on \\(\\beta^2\\). If \\(\\beta^2 = 0\\) then all \\(\\theta_i\\) values are equal so we should set \\(\\gamma = 1\\) (shrink fully to the sample mean), but if \\(\\beta^2 \\gg \\sigma^2\\) we should take \\(\\gamma \\to 0\\) (shrink very little).\nWe could also choose \\(\\gamma\\) adaptively to minimize this estimator. That is, we could take \\[\n\\hat\\gamma(X) = \\argmin_\\gamma \\widehat{\\text{MSE}}^\\gamma(X) = \\sigma^2/V^2,\n\\] which we could think of as an estimator of \\(\\gamma^*(X)\\) since \\(V^2\\) is unbiased for \\(\\beta^2+\\sigma^2\\). If we plug in \\(\\hat\\gamma(X)\\) we get a new adaptive shrinkage estimator which is not the same as \\(\\delta^\\gamma\\) for any fixed \\(\\gamma\\), and we could use the same idea to calculate its MSE, if we wanted to."
  },
  {
    "objectID": "reader/jamesstein.html#risk-of-the-james-stein-estimator",
    "href": "reader/jamesstein.html#risk-of-the-james-stein-estimator",
    "title": "The James-Stein Estimator",
    "section": "3 Risk of the James-Stein estimator",
    "text": "3 Risk of the James-Stein estimator\nWe are now ready to calculate the risk of the James–Stein estimator \\(\\delta_{JS}(X) = \\left(1 - \\frac{d-2}{\\|X\\|^2}\\right)X\\). We can drop the assumption $^2 = $, under the assumption \\(\\sigma^2 = 1\\) (for general \\(\\sigma^2\\), we should replace the numerator \\(d-2\\) with \\((d-2)\\sigma^2\\). Then ).\nProceeding as before, we have \\[\nh(X) = \\frac{d-2}{\\|X\\|^2}X \\Rightarrow \\|h(X)\\|^2 = \\frac{(d-2)^2}{\\|X\\|^2},\n\\] Applying the quotient rule we have \\[Dh(X)_{ii} = (d-2)\\frac{\\partial}{\\partial X_i} \\frac{X_i}{\\sum_j X_j^2} = (d-2)\\frac{\\|X\\|^2 - 2X_i^2}{\\|X\\|^4},\\] and summing over the coordinates gives \\[\\text{tr}(Dh(X)) = (d-2) \\frac{d\\|X\\|^2 - 2\\|X\\|^2}{\\|X\\|^4} = -\\frac{(d-2)^2}{\\|X\\|^2}.\\] We thereby obtain the estimator \\[\n\\widehat{\\text{MSE}}(X) = d + \\frac{(d-2)^2}{\\|X\\|^2} - 2\\frac{(d-2)^2}{\\|X\\|^2} = d - \\frac{(d-2)^2}{\\|X\\|^2}.\n\\] Taking expectations, we obtain \\[\n\\text{MSE}(\\theta; \\delta_{\\text{JS}}) = d - (d-2)^2\\EE_\\theta\\left[\\frac{1}{\\|X\\|^2}\\right].\n\\] Note this is always strictly less than \\(d\\), which is the MSE of \\(\\delta_0(X) = X\\).\nIf \\(\\theta = 0\\) then \\(\\|X\\|^2 \\sim \\chi_d^2\\) and we can apply our previous result to obtain \\[\n\\text{MSE}(0; \\delta_{\\text{JS}}) = d - (d-2)^2\\frac{1}{d-2} = 2.\n\\] Thus, even though we are estimating \\(d\\) parameters, our total MSE does not rise with \\(d\\), because we will shrink harder and harder toward zero the larger \\(d\\) gets. This is fairly remarkable.\nOn the other hand, suppose \\(\\|\\theta\\|^2 \\to \\infty\\). Then \\(\\EE_\\theta \\|X\\|^2 \\to \\infty\\) and the improvement \\((d-2)^2/\\EE_\\theta\\|X\\|^2\\) will be driven to \\(0\\).\nNote that, for more general \\(\\sigma^2\\), the James–Stein estimator is \\(\\left(1-\\frac{(d-2)\\sigma^2}{\\|X\\|^2}\\right)X\\). Then we have \\[h(X) = \\sigma^2\\frac{d-2}{\\|X\\|^2} \\Rightarrow \\|h(X)\\|^2 = \\sigma^4 \\frac{(d-2)^2}{\\|X\\|^2}, \\quad \\text{tr} Dh(X) = \\sigma^2 \\frac{(d-2)^2}{\\|X\\|^2},\\] leading to the estimator \\(\\widehat{\\text{MSE}}(X) = \\sigma^2 d - \\sigma^4\\frac{(d-2)^2}{\\|X\\|^2}\\), and plugging in \\(\\EE_0 1/\\|X\\|^2 = 1/(d-2)\\sigma^2\\), the MSE at \\(\\theta=0\\) is \\(2\\sigma^2\\).\n\n3.1 Final thoughts\nA few more notes: first, \\(\\delta_{JS}(X)\\) also inadmissible, since \\(\\delta_{+}(X) = (1 - \\frac{d-2}{\\|X\\|^2})_+ X\\) is strictly better since we never benefit from using a shrinkage parameter \\(\\zeta &gt; 1\\).\nA practically more useful version of James–Stein shrinks toward the central value \\(\\overline{X}\\):\n\\[\\delta_{JS+, i}(X) = \\overline{X} + (1 - \\frac{d-3}{V^2})_+ (X_i - \\overline{X})\\] This estimator dominates \\(\\delta(X) = X\\) for \\(d \\geq 4\\).\nTaken to its logical extreme, the James–Stein estimator seems to imply absurd things: should all scientists at Berkeley, across all different fields, pool their estimates to calculate a James–Stein estimator even if their different estimation problems have nothing to do with each other, as long as they are estimating Gaussian location parameters? The overall MSE for all of the estimates really would be better.\nTo avoid this conclusion we might note that even when the overall MSE is improved, the MSE for a single coordinate can certainly get worse. For example, if \\(\\theta_1 = 10\\) but \\(\\theta_2=\\theta_3=\\cdots=\\theta_{1000}=0\\), it’s likely that we’ll overshrink our estimate of \\(\\theta_1\\) toward \\(0\\) in order to do well on the other coordinates. So people who believe their estimands are larger than average wouldn’t want to participate in this scheme."
  },
  {
    "objectID": "reader/multiple-testing.html",
    "href": "reader/multiple-testing.html",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/multiple-testing.html#multiple-testing",
    "href": "reader/multiple-testing.html#multiple-testing",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "1 Multiple Testing",
    "text": "1 Multiple Testing\nIn many testing problems, we want to test many hypotheses at a time, e.g.:\n\nTest \\(H_0: \\beta_j = 0\\) for \\(j = 1,\\ldots,d\\) in linear regression\nTest whether each of 20K single nucleotide polymorphisms (SNPs) is associated with a given phenotype (e.g., diabetes, schizophrenia)\nTest whether each of 2000 website tweaks affect user engagement\n\n\n1.1 Setup\n\\(X \\sim P_\\theta \\in \\cP\\), \\(H_{0i}: \\theta \\in \\Theta_i\\), \\(i = 1,\\ldots,m\\)\nCommonly, \\(H_{0i}: \\theta_i = 0\\)\nGoal: Return accept/reject decision for each \\(i\\)\nLet \\(R = \\{i: H_{0i} \\text{ rejected}\\}\\), \\(|R| \\leq m\\)\n\\(H_{0c} = \\{i: H_{0i} \\text{ true}\\}\\), \\(|H_{0c}| = m_0 \\leq m\\)\n\\(R \\cap H_{0c}\\) = false rejections"
  },
  {
    "objectID": "reader/multiple-testing.html#family-wise-error-rate-fwer",
    "href": "reader/multiple-testing.html#family-wise-error-rate-fwer",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "2 Family-wise Error Rate (FWER)",
    "text": "2 Family-wise Error Rate (FWER)\nProblem: Even if all \\(H_{0i}\\) true, might have:\n\\(\\mathbb{P}(\\text{any } H_{0i} \\text{ rejected}) \\leq 1 - (1-\\alpha)^m \\approx m\\alpha\\)\nExample: \\(X_i \\sim N(\\theta_i, 1)\\) iid, \\(i = 1,\\ldots,m\\), \\(H_{0i}: \\theta_i = 0\\)\n\\(\\mathbb{P}_0(\\text{any } H_{0i} \\text{ rejected}) = 1 - (1-\\alpha)^m \\approx m\\alpha\\)\nIs this a problem? Yes, if all attention will be focused on the false rejections and none on the correct non-rejections.\nClassical solution is to control the family-wise error rate (FWER):\nFWER = \\(\\mathbb{P}_\\theta(\\text{any false rejections}) = \\mathbb{P}_\\theta(R \\cap H_{0c} \\neq \\emptyset)\\)\nWant: \\(\\sup_\\theta \\text{FWER}(\\theta) \\leq \\alpha\\)\nTypically achieved by correcting marginal p-values: \\(p_1(X), \\ldots, p_m(X)\\), \\(p_i \\sim U(0,1)\\)\ne.g., \\(\\phi_i = 1\\{\\alpha/(2m)|X_i| &gt; \\Phi^{-1}(1-\\alpha/(2m))\\}\\) for Gaussian"
  },
  {
    "objectID": "reader/multiple-testing.html#bonferroni-correction",
    "href": "reader/multiple-testing.html#bonferroni-correction",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "3 Bonferroni Correction",
    "text": "3 Bonferroni Correction\nAssume \\(p_1,\\ldots,p_m\\) are p-values for \\(H_{01},\\ldots,H_{0m}\\) with \\(p_i \\sim U(0,1)\\) under \\(H_{0i}\\)\nFor general dependence, can guarantee control by rejecting \\(H_{0i}\\) iff \\(p_i \\leq \\alpha/m\\):\n\\(\\mathbb{P}_\\theta(\\text{any false rejections}) \\leq \\mathbb{P}_\\theta(\\text{any } H_{0i} \\text{ rejected}) \\leq \\sum_{i \\in H_{0c}} \\mathbb{P}_\\theta(H_{0i} \\text{ rejected}) \\leq m_0\\alpha/m \\leq \\alpha\\)\nIf p-values independent, can improve to \\(1-(1-\\alpha)^{1/m}\\) (Šidák correction)\nThen \\(\\mathbb{P}_\\theta(\\text{no false rejections}) = \\prod_{i \\in H_{0c}} \\mathbb{P}_\\theta(p_i &gt; (1-(1-\\alpha)^{1/m})) \\geq (1-\\alpha)^{m_0/m} \\geq 1-\\alpha\\)\nFor small \\(\\alpha\\): \\(1-(1-\\alpha)^{1/m} \\approx \\alpha/m\\)\nŠidák doesn’t improve much on Bonferroni"
  },
  {
    "objectID": "reader/multiple-testing.html#testing-with-dependence",
    "href": "reader/multiple-testing.html#testing-with-dependence",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "4 Testing with Dependence",
    "text": "4 Testing with Dependence\nBonferroni isn’t much worse than Šidák e.g., \\(\\alpha = 0.05\\), \\(m = 20\\): \\(0.0025\\) vs \\(0.00256\\)\nBut when tests are highly dependent, can often do much better\n\n4.1 Example: Scheffé’s S-method\n\\(X \\sim N(\\theta, I_d)\\), \\(\\theta \\in \\mathbb{R}^d\\) \\(H_0: a_j^T \\theta = 0\\) for \\(j = 1,\\ldots,m\\), \\(\\|a_j\\| = 1\\)\nReject \\(H_{0j}\\) if \\(|a_j^T X| &gt; \\sqrt{d F_{d,\\infty,1-\\alpha}}\\)\nControls FWER:\n\\(\\mathbb{P}(\\|X - \\theta\\|^2 \\leq dF_{d,\\infty,1-\\alpha}) = 1-\\alpha\\)\nCan view as deduction from confidence region: \\(C(X) = \\{\\theta: \\|X - \\theta\\|^2 \\leq dF_{d,\\infty,1-\\alpha}\\}\\)"
  },
  {
    "objectID": "reader/multiple-testing.html#deduced-inference",
    "href": "reader/multiple-testing.html#deduced-inference",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "5 Deduced Inference",
    "text": "5 Deduced Inference\nGiven any joint confidence region \\(C(X)\\) for \\(\\theta \\in \\Theta\\), we may freely assume \\(\\theta \\in C(X)\\) and deduce any and all implied conclusions without any FWER inflation:\n\\(\\mathbb{P}_\\theta(\\text{any deduced inference is wrong}) \\leq \\mathbb{P}_\\theta(\\theta \\notin C(X)) \\leq \\alpha\\)\nDeduction is often a good paradigm for deriving simultaneous intervals\nWe say \\(C_1(X),\\ldots,C_m(X)\\) are simultaneous \\(1-\\alpha\\) confidence intervals for \\(g_1(\\theta),\\ldots,g_m(\\theta)\\) if:\n\\(\\mathbb{P}_\\theta(g_i(\\theta) \\in C_i(X) \\text{ for all } i = 1,\\ldots,m) \\geq 1-\\alpha\\)\n\n5.1 Example: Simultaneous Intervals for Multivariate Gaussian\nAssume \\(X \\sim N_d(\\theta, \\Sigma)\\), \\(\\Sigma\\) known, \\(\\Sigma_{ii} = 1\\)\nLet \\(t_\\alpha\\) be upper \\(\\alpha\\) quantile of \\(\\|X - \\theta\\|_\\Sigma = \\sqrt{(X-\\theta)^T \\Sigma^{-1}(X-\\theta)}\\)\n\\(C_i(X) = [\\theta_i: |X_i - \\theta_i| \\leq t_\\alpha \\sqrt{\\Sigma_{ii}}]\\) for all \\(i\\)\n\\(\\mathbb{P}(C(X) \\ni \\theta_i \\text{ for any } i) = \\mathbb{P}(\\|X - \\theta\\|_\\Sigma \\leq t_\\alpha) = 1-\\alpha\\)\n\\(t_\\alpha = \\sqrt{\\chi^2_{d,1-\\alpha}}\\) if \\(\\Sigma = I_d\\)\nNote: we could have instead constructed an elliptical conf. region, but then the intervals would be conservative:\n\\(\\mathbb{P}(\\|X - \\theta\\|_\\Sigma^2 \\leq \\chi^2_{d,1-\\alpha}) = 1-\\alpha\\)\n\n\n5.2 Example: Linear Regression (n obs, d variables)\n\\(X \\in \\mathbb{R}^{n \\times d}\\) design, \\(\\beta \\in \\mathbb{R}^d\\), \\(Y \\sim N(X\\beta, \\sigma^2 I_n)\\)\nEstimate \\(\\hat{\\beta} = (X^T X)^{-1} X^T Y\\)\nwhere \\(\\hat{\\beta} \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\)\n\\(S^2 = \\|Y - X\\hat{\\beta}\\|^2/(n-d)\\), \\(V = RS^2\\), \\(R = (X^T X)^{-1}\\)\nDistr. of \\(\\hat{\\beta}_j/\\sqrt{V_{jj}}\\) fully known\nAssume w.l.o.g. \\(X^T X = I_d\\)\nLet \\(t_\\alpha\\) denote upper \\(\\alpha\\) quantile of \\(\\|\\hat{\\beta} - \\beta\\|/\\sqrt{S^2}\\)\nThen \\(C_j = \\hat{\\beta}_j \\pm t_\\alpha \\sqrt{V_{jj}}\\) are simultaneous CIs for \\(\\beta_j\\), \\(j = 1,\\ldots,d\\) (compute \\(t_\\alpha\\) by simulation)\n\\(\\mathbb{P}(|\\hat{\\beta}_j - \\beta_j| \\leq t_\\alpha \\sqrt{V_{jj}} \\text{ for all } j) = 1-\\alpha\\)"
  },
  {
    "objectID": "reader/multiple-testing.html#false-discovery-rate-fdr",
    "href": "reader/multiple-testing.html#false-discovery-rate-fdr",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "6 False Discovery Rate (FDR)",
    "text": "6 False Discovery Rate (FDR)\nProblem: With 10K independent test statistics, all at level \\(\\alpha = 0.001\\), we expect 10 rejections just by chance. What if we get 50? Probably only ~20 of them are false rejections.\nCan we accept 10 false rejections as long as most rejections are valid?\nBenjamini-Hochberg (1995) proposed a more liberal error control criterion called FDR:\n\\(R(X) = |R(X)|\\) = rejections (“discoveries”) \\(V(X) = |R(X) \\cap H_{0c}|\\) = false discoveries\nThe FDP is: \\(\\text{FDP} = \\begin{cases} V(X)/R(X) & \\text{if } R(X) &gt; 0 \\\\ 0 & \\text{if } R(X) = 0 \\end{cases}\\)\nThe FDR is \\(\\mathbb{E}[\\text{FDP}]\\)"
  },
  {
    "objectID": "reader/multiple-testing.html#benjamini-hochberg-procedure",
    "href": "reader/multiple-testing.html#benjamini-hochberg-procedure",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "7 Benjamini-Hochberg Procedure",
    "text": "7 Benjamini-Hochberg Procedure\nB-H also proposed a method to control FDR given ordered p-values \\(p_{(1)} \\leq p_{(2)} \\leq \\cdots \\leq p_{(m)}\\):\n\\(R(X) = \\max\\{r: p_{(r)} \\leq \\alpha r/m\\}\\) (called step-up procedure)\nReject \\(H_{0(1)},\\ldots,H_{0(R)}\\)\nThis is much more liberal than Bonferroni procedure: When \\(\\alpha = 0.05\\), B-H rejects at least \\(r\\) p-values if \\(p_{(r)} \\leq 0.05r/m\\)\n\n7.1 B-H as Empirical Bayes\nEquivalent formulation for \\(R(t) = \\#\\{p_i \\leq t\\}\\): Let \\(\\hat{F}(t) = R(t)/m\\) = estimate of CDF of p-values\nB-H rejects \\(H_i\\) if \\(p_i \\leq T(X) = \\max\\{t: \\hat{F}(t) \\geq t/\\alpha\\}\\)\nWhen \\(\\hat{F}(t)\\) is continuously increasing in \\(t\\) except at jump values where it jumps down:\n\\(\\hat{F}(T(X)) = T(X)/\\alpha\\)\n[Insert graph showing \\(\\hat{F}(t)\\) vs \\(t/\\alpha\\)]\nOnly values of \\(t\\) that matter for the algorithm are \\(t = p_i\\) where \\(\\hat{F}(t) = t/\\alpha\\), i.e., \\(\\alpha i/m = p_{(i)}\\)"
  },
  {
    "objectID": "reader/multiple-testing.html#fdr-control",
    "href": "reader/multiple-testing.html#fdr-control",
    "title": "Multiple Testing: FWER, FDR, and Related Procedures",
    "section": "8 FDR Control",
    "text": "8 FDR Control\nElegant but fragile proof due to Storey, Taylor, Siegmund (2002)\nAssume \\(\\#\\{i: H_{0i} \\text{ true}\\} = m_0\\) indep. \\(p_i \\sim U[0,1]\\) under \\(H_{0i}\\)\nLet \\(V(t) = \\#\\{i \\in H_{0c}: p_i \\leq t\\}\\)\n\\(\\text{FDR} = \\mathbb{E}[\\text{FDP}] = \\mathbb{E}\\left[\\frac{V(T)}{R(T)} \\cdot 1\\{R(T) &gt; 0\\}\\right]\\)\nThen FDR \\(= \\mathbb{E}[\\text{FDP}] = \\mathbb{E}[\\mathbb{E}[\\text{FDP} | T]] \\leq \\mathbb{E}[m_0T/(mT)] = \\alpha m_0/m \\leq \\alpha\\)\nNote: \\(Q(t) = V(t)/(mt)\\) is a martingale when \\(t\\) runs backwards from \\(t=1\\) to \\(t=0\\)\n\\(\\mathbb{E}[V(s) | V(t)] = V(t) \\cdot s/t\\)\n\\(\\mathbb{E}[1\\{p_i \\leq s\\} | 1\\{p_i \\leq t\\}, V(t)] = s/t \\cdot 1\\{p_i \\leq t\\}\\)\nAnd \\(T\\) is a stopping time w.r.t. the filtration \\(\\mathcal{F}_t\\) of \\(\\{V(t), R(t)\\}\\) (again, filtration with \\(t=1 \\to t=0\\))\nWhy? For \\(s&lt;t\\), \\(R(s) = \\#\\{i: p_i \\leq s\\}\\)\n\\(\\mathbb{E}[1\\{p_i \\leq s\\} | \\mathcal{F}_t] = s/t \\cdot 1\\{p_i \\leq t\\}\\)\n\\(\\hat{F}(s) = R(s)/m\\)\n[Insert graph showing \\(\\hat{F}(t)\\) vs \\(t/\\alpha\\) and \\(Q(t)\\)]\n\\(\\text{FDR} = \\alpha \\mathbb{E}[V(T)/(mT)] = \\alpha \\mathbb{E}[Q(T)] = \\alpha \\mathbb{E}[Q(1)] = \\alpha m_0/m\\)\n\n8.1 Remarks\n\nProof only works if p-values indep., null ones exactly uniform\nMore robust proof shows FDR controlled when null p-values conservative\nCan be extended to positive dependence\nFDR controlled under general dependence if we use corrected level \\(\\alpha m / (m+1-i)\\)\n\\(\\sum_{i=1}^m 1/i \\approx \\log m + 0.577\\)"
  },
  {
    "objectID": "reader/empirical-bayes.html",
    "href": "reader/empirical-bayes.html",
    "title": "The James-Stein Estimator",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/empirical-bayes.html#gaussian-sequence-model",
    "href": "reader/empirical-bayes.html#gaussian-sequence-model",
    "title": "The James-Stein Estimator",
    "section": "1 Gaussian sequence model",
    "text": "1 Gaussian sequence model\nRecall that we have discussed a variety of estimators for \\(\\theta \\in \\RR^d\\) in the Gaussian sequence model\n\\[X \\sim N_d(\\theta, I_d)\\]\nNote that this model is somewhat more general than it appears. If \\(X_1,\\ldots,X_n \\simiid N_d(\\theta, \\sigma^2 I_d)\\) for known \\(\\sigma^2&gt; 0\\), we could make a sufficiency reduction to obtain\n\\[Z = \\frac{1}{\\sigma\\sqrt{n}} \\sum\\_i X_i \\sim N_d(\\theta, I_d).\\] For simplicity we will discuss the ``vanilla’’ version here, but we can always translate our results to the more general setting via this transformation.\nWe’ll generally assume in what follows that the loss we care about is the squared error loss, summed over the coordinates:\n\\[L(\\theta, d) =\n\\|\\delta(X) - \\theta\\|\\^2 = \\sum\\_j (\\delta\\_j(X) - \\theta\\_j)\\^2\\]\nThe most obvious estimator is \\(\\delta_0(X) = X\\) itself, which we could justify in a variety of ways: we’ve shown that it is the UMVU estimator for \\(\\theta\\) and also the objective Bayes estimator, since the flat prior on \\(\\theta\\) coincides with the Jeffreys prior (as it does for any location model). It also happens to be the maximum likelihood estimator (MLE), which we’ll discuss later in the course.\n\n1.1 Bayes estimators\nIf we introduce the Bayesian prior \\(\\theta_i \\simiid N(0,\\tau^2)\\) then we have seen that we arrive at the Bayes estimator \\(\\frac{\\tau^2}{1+\\tau^2}X\\).\nWe can think of this as a tuning parameter for a generic linear shrinkage estimator \\[\\delta_\\zeta(X) = (1-\\zeta)X,\\] where \\(\\zeta \\in [0,1]\\) is in effect a tuning parameter we will call the shrinkage parameter. Taking \\(\\zeta = 0\\) corresponds to using \\(X\\) as our estimator for \\(\\theta\\), and taking \\(\\zeta = 1/(1+\\tau^2)\\) corresponds to the Bayes estimator where \\(\\tau^2\\) is known.\nIf we aren’t sure which \\(\\zeta\\) to use, for example because we have some a priori uncertainty about \\(\\tau^2\\), we can try to estimate it from the data using hierarchical Bayes, which we’ve seen would give the final estimator \\[\\delta(X) = (1 - \\EE[\\zeta \\mid X]) X = \\delta_{\\hat\\zeta_{\\text{Bayes}}(X)}(X),\\] so we are in effect estimating \\(\\zeta\\) from the whole data set and then plugging it in as a data-adaptive tuning parameter.\nThe hierarchical Bayes estimator uses a Bayes estimator for \\(\\zeta\\), but if we take an empirical Bayes approach we could try other estimators, such as the MLE or UMVU. If \\(d \\geq 3\\) then the UMVU estimator for \\(\\zeta\\) is \\[\\hat{\\zeta}_{\\text{UMVU}}(X) = \\frac{d-2}{\\|X\\|^2},\\] which we can verify using the identity \\[\\EE[1/Y] = \\frac{1}{d-2}, \\quad \\text{ if } Y \\sim \\chi_d^2 = \\text{Gamma}(d/2, 2), \\text{ for } d &gt; 2,\\] which is proved in the handwritten notes. Plugging in \\(\\hat\\zeta_\\text{UMVU}\\) results in an estimator called the James-Stein estimator, \\[ \\delta_{\\text{JS}}(X)= \\left(1 - \\frac{d-2}{\\|X\\|^2}\\right)X = \\delta_{\\hat\\zeta_{\\text{UMVU}}}(X) \\]\n\n\n1.2 James-Stein Paradox\nWhile the James-Stein estimator can be motivated as an empirical Bayes estimator, it is surprisingly good even without making any Bayesian assumptions at all.\nFor \\(d \\geq 3\\), the estimator \\(X\\) is actually inadmissible as an estimator of \\(\\theta\\) under squared error loss:\n\\[\\text{MSE}(\\theta, \\delta_{JS}) &lt; \\text{MSE}(\\theta, X) \\quad \\text{ for all } \\theta \\in \\mathbb{R}^d.\\]\nIt is not surprising for a Bayes estimator to beat the UMVU estimator an average with respect to some prior, but this result holds for every fixed value of the parameter \\(\\theta\\).\nIn fact, since there is nothing special about shrinking towards \\(0\\). We could use a version of the estimator that shrinks toward any other \\(\\theta_0 \\in \\RR^d\\), i.e. \\[ \\tilde delta(X) = \\theta_0 + \\left(1 - \\frac{d-2}{\\|X - \\theta_0\\|^2}\\right) (X - \\theta_0)\\]. This also dominates \\(\\delta_0\\) because it is just the James-Stein estimator we’d get if we made the substitution \\[Y = X - \\theta_0 \\sim N_d(\\mu, I_d), \\quad \\text{ for } \\mu = \\theta - \\theta_0.\\] The translation-invariance of the Gaussian location model means that the James-Stein estimator for \\(\\mu\\) using \\(Y\\) also dominates the estimator \\(\\hat\\mu_0(Y) = Y\\), which corresponds to the estimator \\(\\delta_0(X) = \\hat\\mu_0 + \\theta_0 = X\\) for \\(\\theta\\).\nThis result was received as a shock in the 1950s when it first came out. It was regarded for a long time as a curiosity, but it was eventually understood to carry the deep implication that shrinkage makes sense, especially in higher-dimensional problems, even when we don’t have a Bayes justification for it.\n\n\n1.3 Linear shrinkage estimators\nEven without introducing a Bayesian prior for \\(\\theta\\), we can motivate our linear shrinkage estimator purely from the perspective of trading a bit of bias for a reduction in variance.\nWe can start by calculating the MSE (considered as a purely frequentist risk function) for a single coordinate, using the bias-variance tradeoff: \\[\n\\begin{aligned}\n\\EE_\\theta[(\\theta - \\delta_i(X))^2]\n&= (\\theta_i - \\EE_\\theta (1-\\zeta)X_i)^2 + \\text{Var}_\\theta (1-\\zeta)X_i\\\\\n&= (\\zeta\\theta_i)^2 + (1-\\zeta)^2\n\\end{aligned}\n\\] Summing over the \\(d\\) coordinates gives \\[\\text{MSE}(\\theta; \\delta) = \\zeta^2\\|\\theta\\|^2 + d(1-\\zeta)^2,\\] where the first term represents the squared bias and the second is the variance.\nNote that the risk is a quadratic in \\(\\zeta\\) with positive second derivative, so we can minimize it by setting \\[0 = \\frac{d}{d\\zeta}\\text{MSE}(\\theta) = 2\\zeta\\|\\theta\\|^2 - 2(1-\\zeta)d,\\] leading to \\[\\zeta^*(\\theta) = \\frac{d}{d+\\|\\theta\\|^2} = \\frac{1}{1+\\|\\theta\\|^2/d},\\] which looks remarkably similar to \\(\\frac{1}{1+\\tau^2}\\), which is the Bayes-optimal \\(\\zeta\\) under the Gaussian prior from the last section.\nOne thing to notice is that \\(\\zeta^*(\\theta) &gt; 0\\), so a small amount of shrinkage helps. But the correct amount of shrinkage depends on \\(\\|\\theta\\|^2\\): if \\(\\|\\theta\\|^2 \\to \\infty\\), the correct amount of shrinkage goes to \\(0\\), so any fixed \\(\\zeta\\) would overshoot for some \\(\\theta\\) parameters.\nIt turns out the James-Stein estimator manages to estimate the correct amount of shrinkage from the data, in such a way that we avoid overshooting most of the time, and thereby improve on the MSE for any \\(\\theta\\).\nTo understand why, we need a general way to calculate the MSE for an estimator with an adaptive \\(\\hat\\zeta(X)\\). Stein’s unbiased risk estimator will give us that."
  },
  {
    "objectID": "reader/empirical-bayes.html#steins-unbiased-risk-estimator",
    "href": "reader/empirical-bayes.html#steins-unbiased-risk-estimator",
    "title": "The James-Stein Estimator",
    "section": "2 Stein’s Unbiased Risk Estimator",
    "text": "2 Stein’s Unbiased Risk Estimator\n\n2.1 Stein’s Lemma\nThe first ingredient in finding the MSE of \\(\\delta_\\text{JS}\\) is a lemma called Stein’s Lemma:\nTheorem (Stein’s lemma, univariate): Suppose \\(X \\sim N(\\theta, \\sigma^2)\\), and that \\(h:\\; \\RR \\to \\RR\\) is differentiable, with \\(\\EE|\\dot{h}(X)| &lt; \\infty\\). Then we have \\[\\Cov(X, h(X)) = \\EE[(X-\\theta)h(X)] = \\sigma^2\\EE[\\dot{h}(X)].\\]\nProof:\nNext, we will do the calculation for \\(\\theta = 0\\) and \\(\\sigma^2 = 1\\). Then \\[\\EE[Xh(X)] = \\int_{-\\infty}^\\infty xh(x)\\phi(x)\\,dx = \\int_{-\\infty}^\\infty \\dot{h}(x)\\phi(x)\\,dx,\\] where we’ve used integration by parts: \\[\\dot{\\phi}(x) = \\frac{d}{dx} \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2} = -x \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2} = -x\\phi(x).\\] (A slightly more rigorous version is in the handwritten notes: we can assume wlog \\(h(0) = 0\\) because otherwise we could center it by using \\(k(X) = h(X) - h(0)\\), which has the same covariance with \\(X\\). Then we can break up the integral into an integral from \\(0\\) to \\(\\infty\\) and another from \\(-\\infty\\) to \\(0\\), and do integration by parts a bit more carefully for each.)\nFor more general \\(\\theta\\), we can write \\(X = \\theta + \\sigma Z\\), where \\(Z \\sim N(0,1)\\). Then applying the result for \\(k(z) = h(\\theta + \\sigma z)\\), we have\n\\[\\EE[(X-\\theta) h(X)] = \\sigma\\EE[Zh(\\theta + \\sigma Z)] = \\sigma^2\\EE[\\dot{h}(\\theta + \\sigma Z)] = \\sigma^2\\EE[\\dot{h}(X)],\\] giving the general result.\nWe will need the multivariate version of Stein’s lemma. For a function \\(h:\\; \\RR^d \\to \\RR^d\\), define the Jacobian matrix \\(Dh \\in \\RR^{d\\times d}\\) by \\[(Dh(x))_{ij} = \\frac{\\partial h_i}{\\partial x_j}(x).\\]\nDefine the Frobenius norm \\(A \\in \\RR^{d\\times d}\\) as \\[\\|A\\|_F = (\\sum_{ij} A_{ij}^2)^{1/2}.\\]\nNow we can state our theorem:\nTheorem (Stein’s lemma, multivariate): Assume \\(X \\sim N_d(\\theta; \\sigma^2 I_d)\\), and \\(h:\\;\\RR^d \\to \\RR^d\\) is differentiable with \\(\\EE\\|Dh(X)\\|_F &lt; \\infty\\). Then \\[ \\EE[(X-\\theta)'h(X)] = \\sigma^2 \\EE \\text{tr}(Dh(X)) = \\sigma^2 \\sum_i \\EE \\frac{\\partial h_i}{\\partial x_i} (X).\\]\nThe proof follows easily from the proof of the univariate version, and appears in the handwritten notes.\n\n\n2.2 Stein’s unbaised risk estimator (SURE)\n\n\n2.3 Risk of James-Stein"
  },
  {
    "objectID": "reader/empirical-bayes.html#empirical-bayes",
    "href": "reader/empirical-bayes.html#empirical-bayes",
    "title": "The James-Stein Estimator",
    "section": "3 Empirical Bayes",
    "text": "3 Empirical Bayes\n\n3.1 \n\n\n3.2 Common Situation in Hierarchical Bayes Models\n\n\\(\\theta \\sim G\\), one draw: hard to justify prior\nLots of info: prior doesn’t matter\n\\(\\theta_i \\sim G\\), only \\(X_i\\) informative: prior helps\nMany draws: can check fit\n\n\\[X_i \\sim p_{\\theta_i}(x), \\quad i = 1,\\ldots,d\\]\n\n\n3.3 Hybrid Approach\nTreat \\(G\\) as fixed:\n\nEstimate \\(G\\) based on observed data\nPlug in \\(\\hat{G}\\) as though known\n\n\n\n3.4 Example\n\\(\\theta_i \\sim N(0, \\tau^2)\\), \\(\\tau^2\\) fixed unknown \\(X_i|\\theta_i \\sim N(\\theta_i, 1)\\), \\(i = 1,\\ldots,d\\)\nBayes estimator if we knew \\(\\tau^2\\) is:\n\\[\\delta(X) = \\frac{\\tau^2}{\\tau^2 + 1}X_i\\]\n\\(\\tau^2\\) is sufficient.\nTo estimate \\(\\tau^2\\), use \\(X \\sim N(0, \\tau^2 I_d + I_d)\\):\n\\[\\mathbb{E}\\|X\\|^2 = d(\\tau^2 + 1)\\]\n\\[\\hat{\\tau}^2 = \\max\\{\\frac{1}{d}\\|X\\|^2 - 1, 0\\}\\]\nPlug in: \\(\\hat{\\delta}(X) = (1 - \\frac{d}{\\|X\\|^2})_+ X_i\\)\nIf \\(d\\) large, should be near optimal."
  },
  {
    "objectID": "reader/empirical-bayes.html#james-stein-estimator",
    "href": "reader/empirical-bayes.html#james-stein-estimator",
    "title": "The James-Stein Estimator",
    "section": "4 James-Stein Estimator",
    "text": "4 James-Stein Estimator\n\\[\\delta_{JS}(X) = (1 - \\frac{d-2}{\\|X\\|^2})X\\]\nProof: If \\(Y \\sim \\text{Gamma}(\\frac{d}{2}, \\frac{1}{2})\\), then:\n\\[\\mathbb{P}(Y &gt; \\frac{d-2}{2}) = 1\\]\nNow use \\(f(x) = x - \\frac{d-2}{x}\\), \\(\\frac{1}{2}\\|X\\|^2 \\sim \\text{Gamma}(\\frac{d}{2}, \\frac{1}{2})\\)\n\\[\\mathbb{E}[f(\\frac{1}{2}\\|X\\|^2)] &gt; f(\\mathbb{E}[\\frac{1}{2}\\|X\\|^2]) = \\frac{d}{2} - \\frac{d-2}{\\frac{d}{2}} = 1\\]\n\n4.1 James-Stein Paradox\nFor \\(d \\geq 3\\), the sample mean \\(\\bar{X} = \\frac{1}{n}\\sum X_i\\) is inadmissible as an estimator of \\(\\theta\\) under squared error loss.\nFor \\(\\delta_{JS}(X) = (1 - \\frac{d-2}{\\|X\\|^2})X\\):\n\\[\\text{MSE}(\\theta, \\delta_{JS}) &lt; \\text{MSE}(\\theta, \\bar{X}) \\quad \\forall \\theta \\in \\mathbb{R}^d\\]\nNotes: - \\(\\bar{X}\\) is UMVU, Minimax, objective Bayes - Might as well take \\(n=1\\) (Sufficiency reduction) - This result holds without assumption of Bayes model on \\(\\theta\\): true for \\(\\theta = (50, 10, 94, \\ldots)\\) - Nothing special about 0: for any \\(\\theta_0 \\in \\mathbb{R}^d\\), \\(\\delta(X) = \\theta_0 + (1 - \\frac{d-2}{\\|X-\\theta_0\\|^2})(X-\\theta_0)\\) also dominates \\(X\\)\nDeep implication: shrinkage makes sense even without Bayes justification.\n\n\n4.2 General Form\nLet \\(\\delta(X) = (1 - \\frac{c}{\\|X\\|^2})X\\), \\(c\\) is tuning parameter\n\\[R(\\theta, \\delta) = \\mathbb{E}_\\theta\\|\\delta(X) - \\theta\\|^2 = \\mathbb{E}_\\theta\\|X - \\theta\\|^2 + \\mathbb{E}_\\theta[\\frac{c^2}{\\|X\\|^2} - 2c]\\]\nWhat is optimal \\(c\\)?\n\\[R(\\theta, \\delta) = d + \\mathbb{E}_\\theta[\\frac{c^2}{\\|X\\|^2}] - 2c\\]\n\\[\\frac{\\partial R}{\\partial c} = 2\\mathbb{E}_\\theta[\\frac{c}{\\|X\\|^2}] - 2 = 0\\]\n\\[c = \\frac{\\mathbb{E}_\\theta[\\|X\\|^2]}{\\mathbb{E}_\\theta[\\frac{1}{\\|X\\|^2}]}\\]\n\\(c\\) always &gt; 0, but → 0 as \\(\\|\\theta\\| → \\infty\\)\nWhat if we estimate \\(c\\)? How does adaptivity of \\(c(X)\\) affect MSE?"
  },
  {
    "objectID": "reader/empirical-bayes.html#steins-lemma-1",
    "href": "reader/empirical-bayes.html#steins-lemma-1",
    "title": "The James-Stein Estimator",
    "section": "5 Stein’s Lemma",
    "text": "5 Stein’s Lemma\nUseful tool for computing/estimating risk in Gaussian estimation problems.\n\n5.1 Theorem (Stein’s Lemma - Univariate)\nSuppose \\(X \\sim N(\\theta, \\sigma^2)\\) \\(h: \\mathbb{R} → \\mathbb{R}\\) differentiable, \\(\\mathbb{E}|h'(X)| &lt; \\infty\\)\nThen \\(\\mathbb{E}[(X-\\theta)h(X)] = \\sigma^2\\mathbb{E}[h'(X)]\\)\n\\(\\text{Cov}(X, h(X)) = \\sigma^2\\mathbb{E}[h'(X)]\\)\nProof: Note we can assume w.l.o.g. \\(h(0) = 0\\) (why?) First assume \\(\\theta = 0\\), \\(\\sigma^2 = 1\\)\n\\[\\mathbb{E}[Xh(X)] = \\int xh(x)\\phi(x)dx = \\int xh(x)\\phi(x)dx - \\int h(x)\\phi'(x)dx = \\int h'(x)\\phi(x)dx\\]\nIn the last step we have used \\(\\phi'(x) = -x\\phi(x)\\)\nSimilar argument shows \\(\\int h(x)\\phi(x)dx = \\int h'(x)\\phi(x)dx\\)\nResult holds for \\(\\theta = 0\\), \\(\\sigma^2 = 1\\)\nGeneral \\(\\theta\\), \\(\\sigma^2 \\neq 1\\): write \\(X = \\theta + \\sigma Z\\), \\(Z \\sim N(0,1)\\)\n\\[\\mathbb{E}[(X-\\theta)h(X)] = \\sigma\\mathbb{E}[Zh(\\theta+\\sigma Z)] = \\sigma^2\\mathbb{E}[h'(\\theta+\\sigma Z)] = \\sigma^2\\mathbb{E}[h'(X)]\\]\n\n\n5.2 Multivariate Version\nDefine Frobenius norm: \\(\\|A\\|_F^2 = \\sum_{i,j} A_{ij}^2 = \\text{tr}(A^TA)\\)\n\n\n5.3 Theorem (Stein’s Lemma - Multivariate)\n\\(X \\sim N_d(\\theta, \\sigma^2 I_d)\\), \\(\\theta \\in \\mathbb{R}^d\\) \\(h: \\mathbb{R}^d → \\mathbb{R}^d\\) differentiable, \\(\\mathbb{E}\\|Dh(X)\\|_F &lt; \\infty\\)\nThen \\(\\mathbb{E}[(X-\\theta)^Th(X)] = \\sigma^2\\mathbb{E}[\\text{tr}(Dh(X))]\\)\n\\(\\mathbb{E}[(X-\\theta)h(X)^T] = \\sigma^2\\mathbb{E}[Dh(X)]\\)\nProof:\n\\[\\mathbb{E}[(X_i-\\theta_i)h_i(X)] = \\mathbb{E}[\\mathbb{E}[(X_i-\\theta_i)h_i(X)|X_{-i}]]\\] \\[= \\sigma^2\\mathbb{E}[\\frac{\\partial h_i}{\\partial x_i}(X)]\\]"
  },
  {
    "objectID": "reader/empirical-bayes.html#steins-unbiased-risk-estimator-sure",
    "href": "reader/empirical-bayes.html#steins-unbiased-risk-estimator-sure",
    "title": "The James-Stein Estimator",
    "section": "6 Stein’s Unbiased Risk Estimator (SURE)",
    "text": "6 Stein’s Unbiased Risk Estimator (SURE)\nUnbiased estimator of the MSE of any \\(\\delta(X)\\)\nApply Stein’s Lemma with \\(h(x) = X - \\delta(x)\\)\nAssume \\(\\sigma^2 = 1\\):\n\\[R(\\theta, \\delta) = \\mathbb{E}_\\theta\\|\\delta(X) - \\theta\\|^2 = \\mathbb{E}_\\theta\\|X - \\theta\\|^2 + \\mathbb{E}_\\theta\\|\\delta(X) - X\\|^2 - 2\\mathbb{E}_\\theta[(X-\\theta)^T(X-\\delta(X))]\\] \\[= d + \\mathbb{E}_\\theta\\|\\delta(X) - X\\|^2 - 2\\mathbb{E}_\\theta[\\text{tr}(D(X-\\delta(X)))]\\]\n\\[\\hat{R}(X) = d + \\|\\delta(X) - X\\|^2 - 2\\text{tr}(D\\delta(X))\\]\nis unbiased for the MSE estimator \\(\\delta(X)\\)\nOnly depends on X\nCan also compute MSE via \\(R = \\mathbb{E}_\\theta[\\hat{R}]\\)\n\\[\\mathbb{E}_\\theta[\\|\\delta(X) - h(X)\\|^2] = \\mathbb{E}_\\theta[\\|\\delta(X) - \\theta\\|^2] + \\mathbb{E}_\\theta[\\|h(X) - \\theta\\|^2] - 2\\mathbb{E}_\\theta[(\\delta(X) - \\theta)^T(h(X) - \\theta)]\\]\n\\[R = d + R(\\theta, \\delta) - 2\\mathbb{E}_\\theta[\\text{tr}(D\\delta(X))]\\]\nExample: \\(\\delta(X) = (1-c)X\\) for fixed \\(c\\)\n\\(h(x) = cX\\), \\(Dh = cI_d\\)\n\\(\\hat{R} = d + c^2\\|X\\|^2 - 2cd\\)\n\n6.1 Risk of James-Stein\n\\(\\delta_{JS}(X) = (1 - \\frac{d-2}{\\|X\\|^2})X\\)\n\\(h(X) = \\frac{d-2}{\\|X\\|^2}X\\), \\(Dh(X) = \\frac{d-2}{\\|X\\|^2}I_d - 2(d-2)\\frac{XX^T}{\\|X\\|^4}\\)\n\\(\\|h(X)\\|^2 = \\frac{(d-2)^2}{\\|X\\|^2}\\)\n\\[\\text{tr}(Dh(X)) = \\frac{d(d-2)}{\\|X\\|^2} - \\frac{2(d-2)}{\\|X\\|^2} = \\frac{(d-2)^2}{\\|X\\|^2}\\]\n\\[\\hat{R} = d + \\frac{(d-2)^2}{\\|X\\|^2} - 2\\frac{(d-2)^2}{\\|X\\|^2} = d - \\frac{(d-2)^2}{\\|X\\|^2}\\]\n\\[R(\\theta) = \\mathbb{E}_\\theta[\\hat{R}] = d - (d-2)^2\\mathbb{E}_\\theta[\\frac{1}{\\|X\\|^2}]\\]\nIf \\(\\theta = 0\\) then \\(\\mathbb{E}_0[\\frac{1}{\\|X\\|^2}] = \\frac{1}{d-2}\\)\n\\[R(0) = d - (d-2) = 2\\]\nPossibly surprising\nIf \\(\\theta \\neq 0\\) then \\(\\mathbb{E}_\\theta[\\frac{1}{\\|X\\|^2}] &lt; \\frac{1}{\\|\\theta\\|^2}\\)\n\\[R(\\theta) &lt; d - \\frac{(d-2)^2}{\\|\\theta\\|^2 + d}\\]\nSmaller and smaller advantage, but always better\nNote: \\(\\delta_{JS}(X)\\) also inadmissible\n\\(\\delta_{+}(X) = (1 - \\frac{d-2}{\\|X\\|^2})_+ X\\) is strictly better\nPractically more useful version:\n\\[\\delta_{JS+}(X) = (1 - \\frac{d-3}{\\|X\\|^2})_+ X\\]\nDominates \\(\\delta(X) = X\\) for \\(d \\geq 4\\)\nTaken to logical extreme, suggestion seems dumb: should everyone at Berkeley pool their estimates?\nNote: \\(\\mathbb{E}\\|\\hat{\\theta}\\|^2\\) is improved, but \\(\\mathbb{E}[(\\hat{\\theta}_i - \\theta_i)^2]\\) may get worse for individual coordinates."
  },
  {
    "objectID": "reader/estimation.html",
    "href": "reader/estimation.html",
    "title": "Statistical models and estimation",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/estimation.html#statistical-models",
    "href": "reader/estimation.html#statistical-models",
    "title": "Statistical models and estimation",
    "section": "Statistical models",
    "text": "Statistical models\nUntil now, we have been discussing the topic of probability. Roughly speaking, in probability we fully specify the distribution of some random variables, and then ask what we can say about the distribution. For example, given a complete description of the rules for generating a random walk, we might ask how long, in expectation, it will take to reach a certain threshold. This is an essentially deductive exercise: while the mathematics might be very hard, the questions we ask generally have unambiguous answers.\nIn statistics, we do essentially the opposite: beginning with the data — the Latin word for “given” — we work backwards to draw inferences about the data-generating distribution. This is an inductive exercise, for which the answers will inevitably be more ambiguous.\nWe will generally use the letter \\(X\\) to denote the full data set, which we assume is drawn randomly from some unknown distribution \\(P\\) over the sample space \\(\\cX\\). Let \\(\\cP\\) denote a family of candidate probability distributions, called the statistical model. We assume the analyst knows that one of the elements of \\(\\cP\\) is the true data-generating distribution \\(P\\), but does not know which one. The set \\(\\cX\\) in which \\(X\\) is\nExample (Binomial): As a simple example, we can imagine an analyst who flips a biased coin \\(n\\) times, getting \\(X\\) heads and \\(n-X\\) tails. If we assume the successive flips are independent, and each has a common probability \\(\\theta\\) of landing heads, we can write the model as\n\\[\nX \\sim \\text{Binom}(n, \\theta), \\quad \\text{ for some } \\theta \\in [0,1].\n\\]\nFormally, we could say the family of distributions is \\(\\cP = \\{\\text{Binom}(n, \\theta):\\; \\theta \\in [0,1]\\}\\), a set of distributions indexed by the real parameter \\(\\theta\\).\nNote that in the previous example, the integer \\(n\\) is another important variable in the problem, but we implicitly assumed that it was “known” by the analyst, meaning that it is the same for all \\(P \\in \\cP\\). The parameter \\(\\theta\\), by contrast, is termed “unknown” in the sense that it varies over the family \\(\\cP\\).\n\nParametric vs nonparametric models\nMany of the models we will consider in this class are parametric, typically meaning that they are indexed by finitely many real parameters. That is, we have \\(\\cP = \\{P_\\theta:\\; \\theta \\in \\Theta\\}\\), typically for some parameter space \\(\\Theta \\subseteq \\RR^d\\). Then \\(\\theta\\) is called the parameter or parameter vector.\nIn other models, there is no natural way to index \\(\\cP\\) using \\(d\\) real numbers. We call these nonparametric models. Sometimes excited authors referred to their methods as “assumption-free,” but essentially all nonparametric models still make some assumptions about the data distribution. For example, we might assume independence between multiple observations, or shape constraints such as unimodality.\nExample (Nonparameric model): Suppose we observe an i.i.d. sample of size \\(n\\) from a distribution \\(P\\) on the real line. Even if we do not want to assume anything about \\(P\\), the i.i.d. assumption will play an important role in the analysis. We might write this model as\n\\[\nX_1,\\ldots,X_n \\simiid P, \\quad \\text{ for some distribution } P \\text{ on } \\RR.\n\\]\nFormally, if \\(X = (X_1,\\ldots,X_n)\\), we can write the family as \\(\\cP = \\{P^n:\\; P \\text{ is a distribution on } \\RR\\}\\), where \\(P^n\\) represents the \\(n\\)-fold product of \\(P\\) on \\(\\RR^n\\).\nNotation: Much of what we will learn in this course applies to parametric and nonparametric models alike, and indeed there is no crisp demarcation between parametric and nonparametric models in practice. It will often be convenient to use notation \\(\\cP = \\{P_\\theta :\\; \\theta \\in \\Theta\\}\\), without specifying what kind of set \\(\\Theta\\) is; in particular there is nothing to stop \\(\\theta\\) from being an infinite-dimensional object such as a density function. We can work in this notation without any loss of generality, since we could always take \\(\\theta = P\\) and \\(\\Theta = \\cP\\).\n\n\nBayesian vs Frequentist inference\nThus far we have assumed the data \\(X\\) follows a distribution \\(P_\\theta\\), for some unknown parameter \\(\\theta\\) which can be any arbitrary member of the set \\(\\Theta\\). In some contexts we will introduce an additional assumption we can call the Bayesian assumption: that \\(\\theta\\) is itself random, drawn from some known distribution \\(\\Lambda\\) that we call the prior.\nA major advantage of this assumption is that it reduces the problem of inference about \\(\\theta\\) to simply calculating the conditional distribution of \\(\\theta\\) given \\(X\\).\nThe philosophical ramifications of this assumption, as well as its practical advantages and disadvantages, will be a major theme later in the course, but for now we will simply say it is an assumption we are sometimes, but not always, willing to make. From a mathematical perspective, it makes no more or less sense to assume \\(\\theta\\) is random than it does to assume \\(\\theta\\) is fixed and unknown.\nFor the remainder of this lecture, and until our unit on Bayesian inference, we will refrain from making this assumption, instead regarding \\(\\theta\\) as taking an arbitrary fixed value in \\(\\Theta\\)."
  },
  {
    "objectID": "reader/estimation.html#estimation-in-statistical-models",
    "href": "reader/estimation.html#estimation-in-statistical-models",
    "title": "Statistical models and estimation",
    "section": "Estimation in statistical models",
    "text": "Estimation in statistical models\nHaving observed \\(X \\sim P_\\theta\\), an unknown distribution in the model \\(\\cP = \\{P_\\theta:\\; \\theta \\in \\Theta\\}\\), we will be interested in learning something about \\(\\theta\\). In estimation, we guess the value of some quantity of interest \\(g(\\theta)\\), called the estimand. Our guess is called the estimate \\(\\delta(X)\\), calculated based on the data. The method \\(\\delta(\\cdot)\\) that we use to calculate the estimate is called the estimator.\nExample (Binomial, continued): Returning to our binomial example from above, we may want to estimate \\(g(\\theta) = \\theta\\), the probability of the coin landing heads. A natural estimator is \\(\\delta_0(X) = X/n\\), the fraction of coins landing heads in any given trial. One favorable property of this estimator is that it is unbiased, meaning that \\(\\EE_\\theta \\delta_0(X) = g(\\theta)\\), for all \\(\\theta \\in \\Theta\\).\nThere are many potential estimators for any given problem, so our goal will generally be to find a good estimator. To evaluate and compare estimators, we must have a way of evaluating how successful an estimator is in any given realization of the data. To this end we introduce the loss function \\(L(\\theta, d)\\), which measures how bad it is to guess that \\(g(\\theta) = d\\) when \\(\\theta\\) is the true parameter value. Typically loss functions are non-negative, with \\(L(\\theta, d) = 0\\) if and only if \\(g(\\theta) = d\\) (no loss from a perfect guess) but this is not required.\nIn any given problem, we should ideally choose the loss that best measures our own true (dis)utility function, but in practice people fall back on simple defaults. One loss function that is especially popular for its mathematical convenience is the squared-error loss, defined by \\(L(\\theta, d) = (d-g(\\theta))^2\\).\nWhereas the loss function measures how (un)successful an estimator is in one realization of the data, we would really like to evaluate an estimator’s performance over the whole range of possible data sets \\(X\\) that we might observe. This is measured by the risk function, defined as\n\\[\nR(\\theta; \\delta(\\cdot)) = \\EE_\\theta [\\, L(\\theta, \\delta(X)) \\,] = \\int L(\\theta, \\delta(x)) \\td P_\\theta(x)\n\\]\nRemark on notation: The subscript in the previous expression tells us which of our candidate probability distributions to use in evaluating the expectation. In some other fields, people may use the subscript to indicate “what randomness to integrate over,” with the implication that any random variable that does not appear in the subscript should be held fixed. In our course, it should generally be assumed that any expectation or probability is integrating over the joint distribution of the entire data set; if we want to hold something fixed we will condition on it. Recall that, for now, the parameter \\(\\theta\\) is fixed unless otherwise specified.\nThe semicolon in the risk function is meant to indicate we are viewing it primarily as a function of \\(\\theta\\). That is, we should think of and estimator \\(\\delta\\) as having a risk function \\(R(\\theta)\\), and the second input in \\(R(\\theta; \\delta)\\) is telling us which estimator’s risk function to evaluate at \\(\\theta\\).\nThe risk for the squared-error loss is called the mean squared error (MSE):\n\\[\n\\textrm{MSE}(\\theta; \\delta) = \\EE_\\theta\\left[\\,(\\delta(X) - g(\\theta))^2\\,\\right]\n\\]\nExample (Binomial, continued): To calculate the MSE of our estimator \\(\\delta_0 = X/n\\), note that \\(\\EE_\\theta[X/n] = \\theta\\) (the estimator is unbiased). As a result, we have\n\\[\n\\begin{aligned}\n\\textrm{MSE}(\\theta; \\delta_0) &= \\EE_\\theta\\left[ \\left(\\frac{X}{n} - \\theta\\right)^2\\right] \\\\[7pt]\n&= \\text{Var}_\\theta(X/n)\\\\[3pt]\n&= \\frac{1}{n}\\theta(1-\\theta)\n\\end{aligned}\n\\]\nOne reason why we might consider estimators other than \\(\\delta_0\\) is that, if \\(n\\) is small, our estimate could be quite noisy. As an extreme example, if \\(n=1\\) we will always estimate either \\(\\theta = 0\\) or \\(\\theta = 1\\), both of which would be extreme conclusions to draw after a single trial. One simple way of reducing the variance is to pretend that we flipped the coin an additional \\(m\\) times resulting in \\(a\\) heads and \\(m-a\\) tails. This will tend to shade our estimate toward \\(a/m\\), reducing the risk if \\(\\theta = a/m\\) but possibly increasing the risk for other values of \\(\\theta\\).\n\nWe show the risk function for several alternative estimators of this form below:\n\\[\n\\delta_1(X) = \\frac{X + 1}{n + 2}, \\quad \\delta_2(X) = \\frac{X + 2}{n + 4}, \\quad \\delta_3(X) = \\frac{X + 1}{n}\n\\]\nThe last estimator, \\(\\delta_3\\), is another example where we add something to \\(X\\) in the numerator but nothing \\(n\\) in the denominator.\n\nlibrary(RColorBrewer)\n\nn = 16\n\n## risk function of estimator (X + synth.heads) / (n + synth.flips)\nbinom.mse &lt;- function(theta, n, synth.heads, synth.flips) {\n  binom.var &lt;- theta * (1 - theta) * n / (n + synth.flips)^2\n  binom.bias &lt;- (n * theta + synth.heads) / (n + synth.flips) - theta\n  return(binom.var + binom.bias^2)\n}\n\n\npalette &lt;- c(\"black\",brewer.pal(4, \"Set1\"))\ncurve(binom.mse(x, n, 0, 0), from=0, to=1, ylim=c(0,0.35/n), lwd=2, col=palette[1],\n      main = \"Mean squared error for binomial estimators (n=16)\", \n      ylab=expression(MSE(theta)), \n      xlab=expression(theta))\ncurve(binom.mse(x, n, 1, 2), add=TRUE, col=palette[2], lwd=2) \ncurve(binom.mse(x, n, 2, 4), add=TRUE, col=palette[3], lwd=2) \ncurve(binom.mse(x, n, 1, 0), add=TRUE, col=palette[4], lwd=2) \nlegend(\"topright\", col=palette, lwd=2,bty=\"n\",\n       legend=c(expression(delta[0]), expression(delta[1]), expression(delta[2]), expression(delta[3])))\n\n\n\n\nIf we look at the vertical axis, the MSE may appear to be very small, especially considering we only have 16 flips. But recall that an MSE of \\(0.01\\) means that we are typically missing by about \\(0.1\\), while estimating a parameter that is between \\(0\\) and \\(1\\).\n\nComparing estimators\nIn comparing the risk functions of these estimators, we can notice a few things. As expected, both \\(\\delta_1\\) and \\(\\delta_2\\) outperform \\(\\delta_0\\) for values of \\(\\theta\\) close to \\(1/2\\), but underperform for more extreme values of \\(\\theta\\). The estimator \\(\\delta_3\\), however, performs worse than \\(\\delta_0\\) throughout the entire parameter space; this is because we have added bias without doing anything to reduce the variance. While it is difficult to choose between the other three estimators, we can at least rule out \\(\\delta_3\\) on the grounds that we have no reason to ever prefer it over \\(\\delta_0\\).\nFormally, we say an estimator \\(\\delta\\) is inadmissible if there is some other estimator \\(\\delta^*\\) for which\n\n\\(R(\\theta; \\delta^*) \\leq R(\\theta; \\delta)\\) for all \\(\\theta\\in\\Theta\\), and\n\\(R(\\theta; \\delta^*) &lt; R(\\theta; \\delta)\\) for some \\(\\theta\\in\\Theta\\).\n\nIn this case we say \\(\\delta^*\\) strictly dominates \\(\\delta\\); more generally we can say \\(\\delta^*\\) *dominates* \\(\\delta\\) if we only have (1). An estimator is admissible if it is not inadmissible. We can see from our plot that \\(\\delta_3\\) is inadmissible because \\(\\delta_0\\) strictly dominates it.\nComparing the other three estimators is more difficult, however, because no one of them dominates any other. In most estimation problems, including this one, we can never hope to come up with an estimator that uniformly attains the smallest risk among all estimators. That is because, for example, we can always choose the constant estimator \\(\\delta(X) \\equiv 1/2\\) that simply ignores the data and always guesses that \\(\\theta = 1/2\\). This estimator may perform poorly for other values of \\(\\theta\\), but it is the only estimator that has exactly zero MSE for \\(\\theta = 1/2\\).\nIf we cannot hope to minimize the risk for every value of \\(\\theta\\) simultaneously then we must come up with some other way to resolve the inherent ambiguity in comparing all of the many estimators that we must choose among.\nIn our unit on estimation, we will consider two main strategies for resolving this ambiguity.\n\n\nStrategy 1: Summarizing the risk function by a scalar\nIf we can find a way to summarize the risk function for each estimator by a single real number that we want to minimize, then we can find an estimator that is optimal in this summary sense. The two main ways to summarize the risk are to examine the average-case risk and the worst-case risk.\n\nAverage-case risk (Bayes estimation)\nThe first option is to minimize some (weighted) average of the risk function over the parameter space \\(\\Theta\\) :\n\\[\n\\minz_{\\delta(\\cdot)} \\int_\\theta R(\\theta; \\delta)\\td \\Lambda(\\theta)\n\\]\nThe average is taken with respect to some measure \\(\\Lambda\\) of our choosing. If \\(\\Lambda(\\Theta) &lt; \\infty\\) we can assume without loss of generality that \\(\\Lambda\\) is a probability measure, since we could always normalize it without changing the minimization problem. Then, this average is simply the estimator’s expected risk, called the Bayes risk, or equivalently the expected loss averaging over the joint distribution of \\(\\theta\\) and \\(X\\). An estimator that minimizes the Bayes risk is called a Bayes estimator.\nIn the binomial problem above, \\(\\delta_1(X) = \\frac{X + 1}{n + 2}\\) is a Bayes estimator that minimizes the average-case risk with respect to the Lebesgue measure on \\(\\Theta = [0,1]\\). \\(\\delta_2(X) = \\frac{X+2}{n+4}\\) is also a Bayes estimator with respect to a different prior, specifically the \\(\\textrm{Beta}(2,2)\\) distribution. We will show this later.\nNote that minimizing the average-case risk may be a natural thing to do regardless of whether we “really believe” that \\(\\theta \\sim \\Lambda\\). Hence Bayes estimators are well-motivated even from a purely frequentist perspective; using them does not have to imply one has any specific position on the philosophical interpretation of probability.\nIf \\(\\Lambda(\\Theta) = \\infty\\) then we call \\(\\Lambda\\) an improper prior, and we can no longer interpret the corresponding Bayes risk as an expectation. But, as we will see, working with improper priors can sometimes be convenient and often leads to good estimators in practice.\n\n\nWorst-case risk (Minimax estimation)\nIf we are reluctant to average over the parameter space, we can instead seek to minimize the worst-case risk over the entire parameter space:\n\\[\n\\minz_{\\delta(\\cdot)} \\sup_{\\theta\\in\\Theta} R(\\theta; \\delta)\n\\]\nThis minimization problem has a game-theoretic interpretation if we imagine that, after we choose our estimator, Nature will adversarially choose the least favorable parameter value.\nAs we will see, minimax estimation is closely related to Bayes estimation and the minimax estimator is commonly a Bayes estimator.\nThe minimax perspective pushes us to choose estimators with flat risk functions, and indeed \\(\\delta_2(X) = \\frac{X + 2}{X + 4}\\) is the minimax estimator when \\(n = 16\\).\n\n\n\nStrategy 2: Restricting the choice of estimators\nThe second main strategy for resolving ambiguity is to restrict ourselves to choose an estimator that satisfies some additional side constraint.\n\nUnbiased estimation\nOne property we might want to demand of an estimator is that it be unbiased, meaning that \\(\\EE_\\theta [\\delta_0(X)] = g(\\theta)\\), for all \\(\\theta\\in\\Theta\\). This rules out, for example, estimators that ignore the data and always guess the same value.\nAs we will see, once we requiring unbiasedness there will often be a clear winner among all remaining estimators under consideration, called the uniformly minimum variance unbiased (UMVU) estimator, which uniformly minimizes the risk for any convex loss function.\nOf the four estimators we considered above, only \\(\\delta_0(X) = X/n\\) is unbiased, and it is indeed the UMVU for this problem."
  },
  {
    "objectID": "reader/exponential-families.html",
    "href": "reader/exponential-families.html",
    "title": "Exponential Families",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/exponential-families.html#exponential-family-structure",
    "href": "reader/exponential-families.html#exponential-family-structure",
    "title": "Exponential Families",
    "section": "Exponential family structure",
    "text": "Exponential family structure\nThus far we have discussed statistical models in the abstract without making any assumptions about them. Exponential families, the topic of this lecture, are models with a special structure that makes them especially easy to work with.\n\nWe say the model \\(\\cP = \\{P_\\eta:\\eta \\in \\Xi\\}\\) is an \\(s\\)-parameter exponential family if it is defined by a family of densities of the form:\n\\[\np_\\eta(x) = e^{\\eta'T(x) - A(\\eta)}h(x),\n\\tag{1}\\]\nall with respect to a common dominating measure \\(\\mu\\), i.e. a measure \\(\\mu\\) such that \\(P_\\eta \\ll \\mu\\) for all \\(\\eta\\in \\Xi\\).\nThe different parts of the expression in Equation 1 have distinct names referring to the role they play in defining the density:\n\\[\n\\begin{aligned}\nT&:\\; \\cX \\to \\RR^s &\\qquad &\\text{ is called the {\\it sufficient statistic}}\\\\\nh&:\\; \\cX \\to [0,\\infty) &\\qquad &\\text{ is called the {\\it carrier density} or {\\it base density}}\\\\\n\\eta &\\in \\Xi \\subseteq \\RR^s &\\qquad &\\text{ is called the {\\it natural parameter}}\\\\\nA&:\\; \\Xi \\to \\RR &\\qquad &\\text{ is called the {\\it log-partition function}}\n\\end{aligned}\n\\]\nThe function \\(A(\\eta)\\) is totally determined by \\(T\\) and \\(h\\), and plays the role of normalizing the density so that \\(P_\\eta(\\cX) = 1\\):\n\\[\nA(\\eta) = \\log\\left( \\int_\\cX e^{\\eta'T(x)}h(x)\\td\\mu(x)\\right) \\leq \\infty\n\\tag{2}\\]\nIf \\(A(\\eta) = \\infty\\) then there is no way to normalize \\(p_\\eta\\), so \\(\\eta\\) is not an allowed value for the natural parameter to take. The natural parameter space, which we denote \\(\\Xi_1\\), is the set of all \\(\\eta\\) values for which the family is normalizable:\n\\[\n\\Xi_1 = \\{\\eta:\\; A(\\eta) &lt; \\infty\\} \\subseteq \\RR^s.\n\\]\nWe will show in Homework 1 that \\(A(\\eta)\\) is a convex function, so \\(\\Xi_1\\) is a convex set.\nNote that, because \\(\\mu\\) is allowed to be an arbitrary measure, \\(h(x)\\) also plays an inessential role since we could always absorb it into the base measure \\(\\mu\\). More precisely, suppose we define a new dominating measure \\(\\nu\\) whose density with respect to \\(\\mu\\) is \\(h\\) (informally we can write \\(\\td\\nu = h\\td\\mu\\)). Then \\(P_\\eta\\), which had density \\(e^{\\eta'T(x)}h(x)\\) with respect to \\(\\mu\\), now has density \\(e^{\\eta'T(x)}\\) with respect to \\(\\nu\\).\nAs a result, if we were aiming for maximal parsimony we could assume without loss of generality that \\(h(x) = 1\\), removing it from the definition Equation 1 and replacing \\(\\mu\\) with our new, bespoke measure \\(\\nu\\). However, it is convenient to leave Equation 1 as is if it allows us to take \\(\\mu\\) to be some simple default measure like a counting measure or Lebesgue measure. In that case, \\(p_\\eta\\) will be a standard pmf or pdf, so we can discuss it without going over the head of anyone who lacks a background in measure theory.\nExample (Poisson): The Poisson distribution \\(\\textrm{Pois}(\\lambda)\\) has probability mass function\n\\[\np_\\lambda(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad \\textrm{ on } x = 0, 1, 2, \\ldots.\n\\]\nFormally this pmf is a density over the counting measure on the set of non-negative integers \\(\\ZZ_+ = \\{0,1,\\ldots\\}\\).\nLetting \\(\\lambda\\) range over \\([0, \\infty)\\) yields an exponential family, but it is not immediately obvious from the form of the density. To see this, we need to massage \\(p_\\lambda(x)\\) a bit, by observing that\n\\[\np_\\lambda(x) = \\exp\\{(\\log \\lambda) x - \\lambda\\}\\frac{1}{x!}.\n\\]\nNow we see that we can reparameterize the family by setting \\(\\eta = \\log\\lambda\\), leading to\n\\[\np_\\eta(x) = \\exp\\{\\eta x - e^\\eta\\} \\frac{1}{x!}.\n\\] At this point, we immediately recognize \\(p_\\eta\\) as an exponential family with sufficient statistic \\(T(x) = x\\), and base density \\(h(x) = \\frac{1}{x!}\\), and log-partition function \\(A(\\eta) = e^{\\eta}\\).\nNote that this is not the only way to decompose the Poisson distribution as an exponential family. For example, we could just as well take \\(T(x) = x/2\\); then we would have \\(\\eta = 2\\log\\lambda\\) and \\(A(\\eta) = e^{\\eta/2}\\). Or, we could take \\(T(x) = x + 1\\) and \\(A(\\eta) = e^{\\eta} + \\eta\\).\nThis is not just a property of the Poisson distribution; the decomposition is generally non-unique for exponential families. For any exponential family of the form Equation 1 , if \\(U \\in \\RR^{s\\times s}\\) is invertible and \\(v \\in \\RR^s\\) then we can write the same density as \\(e^{\\zeta'S(x) - B(\\zeta)}h(x)\\), for new sufficient statistic \\(S(x) = U T(x) + v\\), new natural parameter \\(\\zeta = (U^{-1})'\\eta\\), and and new log-partition function \\(B(\\zeta) = A(U'\\zeta) + \\zeta'U^{-1}v\\). So “the” sufficient statistic of an exponential family is defined only up to invertible affine transformations."
  },
  {
    "objectID": "reader/exponential-families.html#differential-identities",
    "href": "reader/exponential-families.html#differential-identities",
    "title": "Exponential Families",
    "section": "Differential identities",
    "text": "Differential identities\nExponentiating Equation 2, we obtain the equation\n\\[\ne^{A(\\eta)} = \\int_\\cX e^{\\eta'T(x)}h(x)\\td\\mu(x)\n\\tag{3}\\]\nWe can derive many interesting identities by differentiating this function, and related functions, with respect to \\(\\eta\\). We will always evaluate derivatives by differentiating under the integral sign. This is not always a correct operation, but by Theorem 2.4 in Keener, it is correct on the interior of the natural parameter space \\(\\Xi_1\\). We refer the reader to Keener for details.\n\nMean of \\(T(X)\\)\nPartially differentiating Equation 3 once with respect to a generic coordinate \\(\\eta_j\\), for \\(j =1, \\ldots, s\\), we obtain\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\eta_j} e^{A(\\eta)} &= \\int_\\cX \\frac{\\partial}{\\partial \\eta_j} e^{\\eta'T(x)}h(x)\\td\\mu(x)\\\\[7pt]\ne^{A(\\eta)} \\frac{\\partial A}{\\partial \\eta_j}(\\eta) &= \\int_\\cX T_j(x) e^{\\eta'T(x)}h(x)\\td\\mu(x)\\\\[7pt]\n\\frac{\\partial A}{\\partial \\eta_j}(\\eta) &= \\int_\\cX T_j(x) e^{\\eta'T(x) - A(\\eta)}h(x)\\td\\mu(x)\\\\[7pt]\n&= \\EE_\\eta \\left[\\,T_j(X)\\,\\right]\\\\\n\\end{aligned}\n\\]\nArranging these partial derivatives into a vector, we obtain\n\\[\n\\nabla A(\\eta) = \\EE_\\eta\\left[\\,T(X)\\,\\right],\n\\]\nwhich gives us a very convenient method for evaluating the expectation of the sufficient statistic.\n\n\nVariance of \\(T(X)\\)\nPushing our luck further, we can take a second partial derivative:\n\\[\n\\begin{aligned}\n\\frac{\\partial^2}{\\partial \\eta_j\\partial \\eta_k} e^{A(\\eta)} &= \\int_\\cX \\frac{\\partial^2}{\\partial \\eta_j\\partial \\eta_k} e^{\\eta'T(x)}h(x)\\td\\mu(x)\\\\[7pt]\ne^{A(\\eta)}\\left(\\frac{\\partial^2 A}{\\partial \\eta_j\\partial \\eta_k} + \\frac{\\partial A}{\\partial \\eta_j} \\frac{\\partial A}{\\partial \\eta_k} \\right)&= \\int_\\cX T_j(x) T_k(x)e^{\\eta'T(x)}h(x)\\td\\mu(x)\\\\[7pt]\n\\frac{\\partial^2 A}{\\partial \\eta_j\\partial \\eta_k} + \\EE_\\eta[T_j(X)]\\EE_\\eta[T_k(X)] &= \\EE_\\eta \\left[\\,T_j(X) T_k(X)\\,\\right]\\\\\n\\frac{\\partial^2 A}{\\partial \\eta_j\\partial \\eta_k} &= \\text{Cov}_\\eta\\left(T_j(X), T_k(X)\\right).\n\\end{aligned}\n\\]\nAgain, collecting these second partials into a Hessian matrix gives us\n\\[\n\\nabla^2 A(\\eta) = \\text{Var}_\\eta(T(X)),\n\\]\nwhere the right-hand side denotes the \\(s\\times s\\) variance-covariance matrix of the random vector \\(T(X)\\).\nExample (Poisson, continued): As we showed above, in the Poisson exponential family the sufficient statistic is \\(T(X)=X\\), the natural parameter is \\(\\eta = \\log\\lambda\\), and the log-partition function is \\(A(\\eta) = e^\\eta \\;(=\\lambda)\\).\nNote: This calculation would not have worked correctly if we had instead said \\(A(\\eta) = \\lambda\\), and differentiated that expression with respect to \\(\\lambda\\). We would then get \\(\\EE_\\eta[X] = 1\\) and \\(\\text{Var}_\\eta(X) = 0\\), which are clearly incorrect.\n\n\nMoment-generating function and cumulant-generating function\nThe moment generating function (MGF) of a \\(d\\)-dimensional random vector \\(X\\sim P\\) is defined as \\(M^X(u) = \\EE[e^{u'X}]\\), for \\(u\\in \\RR^d\\). If the MGF is well-defined in a neighborhood of \\(u=0\\), then we can use it to calculated moments of \\(X\\) by evaluating its derivatives at 0.\nWe can show this using manipulations very similar to the ones we saw above. To evaluate the first moment of \\(X_j\\), we can differentiate once with respect to \\(u_j\\), since\n\\[\n\\frac{\\partial}{\\partial u_j} M^X(u) = \\int_\\cX \\frac{\\partial}{\\partial u_j} e^{u'x}\\td P(x) = \\int_\\cX x_j e^{u'x}\\td P(x) = \\int_\\cX x_j e^{u'x} \\td P(x).\n\\]\nHere we have again assumed that we can differentiate under the integral sign; this is a technical condition that we would check if we were being more careful.\nEvaluating the derivative at \\(u=0\\), we obtain \\(\\frac{\\partial}{\\partial u_j} M^X(0) = \\int_\\cX x_j \\td P(x) = \\EE[X_j]\\). Moreover, we can repeat this trick as many times as we want, leading to a formula for mixed partial derivatives of any order:\n\\[\n\\left.\\frac{\\partial^{m_1 + \\cdots + m_d}}{\\partial u_1^{m_1}\\cdots\\partial u_d^{m_d}} M^X(u)\\right|_{u=0} = \\left.\\int_{\\cX} x_1^{m_1}\\cdots x_d^{m_d} e^{u'x}\\td P(x)\\right|_{u=0} = \\EE\\left[\\,X_1^{m_1} \\cdots X_d^{m_d}\\,\\right].\n\\tag{4}\\]\nThe MGF is therefore very useful for evaluating moments of \\(X\\). It is also useful for finding distributions of sums of independent random variables, because \\(M^{X + Y}(u) = M^X(u)M^Y(u),\\) if \\(X\\) and \\(Y\\) are independent. If two random variables have the same MGF then they have the same distribution.\nIn an exponential family, the MGF of \\(T(X)\\), under sampling from \\(P_\\eta\\), is simple to evaluate:\n\\[\n\\begin{aligned}\nM^{T(X)}_\\eta(u) &= \\EE_\\eta\\left[\\,e^{u'T(X)}\\,\\right]\\\\[5pt]\n&= \\int_\\cX e^{u'T(x)}e^{\\eta'T(x) - A(\\eta)}h(x)\\td\\mu(x) \\\\[5pt]\n&= e^{-A(\\eta)}\\int_\\cX e^{(u+\\eta)'T(x)} h(x)\\td\\mu(x)\\\\[5pt]\n&= e^{A(\\eta+u)-A(\\eta)}\n\\end{aligned}\n\\]\nExample (Poisson, continued): The MGF for \\(X \\sim \\text{Pois}(\\lambda)\\), with \\(\\eta = \\log\\lambda\\), is\n\\[\nM^{X}_\\eta(u) = \\exp\\{e^{\\eta + u} - e^{\\eta}\\} = \\exp\\{\\lambda (e^u - 1)\\}\n\\]\nTo see how the MGF is useful, suppose we have \\(X_i \\sim \\text{Pois}(\\lambda_i)\\), independently for \\(i = 1,2,\\ldots,n\\), and we want to know the distribution of \\(X_+ = \\sum_i X_i\\). Then we can multiply the MGF’s for \\(X_1,\\ldots,X_n\\) together to obtain the MGF for \\(X_+\\):\n\\[\nM^{X_+}(u) = \\prod_i M_{\\eta_i}^{X_i}(u) = \\exp\\left\\{\\sum_i \\lambda_i (e^u-1)\\right\\}.\n\\]\nAs a result, we have \\(X_+ \\sim \\text{Pois}(\\lambda_+)\\), for \\(\\lambda_+ = \\sum_j \\lambda_i\\).\nLikewise, the closely related cumulant-generating function (CGF) is defined as the log of the MGF:\n\\[\nK^{T(X)}_\\eta(u) = \\log M^{T(X)}_\\eta(u) = A(\\eta+u)-A(\\eta)\n\\]\nEvaluating the CGF’s derivatives at \\(u=0\\) gives us the distribution’s cumulants instead of its moments (the first two cumulants are the mean and variance). \\(K_\\eta^{T(X)}\\) and \\(A\\) are closely related. In particular, note that\n\\[\n\\left.\\frac{\\partial}{\\partial \\eta_j}K_\\eta^{T}(u)\\right|_{u=0} = \\frac{\\partial}{\\partial \\eta_j} A(\\eta),\n\\]\nThis relationship explains why we can also get cumulants for \\(T(X)\\) under \\(P_\\eta\\) by differentiating \\(A(\\eta)\\). It also partly explains why \\(A(\\eta)\\) is sometimes referred to as the CGF even though it is generally not the CGF for \\(T(X)\\)."
  },
  {
    "objectID": "reader/exponential-families.html#other-parameterizations",
    "href": "reader/exponential-families.html#other-parameterizations",
    "title": "Exponential Families",
    "section": "Other parameterizations",
    "text": "Other parameterizations\nSometimes instead of parameterizing \\(\\cP\\) by the natural parameter \\(\\eta\\), it is more convenient to parameterize the family by another parameter \\(\\theta\\). Then we can write the density in terms of this alternative parameterization as\n\\[\np_\\theta(x) = e^{\\eta(\\theta)'T(x) - B(\\theta)}h(x), \\quad \\text{ where } B(\\theta) = A(\\eta(\\theta)).\n\\]\nThe Poisson distribution, if indexed by the mean \\(\\lambda\\), is an example of such an alternative parameterization, with \\(\\eta(\\lambda) = \\log\\lambda\\) and \\(B(\\lambda) = \\lambda\\). Another example is the normal family:\nExample (Normal): Consider the model \\(X\\sim \\cN(\\mu, \\sigma^2)\\), for \\(\\mu\\in\\RR\\) and \\(\\sigma^2&gt;0\\). The usual parameter vector for this problem is \\(\\theta = (\\mu, \\sigma^2)\\). The density in that parameterization is\n\\[\np_\\theta(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-(\\mu-x)^2/2\\sigma^2\\right\\}.\n\\]\nExpanding the square and raising the \\(\\sqrt{2\\pi\\sigma^2}\\) into the exponent, we can massage the density into the exponential family structure we are looking for:\n\\[\np_\\theta(x)\n= \\exp\\left\\{\\frac{\\mu}{\\sigma^2} x - \\frac{1}{2\\sigma^2}x^2 - \\frac{\\mu}{2\\sigma^2} - \\frac{1}{2}\\log\\left(2\\pi\\sigma^2\\right)\\right\\},\n\\]\nwhich we can recognize as an exponential family with sufficient statistic \\(T(x) = (x,x^2)\\), natural parameter \\(\\eta(\\theta) = (\\mu/\\sigma^2,-1/2\\sigma^2)\\), carrier density \\(h(x)=1\\), and log-partition function\n\\[\nB(\\theta) = \\frac{\\mu^2}{2\\sigma^2} + \\frac{1}{2}\\log\\left(2\\pi\\sigma^2\\right).\n\\]\nWe can rewrite the log-partition function in terms of \\(\\eta_1 = \\mu^2/2\\sigma^2\\) and \\(\\eta_2=-1/2\\sigma^2\\) to complete the natural parameterization:\n\\[\np_\\eta(x) = e^{\\eta'T(x) - A(\\eta)}, \\quad \\text{ for } A(\\eta) = \\frac{-\\eta_1^2}{4\\eta_2} + \\frac{1}{2}\\log(-\\pi/\\eta_2)\n\\]\nHence, the Gaussian is the (only) exponential family with sufficient statistic \\(T(x) = (x,x^2)\\), and carrier density \\(h(x)=1\\), with respect to the Lebesgue measure on \\(\\RR\\).\nExample (Binomial): Next, consider the model \\(X \\sim \\text{Binom}(n,\\theta)\\), which has pmf\n\\[\np_\\theta(x) = \\theta^x (1-\\theta)^{n-x}\\binom{n}{x}.\n\\]\nWe can again massage this into canonical form by raising the parameters into the exponent and collecting terms\n\\[\n\\begin{aligned}\np_\\theta(x)\n&= \\exp\\left\\{x\\log\\theta + (n-x)\\log(1-\\theta)\\right\\}\\binom{n}{x}\\\\\n&= \\exp\\left\\{x\\log\\left(\\frac{\\theta}{1-\\theta}\\right)-n\\log(1-\\theta)\\right\\}\\binom{n}{x},\n\\end{aligned}\n\\]\nwhich we recognize as an exponential family structure with \\(T(x)=x\\), natural parameter \\(\\eta=\\log\\left(\\frac{\\theta}{1-\\theta}\\right)\\) The natural parameter \\(\\eta\\) is called the log-odds or logit, which is used in classification models such as logistic regression and the many extensions thereof.\nExample (Beta): The Beta distribution is a common family of distributions on the unit interval. If \\(X \\sim \\text{Beta}(\\alpha,\\beta)\\) then \\(X\\) has pdf\n\\[\n\\begin{aligned}\np_{\\alpha,\\beta}(x)\n&= \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\\\[5pt]\n&= \\exp\\{\\alpha \\log x + \\beta\\log (1-x) - \\log B(\\alpha,\\beta)\\}\\cdot \\frac{1}{x(1-x)},\n\\end{aligned}\n\\]\nwhere \\(B(\\alpha,\\beta) = \\int_0^1 t^{\\alpha-1}(1-t)^{\\beta-1}\\td t\\) is called the beta function. We recognize this as an exponential family with \\(T(x) = (\\log x, \\log(1-x))\\), \\(\\eta = (\\alpha,\\beta)\\), and \\(h(x) = \\frac{1}{x(1-x)}\\), though as always there are other ways to decompose the density.\nIn addition to these examples, we could add most of the distributional families detailed on Wikipedia: the Gamma, multinomial, Dirichlet, Pareto, Wishart, and many others. We will see more exponential family examples throughout the course."
  },
  {
    "objectID": "reader/exponential-families.html#exponential-tilting",
    "href": "reader/exponential-families.html#exponential-tilting",
    "title": "Exponential Families",
    "section": "Exponential tilting",
    "text": "Exponential tilting\nTo help interpret what it means for a model to have an exponential family structure, we can think of \\(p_\\eta(x) = e^{\\eta'T(x) - A(\\eta)} h(x)\\) as an exponential tilt of the carrier density \\(h(x)\\). That is, beginning with \\(h(x)\\), we first multiply by \\(e^{\\eta'T(x)}\\), increasing the density of points in the sample space for which \\(\\eta'T(x)\\) is largest relative to those for which \\(\\eta'T(x)\\) is smaller. Then, we re-normalize by \\(e^{-A(\\eta)}\\) to obtain a probability distribution.\nThis is easiest to understand in a one-parameter family with sufficient statistic \\(T(X) = X\\), (need to finish)"
  },
  {
    "objectID": "reader/exponential-families.html#visualization-of-exponential-tilting",
    "href": "reader/exponential-families.html#visualization-of-exponential-tilting",
    "title": "Exponential Families",
    "section": "Visualization of exponential tilting",
    "text": "Visualization of exponential tilting\nBelow we visualize an exponential family with triangular'' base density $h(x) = \\max\\{0, 1-|x|\\}$ and three sufficient statistics $T_1(X) = X, T_2(X) = X^2, and T_3(X) = \\cos(100 X)$. Use the dials totilt’’ the density toward each statistic, and see how the density changes.\n\nPlot = require(\"@observablehq/plot\")\nd3 = require(\"d3\")\n\nkatex = require(\"https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js\")\n\n// Function to render LaTeX\nfunction tex(string) {\n  let span = document.createElement('span');\n  katex.render(string, span, {output: \"mathml\", throwOnError: false});\n  return span;\n}\n\n// Create sliders for mean and standard deviation\nviewof eta1 = Inputs.range([-5, 5], {step: 0.1, label: tex(\"\\\\eta_1\"), value: 0})\nviewof eta2 = Inputs.range([-5, 5], {step: 0.1, label: tex(\"\\\\eta_2\"), value: 0})\nviewof eta3 = Inputs.range([-0.3, 0.3], {step: 0.01, label: tex(\"\\\\eta_3\"), value: 0})\n\n// Function to generate normal distribution data\nfunction explDistribution(eta1, eta2, eta3, n = 2000) {\n  const x = d3.range(-1, 1, 2/n);\n  const unnorm = x.map(x =&gt; ({\n    x: x,\n    y: Math.exp(eta1 * x + eta2 * x * x + eta3 * Math.cos(30 * x * Math.PI)) * (1 - Math.abs(x))\n  }));\n  const normConst = d3.sum(unnorm, point =&gt; point.y) * 2 / n;\n  return unnorm.map(point =&gt; ({\n    x: point.x,\n    y: point.y / normConst\n  }));\n}\n\n// Create the plot\nPlot.plot({\n  width: 640,\n  height: 400,\n  x: {label: \"X\"},\n  y: {label: \"Density\"},\n  marks: [\n    Plot.line(explDistribution(eta1, eta2, eta3), {x: \"x\", y: \"y\", stroke: \"steelblue\"}),\n    Plot.ruleY([0])\n  ]\n})"
  },
  {
    "objectID": "reader/exponential-families.html#repeated-sampling-from-exponential-families",
    "href": "reader/exponential-families.html#repeated-sampling-from-exponential-families",
    "title": "Exponential Families",
    "section": "Repeated sampling from exponential families",
    "text": "Repeated sampling from exponential families\nOne of the most important properties of exponential families is that a large sample can be summarized by a low-dimensional statistic. Suppose we observe a vector of observations \\(X = (X_1,\\ldots,X_n)\\) representing an independent and identically distributed (i.i.d.) sample from an exponential family. Let \\(p_\\eta^{(1)\\) denote the density for a single observation:\n\\[\nX_1\\ldots,X_n \\simiid p_\\eta^{(1)}(x) = e^{\\eta'T(x) - A(\\eta)}h(x).\n\\]\nThen the random vector \\(X = (X_1,\\ldots,X_n)\\) follows another closely related exponential family:\n\\[\n\\begin{aligned}\np_\\eta(x)\n&= \\prod_{i=1}^n e^{\\eta'T(x_i) - A(\\eta)}h(x_i)\\\\[7pt]\n&= \\exp\\left\\{\\eta'\\sum_{i=1}^n T(x_i) - nA(\\eta)\\right\\} \\prod_{i=1}^n h(x_i).\n\\end{aligned}\n\\]\nThis new density \\(p_\\eta\\), which governs the distribution of the entire sample, is an exponential family with the same natural parameter as before, sufficient statistic \\(\\sum_i T(X_i)\\), carrier density \\(\\prod_i h(x_i)\\), and log-partition function \\(nA(\\eta)\\).\nFor reasons that will become clearer in the next lecture, it is very significant that the sufficient statistic does not increase in dimension as the sample size grows. This means that the \\(s\\)-dimensional vector \\(\\sum_i T(X_i)\\) is for all intents and purposes a complete summary of the entire sample, no matter how large \\(n\\) is."
  },
  {
    "objectID": "reader/sufficiency.html",
    "href": "reader/sufficiency.html",
    "title": "Sufficiency",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/sufficiency.html#sufficiency",
    "href": "reader/sufficiency.html#sufficiency",
    "title": "Sufficiency",
    "section": "Sufficiency",
    "text": "Sufficiency\nSufficiency is a central concept in statistics that allows us to focus on the essential aspects of the data set while ignoring details that are irrelevant to the inference problem. If \\(X\\sim P_\\theta\\) represents the entire data set, drawn from a model \\(\\cP = \\{P_\\theta:\\; \\theta \\in \\Theta\\}\\), then this lecture will concern the idea of a sufficient statistic \\(T(X)\\) that carries all of the information in the data that can help us learn about \\(\\theta\\).\nA statistic \\(T(X)\\) is any random variable which is a function of the data \\(X\\), and which does not depend on the unknown parameter \\(\\theta\\). We say the statistic \\(T(X)\\) is sufficient for the model \\(\\cP\\) if \\(P_\\theta(X \\mid T)\\) does not depend on \\(\\theta\\). This lecture will be devoted to interpreting this definition and giving examples.\nExample (Independent Bernoulli sequence): We introduced the binomial example from Lecture 2 by telling a story about an investigator who flips a biased coin \\(n\\) times and records the total number of heads, which has a binomial distribution. All of the estimators we considered were functions only of the (binomially-distributed) count of heads.\nBut if the investigator had actually performed this experiment, they would have observed more than just the total number of heads: they would have observed the entire sequence of \\(n\\) heads and tails. If we let \\(X_i\\) denote a binary indicator of whether the \\(i\\)th throw is heads, for \\(i=1,\\ldots,n\\), then we have assumed that these indicators are i.i.d. Bernoulli random variables:\n\\[\nX_1,\\ldots,X_n \\simiid \\text{Bern}(\\theta).\n\\]\nLet \\(T(X) = \\sum_i X_i \\sim \\text{Binom}(n,\\theta)\\) denote the summary statistic that we previously used to represent the entire data set. It is undeniable that we have lost some information by only recording \\(T(X)\\) instead of the entire sequence \\(X = (X_1,\\ldots,X_n)\\). As a result, we might wonder whether we could have improved the estimator by considering all functions of \\(X\\), not just functions of \\(T(X)\\).\nThe answer is that, no, we did not really lose anything by summarizing the data by \\(T(X)\\) because \\(T(X)\\) is sufficient. The joint pmf of the data set \\(X \\in \\{0,1\\}^n\\) (i.e., the density wrt the counting measure on \\(\\{0,1\\}^n\\)) is\n\\[\np_\\theta(x) = \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i} = \\theta^{\\sum_i x_i}(1-\\theta)^{n-\\sum_i x_i}.\n\\]\nNote that this pmf depends only on \\(T(x)\\): it assigns probability \\(\\theta^t (1-\\theta)^{n-t}\\) to every sequence with \\(T(X)=t\\) total heads. As a result, the conditional distribution given \\(T(X)=t\\) should be uniform on all of the \\(\\binom{n}{t}\\) sequences with \\(t\\) heads. We can confirm this by calculating the conditional pmf directly:\n\\[\n\\begin{aligned}\n\\PP_\\theta(X = x \\mid T(X) = t)\n&= \\frac{\\PP_\\theta(X=x, \\sum_i X_i = t)}{\\PP_\\theta(T(X) = t)} \\\\[7pt]\n&= \\frac{\\theta^t (1-\\theta)^{n-t}1\\{\\sum_i x_i = t\\}}{\\theta^t(1-\\theta)^{n-t}\\binom{n}{t}}\\\\[5pt]\n&= \\binom{n}{t}^{-1}1\\{T(x) = t\\}.\n\\end{aligned}\n\\]\nSince the conditional distribution does not depend on \\(\\theta\\), \\(T(X)\\) is sufficient for the model \\(\\cP\\)."
  },
  {
    "objectID": "reader/sufficiency.html#visualization-of-sufficiency-for-two-binomials",
    "href": "reader/sufficiency.html#visualization-of-sufficiency-for-two-binomials",
    "title": "Sufficiency",
    "section": "Visualization of sufficiency for two binomials",
    "text": "Visualization of sufficiency for two binomials\nWe now illustrate the concept of sufficiency with a visualization for a closely related example where now the initial data set is a pair of independent binomial random variables \\(X_1, X_2 \\simiid \\text{Binom}(n, \\theta)\\). We can make a similar calculation to show that \\(T(X) = X_1 + X_2\\) is sufficient:\n\\[\np_\\theta(x) = \\binom{n}{x_1} \\theta^{x_1}(1-\\theta)^{n-x_1} \\cdot \\binom{n}{x_2} \\theta^{x_2}(1-\\theta)^{n-x_2} = \\theta^{T(x)}(1-\\theta)^{2n-T(x)} \\binom{n}{x_1}\\binom{n}{x_2}.\n\\]\n\nhtml`\n&lt;style&gt;\n  .plot-container {\n    display: flex;\n    justify-content: space-around;\n    flex-wrap: wrap;\n  }\n  .plot {\n    flex: 0 1 450px;\n    margin: 10px;\n  }\n  .plot-title {\n    text-align: center;\n    font-size: 20px;\n    font-weight: bold;\n    margin-bottom: 10px;\n  }\n&lt;/style&gt;\n`\n\n\n\n\n\n\n\nfunction binomialProbability(n, k, p) {\n  return combination(n, k) * Math.pow(p, k) * Math.pow(1 - p, n - k);\n}\n\nfunction combination(n, k) {\n  return factorial(n) / (factorial(k) * factorial(n - k));\n}\n\nfunction factorial(n) {\n  if (n === 0 || n === 1) return 1;\n  return n * factorial(n - 1);\n}\n\n// Input parameters\nviewof n = Inputs.range([2, 10], {step: 1, value: 5, label: \"n:\"})\nviewof theta = Inputs.range([0, 1], {step: 0.01, value: 0.5, label: \"θ:\"})\nviewof t = Inputs.range([0, 2 * n], {step: 1, value: n, label: \"T(X) = X₁ + X₂:\"})\n\n// Calculate probabilities\nprobabilities = {\n  let probs = [];\n  for (let x1 = 0; x1 &lt;= n; x1++) {\n    for (let x2 = 0; x2 &lt;= n; x2++) {\n      let prob = binomialProbability(n, x1, theta) * binomialProbability(n, x2, theta);\n      probs.push({x1, x2, prob, sum: x1 + x2});\n    }\n  }\n  return probs;\n}\n\n// Calculate conditional probabilities\nconditionalProbs = {\n  let totalProb = probabilities.filter(p =&gt; p.sum === t).reduce((sum, p) =&gt; sum + p.prob, 0);\n  return Array.from({length: n + 1}, (_, x1) =&gt; {\n    let x2 = t - x1;\n    let prob = (x2 &gt;= 0 && x2 &lt;= n) ? probabilities.find(p =&gt; p.x1 === x1 && p.x2 === x2)?.prob || 0 : 0;\n    return {x1, prob: prob / totalProb};\n  });\n}\n\n// Create plot container\nhtml`&lt;div class=\"plot-container\"&gt;\n  &lt;div class=\"plot\"&gt;\n    &lt;div class=\"plot-title\"&gt;P(X₁, X₂)&lt;/div&gt;\n    ${\n      Plot.plot({\n        width: 450,\n        height: 450,\n        margin: 60,\n        style: {\n          fontSize: \"14px\",\n          fontWeight: \"normal\"\n        },\n        x: {\n          label: \"X₂\",\n          domain: [-1, n],\n          tickFormat: d =&gt; d &gt;= 0 ? d : \"\",\n        },\n        y: {\n          label: \"X₁\",\n          domain: [-1, n],\n          tickFormat: d =&gt; d &gt;= 0 ? d : \"\",\n        },\n        marks: [\n          Plot.dot(probabilities, {\n            x: \"x2\",\n            y: \"x1\",\n            r: d =&gt; Math.sqrt(d.prob) * 200,\n            fill: d =&gt; d.sum === t ? \"red\" : \"black\",\n          }),\n          Plot.text([{x: n/2, y: -1.5}], {\n            text: [\"Distribution of (X₁, X₂)\"],\n            fontSize: 16,\n          }),\n        ]\n      })\n    }\n  &lt;/div&gt;\n  &lt;div class=\"plot\"&gt;\n    &lt;div class=\"plot-title\"&gt;P(X₁ | T(X))&lt;/div&gt;\n    ${\n      Plot.plot({\n        width: 450,\n        height: 450,\n        margin: 60,\n        style: {\n          fontSize: \"14px\",\n          fontWeight: \"normal\"\n        },\n        x: {\n          label: \"X₁\",\n        },\n        y: {\n          label: \"Probability\",\n          domain: [0, Math.max(...conditionalProbs.map(d =&gt; d.prob)) * 1.1],\n        },\n        marks: [\n          Plot.barY(conditionalProbs, {x: \"x1\", y: \"prob\", fill: \"red\"}),\n        ]\n      })\n    }\n  &lt;/div&gt;\n&lt;/div&gt;`"
  },
  {
    "objectID": "reader/sufficiency.html#factorization-theorem",
    "href": "reader/sufficiency.html#factorization-theorem",
    "title": "Sufficiency",
    "section": "Factorization theorem",
    "text": "Factorization theorem\nAs we will see next, we didn’t really need to go to the trouble of calculating the conditional distribution in the first example. Once we noticed that the density depends on \\(x\\) only through \\(T(x)\\), we could have concluded that \\(T(X)\\) was sufficient.\nThe easiest way to verify that a statistic is sufficient is to show that the density \\(p_\\theta\\) factorizes into a part that involves only \\(\\theta\\) and \\(T(x)\\), and a part that involves only \\(h(x)\\).\nFactorization Theorem: Let \\(\\cP\\) be a model having densities \\(p_\\theta(x)\\) with respect to a common dominating measure \\(\\mu\\). Then \\(T(X)\\) is sufficient for \\(\\cP\\) if and only if there exist non-negative functions \\(g_\\theta\\) and \\(h\\) for which\n\\[\np_\\theta(x) = g_\\theta(T(x)) h(x),\n\\]\nfor almost every \\(x\\) under \\(\\mu\\).\nThe “almost every \\(x\\)” qualification means that\n\\[\n\\mu\\left(\\{x:\\; p_\\theta(x) \\neq g_\\theta(T(x))h(x)\\}\\right) = 0.\n\\]\nIt is needed to avoid counterexamples where we mess with the densities on a set of points that the base measure doesn’t assign any mass, which would let us destroy the factorization structure without changing any of the distributions.\nProof (discrete \\(\\cX\\)): The proof is easiest in the discrete case, so that we don’t have to deal with conditioning on measure-zero events and worry about things like Jacobians for change of variables.\nWe’ll assume without loss of generality that \\(\\mu\\) is the counting measure: if \\(\\mu\\) were some other measure, it would have to have a density \\(m\\) with respect to the counting measure and we would just have to carry around \\(m(x)\\) in all of our expressions.\nFirst, assume that there exists a factorization \\(p_\\theta(x) = g_\\theta(T(x)) h(x)\\). Then we have\n\\[\n\\begin{aligned}\n\\PP_\\theta(X = x \\mid T(X) = t)\n&= \\frac{\\PP_\\theta(X = x, T(X) = t)}{\\PP_\\theta(T(X) = t)}\\\\[7pt]\n&= \\frac{g_\\theta(t) h(x) 1\\{T(x) = t\\}}{g_\\theta(t)\\displaystyle\\sum_{z:\\;T(z) = t} h(z)}\\\\[7pt]\n&= \\frac{h(x) 1\\{T(x) = t\\}}{\\displaystyle\\sum_{z:\\;T(z) = t} h(z)},\n\\end{aligned}\n\\]\nwhich we see does not depend on \\(\\theta\\).\nNext consider the opposite direction. If \\(T(X)\\) is sufficient, then we can construct a factorization by writing\n\\[\n\\begin{aligned}\ng_\\theta(t) &= \\PP_\\theta(T(X)=t)\\\\\nh(x) &= \\PP(X = x \\mid T(X) = T(x)),\n\\end{aligned}\n\\] noting that the conditional probability in the definition of \\(h(x)\\) does not depend on \\(\\theta\\) by sufficiency. Then we have\n\\[\n\\PP_\\theta(X = x) = \\PP_\\theta(T(X) = T(x)) \\;\\cdot\\;\\PP_\\theta(X = x \\mid T(X) = T(x)) = g_\\theta(T(x)) h(x),\n\\] so \\(g_\\theta(T(x))h(x)\\) is indeed the pmf \\(p_\\theta(x)\\)."
  },
  {
    "objectID": "reader/sufficiency.html#statement-for-general-cx",
    "href": "reader/sufficiency.html#statement-for-general-cx",
    "title": "Sufficiency",
    "section": "Statement for general \\(\\cX\\)",
    "text": "Statement for general \\(\\cX\\)"
  },
  {
    "objectID": "reader/sufficiency.html#examples",
    "href": "reader/sufficiency.html#examples",
    "title": "Sufficiency",
    "section": "Examples",
    "text": "Examples\nSome initial examples:\nExample: Normal location family Assume we observe an i.i.d. sample from a normal distribution with unit variance and unknown mean:\n\\[\nX_1,\\ldots,X_n \\simiid N(\\theta,1) = \\frac{1}{\\sqrt{2\\pi}} e^{-(x-\\theta)^2/2} = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2 + \\theta x - \\theta^2/2}\n\\]\nThe joint density function for the full data set \\(X = (X_1,\\ldots,X_n)\\) over \\(\\RR^n\\) is\n\\[\n\\begin{aligned}\np_\\theta(x) &= (2\\pi)^{-n/2} \\cdot \\prod_{i=1}^n e^{-x_i^2/2 + \\theta x_i - \\theta^2/2}\\\\[7pt]\n&= \\underbrace{e^{\\theta \\left(\\sum_i x_i\\right) -n\\theta^2/2}}_{g_\\theta\\left(\\sum_i x_i\\right)}\\cdot \\underbrace{\\frac{\\prod_{i=1}^n e^{-x_i^2/2}}{(2\\pi)^{-n/2}}}_{h(x)},\n\\end{aligned}\n\\] which by the factorization theorem shows that \\(\\sum_i X_i\\) is sufficient.\nExample: Poisson family Next assume we observe an i.i.d. sample from a Poisson distribution with unknown mean \\(\\theta\\):\n\\[\nX_1,\\ldots,X_n \\simiid \\text{Pois}(\\theta) = \\frac{\\theta^x e^{-\\theta}}{x!}, \\quad \\text{ for } x = 0,1,\\ldots\n\\] The joint pmf for the full data set \\(X = (X_1,\\ldots,X_n)\\) over \\(\\{0,1,\\ldots\\}^n\\) is\n\\[\n\\begin{aligned}\np_\\theta(x) &= \\prod_{i=1}^n \\frac{\\theta^{x_i}e^{-\\theta}}{x_i!}\\\\[7pt]\n&= \\underbrace{e^{\\theta \\left(\\sum_i x_i\\right) -n\\theta}}_{g_\\theta\\left(\\sum_i x_i\\right)}\\cdot \\underbrace{\\left(\\prod_{i=1}^n x_i!\\right)^{-1}}_{h(x)},\n\\end{aligned}\n\\] Showing once again that \\(T(X) = \\sum_i X_i\\) is sufficient. As we’ll find out in the next lecture, there is a good reason why these calculations turned out so similarly: both of these models have what is called an exponential family structure.\nExample: Uniform location family As a third example, suppose we observe an i.i.d. sample from a uniform distribution on the interval \\([\\theta, \\theta + 1]\\):\n\\[\nX_1,\\ldots, X_n \\simiid U[\\theta, \\theta+1] = 1\\{\\theta \\leq x \\leq \\theta + 1\\}.\n\\] The joint density function for the full data set is\n\\[\np_\\theta(x) = \\prod_{i=1}^n 1\\{\\theta \\leq x \\leq \\theta + 1} = 1\\{\\theta \\leq \\min_i x_i\\} 1\\{\\max_i x_i \\leq \\theta + 1\\},\n\\] so \\(T(X) = (\\min_i X_i, \\max_i X_i)\\) is sufficient."
  },
  {
    "objectID": "reader/sufficiency.html#interpretations-of-sufficiency",
    "href": "reader/sufficiency.html#interpretations-of-sufficiency",
    "title": "Sufficiency",
    "section": "Interpretations of sufficiency",
    "text": "Interpretations of sufficiency"
  },
  {
    "objectID": "reader/sufficiency.html#sufficient-statistics-under-i.i.d.-sampling",
    "href": "reader/sufficiency.html#sufficient-statistics-under-i.i.d.-sampling",
    "title": "Sufficiency",
    "section": "Sufficient statistics under i.i.d. sampling",
    "text": "Sufficient statistics under i.i.d. sampling"
  },
  {
    "objectID": "reader/sufficiency.html#minimal-sufficiency",
    "href": "reader/sufficiency.html#minimal-sufficiency",
    "title": "Sufficiency",
    "section": "Minimal sufficiency",
    "text": "Minimal sufficiency\nConsider again the example with \\(X_1,\\ldots,X_n \\simiid N(\\theta,1)\\). We have shown that \\(\\sum_i X_i\\) is sufficient. It follows that \\(\\overline{X} = \\frac{1}{n}\\sum_i X_i\\) is also sufficient. But we have also shown that the vector of order statistics \\(S(X) = (X_{(1)}, \\ldots, X_{(n)})\\) is another sufficient statistic. Finally, the full data set \\(X\\) is always a sufficient statistic, by definition.\nWhile these are indeed all sufficient statistics, some of them represent more significant compressions of the data than others. We can see this from the fact that \\(\\sum_i X_i\\) and \\(\\overline{X}\\) are recoverable from each other, and can also be recovered from either \\(S(X)\\) or \\(X\\) itself, but \\(S(X)\\) cannot be recovered from \\(\\sum_i X_i\\) or \\(\\overline{X}\\). The full data \\(X\\) cannot be recovered from any of the other three, but any of the other three can be recovered from it. Among these statistics, the first two represent the greatest reduction of the data, and \\(X\\) represents no reduction at all, while \\(S(X)\\) sits in the middle.\nAny statistic from which a sufficient statistic can be recovered is immediately a sufficient statistic:\nProposition: If \\(T(X)\\) is sufficient and \\(T(X) = f(S(X))\\) then \\(S(X)\\) is also sufficient.\nProof: By the factorization theorem we can find densities with \\[\np_\\theta(x) = g_\\theta(T(x))h(x) = (g_\\theta \\circ f)(S(x)) h(x),\n\\] showing that \\(S(X)\\) is sufficient as well.\nWe say that a sufficient statistic is minimal if it can be recovered from any other sufficient statistic. That is, \\(T(X)\\) is minimal sufficient if\n\n\\(T(X)\\) is sufficient, and\nFor any other sufficient statistic \\(S(X)\\), we have \\(T(X) = f(S(X))\\) for some \\(f\\) (almost surely in \\(\\cP\\)).\n\nWe would like to be able to recognize minimal sufficient statistics when we can. For a model \\(\\cP\\) with densities \\(p_\\theta\\), we can say that two data sets \\(x,y\\in \\cX\\) are equivalent (with respect to statistical inference in \\(\\cP\\)) if \\(p_\\theta(x)/p_\\theta(y)\\) does not depend on \\(\\theta\\). We can write \\(x \\equiv_{\\cP} y\\) if this is the case.\nNote that, for any sufficient statistic \\(T(X)\\), values that map to the same output value \\(t\\) must be equivalent: if \\(T(x)=T(y)=t\\), we have \\[\n\\frac{p_\\theta(x)}{p_\\theta(y)} = \\frac{\\PP_\\theta(X = x \\text{ and } T(X) = t)}{\\PP_\\theta(X = y \\text{ and } T(X) = t)} = \\frac{\\PP(X = x \\mid T(X) = t)}{\\PP(X = y \\mid T(X) = t)},\n\\] which does not depend on \\(\\theta\\) by sufficiency of \\(T(X)\\). Thus, for any sufficient statistic we have \\(T(x) = T(y) \\Rightarrow x \\equiv_{\\cP} y\\): the function \\(T\\) is only allowed to collapse values that are equivalent to each other. For a minimal sufficient statistic, this implication goes both ways since \\(T\\) collapses the sample space as much as possible. That is,\nProposition: Assume \\(\\cP\\) has densities \\(p_\\theta(x)\\), and \\(T(X)\\) is any statistic. If we have \\[x \\equiv_{\\cP} y \\iff T(x) = T(y),\\] then \\(T(X)\\) is minimal sufficient.\nProof (discrete \\(\\cX\\)): First, we show \\(T(X)\\) is sufficient. For any \\(x\\) with \\(T(x) = t\\), we have\n\\[\n\\PP_\\theta(X = x \\mid T(X) = t) = \\frac{p_\\theta(x)}{\\sum_{z: T(z) = t} p_\\theta(z)} = \\frac{1}{\\sum_{z:\\; T(z) = t} p_\\theta(z)/p_\\theta(x)},\n\\] which does not depend on \\(\\theta\\) because all of the values \\(z\\) that we sum over in the denominator map to the same \\(t\\), and are therefore equivalent to \\(x\\) by assumption.\nNext, assume \\(S(X)\\) is any other sufficient statistic. If \\(S(x) = S(y) = s\\), then \\(x \\equiv_{\\cP} y\\), by the argument above the theorem, and consequently \\(T(x) = T(y)\\) by assumption. Then we can set \\(f(s) = T(x)\\). For any other value \\(z\\) with \\(S(z) = s\\), we must also have \\(x \\equiv_{\\cP} z\\) so \\(T(z) = T(x) = f(S(z))\\). Since \\(s\\) was arbitrary, we have the result."
  },
  {
    "objectID": "reader/completeness.html",
    "href": "reader/completeness.html",
    "title": "Completeness, Ancillarity, and Basu’s Theorem",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/completeness.html#completeness",
    "href": "reader/completeness.html#completeness",
    "title": "Completeness, Ancillarity, and Basu’s Theorem",
    "section": "1 Completeness",
    "text": "1 Completeness\nAs we have seen, for a given statistical problem we may have many different sufficient statistics, some of which reduce the data more than others. We usually want to look for one that is minimal sufficient, meaning that it strips away as much irrelevant information as possible and only retains the information that is relevant to estimating the parameter.\nIn some cases the minimal sufficient statistic has an additional property called completeness. The definition of completeness is initially counterintuitive, but it has a number of useful implications we will explore throughout the semester.\n\n1.1 Definition of completeness\nA statistic \\(T(X)\\) is complete for a family of distributions \\(\\cP = \\{P_\\theta: \\theta \\in \\Theta\\}\\) if no nontrivial function of \\(T\\) can have expectation zero for every distribution in the family:\n\\[\\EE_\\theta \\,f(T(X)) = 0 \\quad \\forall \\theta \\in \\Theta \\implies f(T) \\eqPas 0\\] ::: callout-note The name for complete statistics comes from a prior notion that \\(\\cP^T = \\{P_\\theta^T:\\; \\theta \\in \\Theta\\}\\) is ``complete’’ as a model if its linear span includes all possible distributions on \\(T(X)\\); see Homework 3. :::\nIf \\(X\\) itself is complete, then the definition immediately implies that there can be at most one unbiased estimator for any estimand: if \\(\\EE_\\theta \\delta_1(X) = \\EE_\\theta \\delta_2(X) = g(\\theta)\\) for all \\(\\theta \\in \\Theta\\), then \\(f(X) = \\delta_1(X) - \\delta_2(X) = 0\\) almost surely. More generally, if \\(T(X)\\) is a complete statistic then there can be at most one unbiased estimator that runs through \\(T\\). We will return to this fact when we discuss unbiased estimation.\nWe will be especially interested in statistics that are both complete and sufficient. If \\(T(X)\\) is complete and sufficient we call it a complete sufficient statistic.\n\n\n\n\n\n\nWarning\n\n\n\nA complete statistic need not be sufficient: the constant “statistic” \\(T(X) \\equiv 0\\) is complete in any model. In general, to show that \\(T(X)\\) is complete sufficient we must establish both properties.\n\n\n\n\n1.2 Examples\nExample 1 (Laplace location family): Let \\(X_1,\\ldots,X_n \\simiid \\text{Lap}(\\theta)\\) for \\(\\theta \\in \\RR\\), and recall that the vector of order statistics \\(S(X) = (X_{(1)},\\ldots,X_{(n)})\\) is a minimal sufficient statistic. Is \\(S(X)\\) complete?\n\n\n\n\n\n\nExpand to see answer\n\n\n\n\n\nNo, \\(S(X)\\) is not complete.\nBoth the sample median \\(\\text{Med}(S)\\) (as defined in Homework 2 Problem XX) and sample mean \\(\\overline{X}(S)\\) can be calculated using \\(S(X)\\) alone, and both are unbiased estimators for \\(\\theta\\). Hence \\(f(S) = \\text{Med}(S) - \\overline{X}(S)\\) has expectation zero, but is not almost surely equal to zero because the median and mean are not equal to each other.\n\n\n\nExample 1 (Uniform scale family): Let \\(X_1, \\ldots, X_n \\simiid U[0, \\theta]\\), for \\(\\theta &gt; 0\\). We showed previously that the maximum \\(T(X) = X_{(n)}\\) is minimal sufficient. Is it complete?\n\n\n\n\n\n\nExpand to see answer\n\n\n\n\n\nYes, \\(T(X)\\) is complete.\nAs we showed previously, the density of \\(T(X)\\) for \\(t &gt; 0\\) is \\[\np_\\theta(t) =  \\frac{nt^{n-1}}{\\theta^n}\\,\\cdot \\;1\\{t \\leq \\theta\\}.\n\\]\nSuppose we could find \\(f(t)\\) such that \\[\n0 = \\EE_\\theta f(T) = \\frac{n}{\\theta^n}\\int_0^\\theta f(t) t^{n-1} \\,dt, \\quad \\text{ for all } \\theta &gt; 0.\n\\] Dividing the last expression by \\(n/\\theta^n\\) and then differentiating with respect to \\(\\theta\\), we obtain \\[\n0 = f(\\theta) \\theta^{n-1}, \\quad \\text{ for all } \\theta &gt; 0,\n\\] hence \\(f \\equiv 0\\).\n\n\n\n\n\n1.3 Full-rank exponential families\nIn the general case where \\(T(X)\\) can take on infinitely many values, it is hard to show completeness because the space of possible counterexample functions \\(f\\) is infinite-dimensional. But there is an important class of examples where we can quickly verify complete sufficiency, as we see next.\nDefinition: Let \\(\\cP = \\{P_\\eta:\\; \\eta \\in \\Xi\\}\\) be an \\(s\\)-parameter exponential family with densities \\[\np_\\eta(x) = e^{\\eta'T(x) - A(\\eta)} h(x),\n\\] with respect to some carrier measure \\(\\mu\\). Assume further that the sufficient statistic \\(T(X)\\) satisfies no affine constraint: that is, there is no \\(\\alpha \\in \\RR\\) and nonzero \\(\\beta \\in \\RR^s\\) with \\(\\beta'T(x) \\eqPas \\alpha\\).\nIf \\(\\Xi\\) contains an open set we say \\(\\cP\\) is full-rank; otherwise we say it is curved.\n\n\n\n\n\n\nNote\n\n\n\nIf \\(T(X)\\) does satisfy a linear constraint, that means \\(\\cP\\) can be defined equivalently as an \\(r\\)-parameter exponential family for some \\(r &lt; s\\). It may be full-rank or curved depending on the parameter space in a lower-dimensional parameterization.\n\n\nTheorem (Complete sufficiency in full-rank exponential families): If \\(\\cP\\) is a full-rank \\(s\\)-parameter exponential family, then \\(T(X)\\) is complete sufficient.\nThe proof is somewhat technical and uses the uniqueness of moment-generating functions.\n\n\n\n\n\n\nExpand for proof\n\n\n\n\n\n\\(T(X)\\) is sufficient by the factorization theorem, so it remains only to prove completeness.\nAssume without loss of generality that \\(0\\) is in the interior of \\(\\Xi\\); otherwise we can reparameterize. Assume also that \\(\\cP\\) is in canonical form, i.e. \\(T(X) = X\\) and \\(p_\\eta(x) = e^{\\eta'x - A(\\eta)}\\); in general we can always reduce to this case by making a sufficiency reduction and taking \\(P_0^T\\) as the carrier measure.\nAny measurable function \\(f\\) can be decomposed as \\(f(x) = f^+(x) - f^-(x)\\) where \\(f^+\\) and \\(f^-\\) are non-negative measurable functions. If \\(\\EE_\\eta f(X) = \\int (f^+-f^-)p_\\eta \\,d\\mu = 0\\) for all \\(\\eta\\in\\Xi\\), then we have \\[\n\\int e^{\\eta'x} f^+(x) \\,d\\mu(x) = \\int e^{\\eta'x} f^-(x) \\,d\\mu(x), \\quad \\text{ for all } \\eta \\in \\Xi.\n\\tag{1}\\] By assumption, \\(\\Xi\\) contains an open neighborhood that includes \\(0\\) on which both integrals are finite. Let \\(c = \\int f^+(x)\\,d\\mu(x) \\geq 0\\).\nIf \\(c&gt;0\\) then we can define the random variables \\(Y^+\\) and \\(Y^-\\) with probability densities \\(f^+(x)/c\\) and \\(f^-(x)/c\\) respectively; then Equation 1 implies that \\(Y^+\\) and \\(Y^-\\) have equal MGFs in a neighborhood of \\(0\\); hence they have the same distribution and their densities must be a.s. equal to each other. But \\(f^+(x)=f^-(x)\\) only when both are zero, so we have \\(f^+, f^- \\eqmuas 0\\).\n\n\n\nThe next figure shows three cases for exponential families with the same sufficient statistic. The set \\(\\Xi_1\\) indicates the full natural parameter space for a generic 2-parameter exponential family, and (A), (B), and (C) denote parameter spaces for three different subfamilies. The subfamily described by the shaded circle (A) is a full-rank exponential family, because it contains an open set. The subfamily described by the curve (B) is a typical example of a curved family, because it does not contain an open set. The subfamily described by the line segment (C) is technically curved according to the definition above, but we could view it as a full-rank \\(1\\)-parameter exponential family after reparameterizing it.\n\n\n\nThis figure shows three cases:\n\n\n\n\n1.4 Complete sufficient statistics are minimal\nA second convenient property of completeness is that complete sufficient statistics are always minimal sufficient.\nTheorem: If \\(T(X)\\) is complete sufficient for the family \\(\\mathcal{P}\\), then \\(T(X)\\) is minimal sufficient for \\(\\cP\\).\nThe way that we usually use completeness in proofs is to show that two quantities are almost surely equal by showing that they have the same expectation. The next proof is an example.\nProof: Let \\(S(X)\\) represent any minimal sufficient statistic, and define the conditional expectation of \\(T\\) given \\(S\\): \\[\n\\overline{T}(S(X)) = \\EE[T(X) \\mid S(X)].\n\\] Note that this conditional expectation does not depend on the parameter \\(\\theta\\), because \\(S(X)\\) is sufficient, so \\(\\overline{T}\\) is a valid statistic. If we can show that \\(\\overline{T} \\eqas T(X)\\), that means we can calculate \\(T(X)\\) from \\(S(X)\\), so \\(T(X)\\) is also minimal sufficient.\nBecause \\(S(X)\\) is minimal sufficient, we can write it as \\(f(T(X))\\) for some function \\(f\\), and use \\(f\\) to define a function \\(g\\) giving the difference between \\(T\\) and \\(\\overline{T}\\): \\[\ng(t) = t - \\overline{T}(f(t)).\n\\] The expectation of \\(g(T)\\) is always zero, because \\[\n\\begin{aligned}\n\\EE_\\theta\\; g(T(X)) &= \\EE_\\theta\\; T(X) - \\EE_\\theta\\; \\overline{T}(S(X)) \\\\[5pt]\n&= \\EE_\\theta\\; T(X) - \\EE_\\theta\\left[\\EE[T(X) \\mid S(X)]\\right]\\\\\n&= 0.\n\\end{aligned}\n\\] As a result, \\(g(T) \\eqas 0\\) and hence \\(T \\eqas \\overline{T}\\), as desired."
  },
  {
    "objectID": "reader/completeness.html#ancillarity",
    "href": "reader/completeness.html#ancillarity",
    "title": "Completeness, Ancillarity, and Basu’s Theorem",
    "section": "2 Ancillarity",
    "text": "2 Ancillarity\nWe’ve spent a lot of time discussing sufficient statistics, which are statistics that carry all of the information about \\(\\theta\\). Our next definition describes a type of statistic that carry no information about the parameter.\nDefinition: We say \\(V(X)\\) is ancillary for the model \\(\\cP = \\{P_\\theta: \\theta \\in \\Theta\\}\\) if its distribution does not depend on \\(\\theta\\).\nJust as the sufficiency principle tells us that our inference procedures should depend only on information from sufficient statistics, an analogous principle suggests in effect that procedures should depend as little as possible on ancillary statistics. Specifically, it recommends treating \\(V(X)\\) as a fixed value and evaluating the rest of the data set according to its distribution conditional on \\(V(X)\\):\nConditionality Principle: If \\(V(X)\\) is ancillary, then all inference should be conditional on \\(V(X)\\).\nIt may not be immediately clear why conditioning on \\(V(X)\\) “removes” it from the problem, but we will return to the idea of conditional inference during our unit on hypothesis testing and interval estimation, where it will play an important role."
  },
  {
    "objectID": "reader/completeness.html#basus-theorem",
    "href": "reader/completeness.html#basus-theorem",
    "title": "Completeness, Ancillarity, and Basu’s Theorem",
    "section": "3 Basu’s Theorem",
    "text": "3 Basu’s Theorem\nBasu’s Theorem gives us a simple way to prove that statistics are independent of one another using the definitions introduced above.\nTheorem (Basu): If \\(T(X)\\) is complete sufficient and \\(V(X)\\) ancillary for the model \\(\\cP\\), then \\(V(X) \\indep T(X)\\) for all \\(\\theta \\in \\Theta\\).\nAgain, for this proof our strategy will be to show that two quantities are almost surely equal to each other, by showing that they have the same expectation for all \\(\\theta\\).\nProof: Define the following two quantities representing the marginal and conditional probabilities that \\(V\\) falls into a generic set \\(A\\). \\[\n\\begin{aligned}\np_A &= \\PP(V \\in A)\\\\[5pt]\nq_A(T(X)) &= \\PP(V \\in A \\mid T(X))\n\\end{aligned}\n\\] Note that \\(p_A\\) does not depend on \\(\\theta\\) by ancillarity of \\(V\\), while \\(q_A\\) does not depend on \\(\\theta\\) by sufficiency of \\(T\\).\nThe expectation of their difference is \\[\n\\EE_\\theta\\left[q_A(T) - p_A\\right] = p_A - p_A = 0, \\quad \\text{ for all } \\theta.\n\\] By completeness of \\(T\\), this implies that \\(q_A(T) \\eqas p_A\\): the conditional probability equals the marginal probability. Hence, for any \\(B\\), we have \\[\n\\begin{aligned}\n\\PP_\\theta(V \\in A, T \\in B) &= \\int q_A(t) 1\\{t \\in B\\}\\,dP_\\theta^T(t)\\\\\n&= \\int p_A 1\\{t \\in B\\}\\,dP_\\theta^T(t)\\\\\n&= \\PP_\\theta(V \\in A) \\PP_\\theta(T \\in B).\n\\end{aligned}\n\\]\n\n3.1 Using Basu’s Theorem\nBasu’s Theorem can be helpful in proving independence. To use it, remember that the hypotheses of the theorem (sufficiency, completeness, and ancillarity) are all defined with respect to a family \\(\\cP\\). The conclusion, however, is defined with respect to individual distributions. As a result, when we apply the theorem we can often benefit from being a little clever about how to define \\(\\cP\\). The following example should make this clear:\nExample (Independence of sample mean and sample variance for Gaussian): Assume \\(X_1,\\ldots,X_n \\simiid \\cN(\\mu, \\sigma^2)\\) for \\(\\mu \\in \\RR\\) and \\(\\sigma^2 &gt; 0\\). Define the sample mean and sample variance as \\[\n\\begin{aligned}\n\\overline{X} &= \\frac{1}{n}\\sum_{i=1}^n X_i\\\\[5pt]\nS^2 &= \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\overline{X})^2\n\\end{aligned}\n\\] We would like to show \\(\\overline{X} \\indep S^2\\).\nInitially the approach of applying Basu’s Theorem appears hopeless because, in the model with \\(\\mu\\) and \\(\\sigma^2\\) unknown, neither of these two statistics is ancillary or sufficient. However, we can nevertheless apply Basu’s Theorem if we are just a bit more clever:\n\n\n\n\n\n\nExpand for answer\n\n\n\n\n\nConsider the model \\(\\cP\\) with known \\(\\sigma^2 &gt; 0\\) and unknown \\(\\mu\\in \\RR\\). This \\(\\cP\\) is a one-parameter full-rank exponential family with complete sufficient statistic \\(\\overline{X}\\). Moreover, \\(S^2\\) is ancillary, since we can write \\[\nS^2 = \\sum_{i=1}^n (Z_i - \\overline{Z})^2, \\quad \\text{ for } Z_i = X_i - \\mu.\n\\] Because the distribution of \\(Z_1,\\ldots,Z_n \\simiid N(0,\\sigma^2)\\) is known, it follows that the distribution of \\(S^2\\) is known as well (specifically, \\(S^2/\\sigma^2\\) is a \\(\\chi^2\\) random variable with \\(n-1\\) degrees of freedom). Since \\(\\mu\\) is the only unknown parameter, \\(S^2\\) is therefore ancillary in \\(\\cP\\). Applying Basu’s theorem, we have \\(\\overline{X} \\indep S^2\\) for any \\(\\mu \\in \\RR\\). But \\(\\sigma^2\\) was arbitrary, so we have the result for all \\(\\mu\\) and \\(\\sigma^2\\)."
  },
  {
    "objectID": "reader/score-fisher.html",
    "href": "reader/score-fisher.html",
    "title": "Score Function and Fisher Information",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/score-fisher.html#outline",
    "href": "reader/score-fisher.html#outline",
    "title": "Score Function and Fisher Information",
    "section": "1 Outline",
    "text": "1 Outline\n\nScore function\nFisher information\nCramér-Rao Lower Bound\nExamples"
  },
  {
    "objectID": "reader/score-fisher.html#motivation-tangent-family",
    "href": "reader/score-fisher.html#motivation-tangent-family",
    "title": "Score Function and Fisher Information",
    "section": "2 Motivation: Tangent Family",
    "text": "2 Motivation: Tangent Family\nConsider a family of densities:\n\\[p(x; \\theta) = e^{\\theta'T(x) - A(\\theta)}h(x)\\]\nwhere \\(\\theta \\in \\RR^d\\) and \\(A(\\theta) = \\log \\int e^{\\theta'T(x)}h(x)dx\\).\nFor this family:\n\n\\(T(X)\\) is complete sufficient\n\\(T(X)\\) is minimal\n\\(\\PP_\\theta(T(X) = t) = e^{\\theta't - A(\\theta)}\\)\n\\(\\EE_\\theta[T(X)] = A'(\\theta)\\)\n\nLet \\(\\theta_0 \\in \\RR^d\\) be fixed. Define the tangent family\n\\[q(x; t) = e^{t'\\nabla l_{\\theta_0}(x) - k(t)}p_{\\theta_0}(x)\\]\nwhere \\(k(t) = \\log \\int e^{t'\\nabla l_{\\theta_0}(x)}p_{\\theta_0}(x)dx\\).\nThen \\(\\nabla l_{\\theta_0}(X)\\) is complete sufficient for the tangent family at \\(\\theta_0\\).\nThis is called the Score function."
  },
  {
    "objectID": "reader/score-fisher.html#score-function",
    "href": "reader/score-fisher.html#score-function",
    "title": "Score Function and Fisher Information",
    "section": "3 Score Function",
    "text": "3 Score Function\nAssume a family \\(\\cP\\) has densities \\(p_\\theta\\) with respect to a measure \\(\\mu\\), for \\(\\theta \\in \\Theta \\subseteq \\RR^d\\). Assume additionally that these densities have common support: that \\(\\{x: p_\\theta(x) &gt; 0\\}\\) is the same for all \\(\\theta\\).\nRecall the log-likelihood is \\(l(\\theta;X) = \\log p_\\theta(X)\\) (thought of as a random function of \\(\\theta\\))\nDefinition: The Score function is \\(\\nabla l_\\theta(X)\\).\nIt plays a key role in many areas of statistics, especially in asymptotics. We can think of it as a “local complete sufficient statistic.” For \\(\\eta \\approx 0\\), and \\(\\theta_0 \\in \\Theta^\\circ\\), we have\n\\[p_{\\theta_0+\\eta}(x) = e^{\\ell(\\theta_0 + \\eta; x)} \\approx e^{\\eta'\\nabla \\ell(\\theta_0;x)}p_{\\theta_0}(x).\\]"
  },
  {
    "objectID": "reader/score-fisher.html#differential-identities-and-the-fisher-information",
    "href": "reader/score-fisher.html#differential-identities-and-the-fisher-information",
    "title": "Score Function and Fisher Information",
    "section": "4 Differential Identities and the Fisher Information",
    "text": "4 Differential Identities and the Fisher Information\nAssuming enough regularity, we can arrive at some important differential identities by differentiating both sides of the equation\n\\[1 = \\int_\\cX e^{\\ell(\\theta;x)}\\,d\\mu(x).\\]\nDifferentiating both sides with respect to \\(\\theta_j\\), we obtain \\[0 = \\int_\\cX \\frac{\\partial}{\\partial \\theta_j} \\ell(\\theta; x) e^{\\ell(\\theta; x)}\\,d\\mu(x) = \\EE_\\theta \\left[\\frac{\\partial}{\\partial\\theta_j}\\ell(\\theta;X)\\right].\\] Collecting these identities into a vector, we obtain \\[\\EE_\\theta [\\nabla \\ell(\\theta; X)] = 0.\\] Importantly, note that this identity only holds if the \\(\\theta\\) in the subscript (defining the distribution with respect to which the expectation is taken) matches the \\(\\theta\\) at which the gradient is being evaluated.\nIf we differentiate the identity a second time with respect to \\(\\theta_k\\), we obtain \\[0 = \\int_\\cX \\left(\\frac{\\partial^2\\ell}{\\partial \\theta_j\\partial\\theta_k} + \\frac{\\partial \\ell}{\\partial \\theta_j}\\frac{\\partial \\ell}{\\partial\\theta_k}\\right) e^{\\ell}\\,d\\mu = \\EE_\\theta\\left[\\frac{\\partial^2\\ell}{\\partial \\theta_j\\partial\\theta_k}\\right] + \\EE_\\theta\\left[\\frac{\\partial \\ell}{\\partial \\theta_j}\\frac{\\partial \\ell}{\\partial \\theta_k}\\right]\n%= \\EE_\\theta\\left[\\frac{\\partial^2\\ell}{\\partial \\theta_j\\partial\\theta_k}\\right] + \\Cov_\\theta\\left(\\frac{\\partial \\ell}{\\partial \\theta_j},\\frac{\\partial \\ell}{\\partial \\theta_k}\\right).\n\\] Again collecting these identities into a matrix, and noting that \\[\\EE_\\theta\\left[\\frac{\\partial \\ell}{\\partial \\theta_j}\\frac{\\partial \\ell}{\\partial \\theta_k}\\right] = \\Cov_\\theta\\left(\\frac{\\partial \\ell}{\\partial \\theta_j},\\frac{\\partial \\ell}{\\partial \\theta_k}\\right),\\] we obtain \\[\\Var_\\theta\\left(\\nabla\\ell(\\theta;X)\\right) = \\EE_\\theta\\left[-\\nabla^2\\ell(\\theta;X)\\right],\\] again with the important observation that the \\(\\theta\\) in both subscripts must match the \\(\\theta\\) where the first and second derivatives are evaluated.\nThe left-hand side of the last equation, the variance of the score, is called the Fisher Information matrix \\[ J(\\theta) := \\Var_\\theta(\\nabla\\ell(\\theta;X)). \\] Note \\(J(\\theta)\\) is always positive semidefinite. It is possible to extend this definition to certain models where \\(\\ell(\\theta;x)\\) is not differentiable with respect to \\(\\theta\\), such as the Laplace location family. However we will not explore these generalizations."
  },
  {
    "objectID": "reader/score-fisher.html#cramér-rao-lower-bound",
    "href": "reader/score-fisher.html#cramér-rao-lower-bound",
    "title": "Score Function and Fisher Information",
    "section": "5 Cramér-Rao Lower Bound",
    "text": "5 Cramér-Rao Lower Bound\nLet \\(\\delta(X)\\) be any real-valued statistic. Let \\(g(\\theta) = \\EE_\\theta[\\delta]\\), so \\(\\delta\\) is an unbiased estimator for \\(g(\\theta)\\). If we repeat the idea of differentiating \\(g(\\theta) = \\int \\delta(x) e^{\\ell(\\theta;x)}\\,d\\mu(x)\\) with respect to \\(\\theta_j\\) for each \\(j\\), and collect the resulting partial derivatives into a vector, we obtain\n\\[\\nabla g(\\theta) = \\int \\delta(x) \\nabla \\ell(\\theta;x) e^{\\ell(\\theta;x)}\\,d\\mu(x) = \\EE_\\theta\\left[\\delta(X) \\nabla\\ell(\\theta;X)\\right] = \\Cov_\\theta\\left(\\delta(X), \\nabla\\ell(\\theta;X)\\right).\\] Combining these results with the Cauchy-Schwarz inequality gives us the Cramér-Rao Lower Bound, also known as the Information lower bound. For a single parameter (\\(d=1\\)), we have \\[\\Var_\\theta(\\delta(X)) \\cdot \\Var_\\theta(\\dot{\\ell}(\\theta;X)) \\geq \\Cov_\\theta(\\delta(X), \\dot{\\ell}(\\theta; X))^2, \\] so after rearranging terms and applying identities, \\[\\Var_\\theta(\\delta(X)) \\geq \\frac{\\dot{g}(\\theta)^2}{J(\\theta)}.\\]\nFor the multivariate case (\\(d&gt;1\\)), we have more generally \\[ \\Var_\\theta(\\delta(X) \\geq \\nabla g(\\theta)'J(\\theta)^{-1}\\nabla g(\\theta).\\] The interpretation of this identity is that no unbiased estimator for \\(g(\\theta)\\) can have variance smaller than \\(\\nabla g(\\theta)'J(\\theta)^{-1}\\nabla g(\\theta)\\). In particular, if \\(g(\\theta) = \\theta_j\\), no estimator can have variance smaller than \\((J(\\theta)^{-1})_{jj}\\).\n\\[\\Var_\\theta(\\delta) \\geq \\Var_\\theta(\\delta(X)) \\Cov_\\theta(\\delta, \\nabla l_\\theta(X))I(\\theta)^{-1}\\Cov_\\theta(\\delta, \\nabla l_\\theta(X))' = g'(\\theta)I(\\theta)^{-1}g'(\\theta)'\\]\n\n\n\n\n\n\nExpand to see proof\n\n\n\n\n\nFor any \\(a \\in \\RR^d\\), we can write \\[\\begin{aligned}\n\\Var_\\theta(\\delta(X)) \\cdot a'J(\\theta)a &= \\Var_\\theta(\\delta)\\Var_\\theta(a'\\nabla\\ell(\\theta;X))\\\\\n&\\geq \\Cov_\\theta(\\delta(X), a'\\nabla\\ell(\\theta;X))^2\\\\\n&= \\left(a'\\nabla \\Cov_\\theta(\\delta, \\nabla\\ell(\\theta))\\right)^2\\\\\n&= (a'\\nabla g(\\theta))^2.\n\\end{aligned}\\]\nThus we obtain for all nonzero \\(a \\in \\RR^d\\),\n\\[\\Var_\\theta(\\delta(X)) \\geq \\frac{(a'\\nabla g(\\theta))^2}{a'J(\\theta)a}.\\]\nWe obtain the result by optimizing the bound, with \\(a = J(\\theta)^{-1}\\nabla g(\\theta)\\) (show this as an exercise)."
  },
  {
    "objectID": "reader/score-fisher.html#examples",
    "href": "reader/score-fisher.html#examples",
    "title": "Score Function and Fisher Information",
    "section": "6 Examples",
    "text": "6 Examples\nExample: i.i.d. sample\nAssume \\(X_1, \\ldots, X_n \\simiid p_\\theta^{(1)}(x)\\), for \\(\\theta \\in \\Theta \\subseteq \\RR^d\\).\nAssume additionally that \\(p_\\theta^{(1)}\\) is “regular:” it has common support, and finite derivative w.r.t. \\(\\theta\\).\nThen the full data density is \\(p_\\theta(x) = \\prod_i p_\\theta^{(1)}(x_i)\\).\nDefine the single-sample log-likelihood \\(\\ell_1(\\theta;x_i) = \\log p_\\theta^{(1)}(x_i)\\); then we have \\(\\ell(\\theta;x) = \\sum_i \\ell_1(\\theta;x_i)\\).\nThen the Fisher information for the full sample is \\[J(\\theta) = \\Var_\\theta(\\nabla \\ell(\\theta; X)) = \\sum_{i=1}^n \\Var_\\theta(\\nabla \\ell_1(\\theta; X_i)) = n J_1(\\theta),\\] where \\(J_1(\\theta) = \\Var_\\theta(\\nabla\\ell(\\theta; X_1))\\) is the Fisher information for a single sample.\nAs a result, we see that the Information bound scales like \\(n^{-1}\\) for regular families; in other words, the standard deviation of an estimator should scale roughly like \\(1/\\sqrt{n}\\).\nExample: exponential family\nSuppose we have an exponential family of the form \\[ p_\\eta(x) = e^{\\eta'T(x) - A(\\eta)} h(x).\\]\nThe log-likelihood is \\(\\ell(\\eta;X) = \\eta'T(X) - A(\\eta) + \\log h(X)\\), and its gradient (the score) is \\[\\nabla \\ell(\\eta;X) = T(X) - \\nabla A(\\eta) = T(X) - \\EE_\\eta T(X).\\] Since \\(\\EE_\\eta T(X)\\) is nonrandom, the variance is \\[ J(\\eta) = \\Var_\\eta (T(X)) = \\nabla^2 A(\\eta).\\]\nWe could alternatively derive the Fisher information from taking a second derivative with respect to \\(\\eta\\), giving \\[ \\nabla^2\\ell(\\eta;X) = -\\nabla^2 A(\\eta),\\] which is deterministically equal to \\(-\\Var_\\eta(T(X))\\), so we have confirmed the identity \\(J(\\eta) = -\\EE_\\eta[\\nabla^2 \\ell(\\eta;X)]\\).\nExample: Curved exponential family\nNext, consider a curved version of the previous family, parameterized by \\(\\theta \\in \\RR\\): \\[p_\\theta(x) = e^{\\eta(\\theta)'T(x) - B(\\theta)}h(x),\\quad \\text{ with } B(\\theta) = A(\\eta(\\theta))\\] Again, the log-likelihood is \\[\\ell(\\theta;X) = \\eta(\\theta)'T(x) - B(\\theta)  + \\log h(x),\\] and its first derivative is \\[\\begin{aligned}\n\\dot{\\ell}(\\theta;X) &= \\dot{\\eta}(\\theta)'T(X) - \\dot{\\eta}(\\theta)'\\nabla_\\eta A(\\eta(\\theta))\\\\\n&= \\dot{\\eta}(\\theta) '\\left(T(X) - \\nabla_\\eta A(\\eta(\\theta))\\right)\\\\\n&= \\dot{\\eta}(\\theta)'(T(X) - \\EE_\\theta T(X)).\\end{aligned}\\]\nAs a result, the Fisher information is \\[J(\\theta) = \\Var_\\theta(\\dot{\\eta}(\\theta)'T(X)) =  \\dot{\\eta}(\\theta)'\\Var_\\theta(T(X))\\dot{\\eta}(\\theta).\\] Note in this model \\(\\dot{\\eta}'T(X)\\) is a “local complete sufficient statistic” for the model near \\(\\theta\\)."
  },
  {
    "objectID": "reader/score-fisher.html#efficiency",
    "href": "reader/score-fisher.html#efficiency",
    "title": "Score Function and Fisher Information",
    "section": "7 Efficiency",
    "text": "7 Efficiency\nThe CRLB is not necessarily attainable.\nWe define the efficiency of an unbiased estimator as:\n\\[\\text{eff}_\\delta(\\theta) = \\frac{\\text{CRLB}(\\theta)}{\\Var_\\theta(\\delta)} \\leq 1,\\]\nWe say \\(\\delta(X)\\) is efficient if \\(\\text{eff}_\\delta(\\theta) = 1\\) for all \\(\\theta\\).\nFor \\(g(\\theta)=\\theta\\in \\RR\\), the efficiency depends on how correlated \\(\\delta(X)\\) is with the score: \\[\\begin{aligned}\\text{eff}_\\delta(\\theta) &= \\frac{\\Cov_\\theta(\\delta(X), \\dot{\\ell}(\\theta;X))^2}{\\Var_\\theta(\\delta(X)) \\cdot \\Var_\\theta(\\dot{\\ell}(\\theta;X))}\\\\\n&= \\Corr_\\theta(\\delta,\\dot{\\ell}(\\theta))^2\n\\end{aligned}\\]\nThus, an efficient estimator for \\(\\theta\\) is one that is perfectly correlated with the score. This is rarely achieved in finite samples, but we can often approach it asymptotically as \\(n \\to \\infty\\)."
  },
  {
    "objectID": "reader/introduction.html",
    "href": "reader/introduction.html",
    "title": "Course introduction",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/introduction.html#about-stat-210a",
    "href": "reader/introduction.html#about-stat-210a",
    "title": "Course introduction",
    "section": "About Stat 210A",
    "text": "About Stat 210A\n\nWhat is the theory of statistics?\nStatistics is the study of methods that use data to understand the world. Statistical methods are used throughout the natural and social sciences, in machine learning and artificial intelligence, and in engineering. Despite the ubiquitous use of statistics, its practitioners are perpetually accused of not actually understanding what they are doing. Statistics theory is, broadly speaking, about trying to understand what we are doing when we use statistical methods.\nWhile there are many possible ways to analyze data, most (but certainly not all) statistical methods are based on statistical modeling: treating the data as a realization of some random data-generating process with attributes, usually called parameters, that are a priori unknown. The goal of the analyst, then, is to use the data to draw accurate inferences about these parameters and/or to make accurate predictions about future data. If the modeling has been done well (a very big “if”) then these unknown parameters will correspond well to whatever real-world questions initially motivated the analysis. Applied statistics courses like Stat 215A and B concern questions about how to ensure that the statistical modeling exercise successfully captures something interesting about reality.\nIn this course we will instead focus on how the analyst can use the data most effectively within the context of a given mathematical setup. We will discuss the structure of statistical models, how to evaluate the quality of a statistical method, how to design good methods for new settings, and the philosophy of Bayesian vs frequentist modeling frameworks. We will cover estimation, confidence intervals, and hypothesis testing, in parametric and nonparametric methods, in finite samples and asymptotic regimes.\n\n\nRelationship of Stat 210A to other Berkeley courses\nStat 210A focuses on classical statistical contexts: either inference in finite samples, or in fixed-dimensional asymptotic regimes. Stat 210B (for which 210A is a prerequisite) is more technical and covers topics like empirical process theory and high-dimensional statistics.\nBerkeley’s graduate course on Statistical Learning Theory (CS 281A / Stat 241A) is also very popular and has some overlap in its topics. Roughly speaking, it is more tilted toward “machine learning”: it spends more time on topics in predictive modeling (i.e. classification and regression, which are covered in Stat 215A), optimization, and signal processing, but spends less time on inferential questions and (I believe) does not cover topics like hypothesis testing, confidence intervals, and causal inference. Both courses cover estimation and exponential families."
  },
  {
    "objectID": "reader/introduction.html#deductive-vs-inductive-reasoning",
    "href": "reader/introduction.html#deductive-vs-inductive-reasoning",
    "title": "Course introduction",
    "section": "Deductive vs inductive reasoning",
    "text": "Deductive vs inductive reasoning\n\nDeductive reasoning\nMost mathematics courses are entirely concerned with deductive reasoning: drawing conclusions that follow logically from premises. For example:\n\nAll real, symmetric matrices have real eigenvalues.\n\\(A\\) is a real, symmetric matrix.\nTherefore, \\(A\\) has real eigenvalues.\n\nDeductive reasoning comes up in everyday life, for example\n\nNo one in my daughter’s preschool class has a nut allergy.\nZoe is in my daughter’s preschool class.\nTherefore, Zoe is not allergic to peanuts.\n\nThis type of argument is risk-free in the sense that, as long as the premises are true, the conclusions must hold. Of course, the premises could be false: I might be confusing my neighbor Zoe with a different Zoe who is in my daughter’s class. But that is the only way my conclusion could be wrong.\nDeductive arguments can involve statements about probability:\n\nThis die has six faces labeled 1, 2, 3, 4, 5, and 6.\nIf I roll it, it is equally likely to land on any face.\nTherefore, the chance of rolling a 4 is exactly 1/6.\n\nA probability course like Stat 205A is about statements like this.\n\n\nInductive reasoning\nStatistics, on the other hand, is the mathematical science of inductive reasoning: reasoning from observations to make general claims about the world. Unlike deductive reasoning, such arguments are inherently risky: the conclusions we draw can be false even when the premises are correct.\n\n\n\n\n\n\nCaution\n\n\n\nNote that inductive proofs in mathematics are not an example of inductive reasoning as we mean it here. Inductive proofs are really examples of deductive reasoning because they provide a logically valid argument (i.e. the inductive step) for extending the conclusion to the entire class of objects under study.\n\n\nFor example:\n\nI ate a blueberry from the free sample tray at the supermarket.\nIt was ripe and delicious.\nTherefore, if I buy a carton of blueberries, they will probably be ripe and delicious.\n\nHere we have added the weasel word “probably” not to convey a rigorous quantitative statement about probability, but just to informally convey some uncertainty about the conclusion.\nThis example would be more persuasive if we had taken a sample randomly from the carton we planned on buying:\n\nI ate five blueberries at random from the carton I intended to buy.\nThey were all ripe and delicious.\nTherefore, if I buy the carton, the rest of the blueberries will probably be ripe and delicious.\n\nOf course, we could still always be wrong: maybe there were only five good blueberries in the whole carton and we just happened to take those. But that is not very likely.\nScientists very often reason inductively. For example:\n\nWater at 1atm of pressure has been observed to boil at 100°C every time it has been measured in the laboratory.\nTherefore, water at 1atm of pressure probably always boils at 100°C.\n\nInductive reasoning is the basis of all of the empirical sciences.\nWe can also make inductive statements about probability:\n\nI flipped this penny 1000 times and got 502 heads.\nTherefore, it probably has about a 50% chance of landing heads.\n\nFor now, we’ll assume we know what it means for a penny to have a 50% chance of landing heads; something like: it’s physical properties give it an equal chance of landing heads or tails (and a negligible chance of landing on its side or flying off into space). Generally, there is some controversy among different camps of philosophers and statisticians about what probability means, but not too much when it comes to coin flips. We’ll discuss this more later in the semester."
  },
  {
    "objectID": "reader/introduction.html#the-problem-of-induction",
    "href": "reader/introduction.html#the-problem-of-induction",
    "title": "Course introduction",
    "section": "The problem of induction",
    "text": "The problem of induction\n\nHume’s problem of induction\nUnfortunately, inductive reasoning is not valid in the sense meant by logicians or mathematicians. It doesn’t matter how many times I’ve seen real symmetric matrices that had real eigenvalues. Without a proof, I can’t make the general claim. There are entertaining examples of patterns being unexpectedly violated in math, such as the Borwein Integral:\n\\[\n\\begin{aligned}\n\\int_0^\\infty \\frac{\\sin x}{x}\\,dx &= \\frac{\\pi}{2}\\\\[10pt]\n\\int_0^\\infty \\frac{\\sin x}{x}\\, \\frac{\\sin(x/3)}{x/3}\\,dx &= \\frac{\\pi}{2}\\\\[10pt]\n\\int_0^\\infty \\frac{\\sin x}{x}\\, \\frac{\\sin(x/3)}{x/3}\\, \\frac{\\sin(x/5)}{x/5}\\,dx &= \\frac{\\pi}{2}\\\\[10pt]\n&\\vdots\\\\[10pt]\n\\int_0^\\infty \\frac{\\sin x}{x}\\, \\frac{\\sin(x/3)}{x/3}\\,\\cdots\\, \\frac{\\sin(x/13)}{x/13}\\,dx &= \\frac{\\pi}{2}\\\\[10pt]\n\\int_0^\\infty \\frac{\\sin x}{x}\\, \\frac{\\sin(x/3)}{x/3}\\,\\cdots\\, \\frac{\\sin(x/15)}{x/15}\\,dx &= \\frac{\\pi}{2} - 2.31 \\times 10^{-11}.\n\\end{aligned}\n\\]\nBertrand Russell also warned about how inductive inference can go awry in real life:\n\nDomestic animals expect food when they see the person who usually feeds them. We know that all these rather crude expectations of uniformity are liable to be misleading. The man who has fed the chicken every day throughout its life at last wrings its neck instead, showing that more refined views as to the uniformity of nature would have been useful to the chicken.\n\nDavid Hume’s work A Treatise of Human Nature (1739) first proposed the problem of induction, namely that inductive reasoning presumes — seemingly without justification — that yet-to-be-observed cases will be similar to observed cases. This presumption is sometimes called the uniformity principle, and it is difficult to see how we can justify it. We can’t justify it through a direct logical argument, because it’s not logically justified. We could justify it by past experience — for example, at a low enough level, physical properties of the world generally seem to be uniform across space and time — but that argument is circular! Just because the future has been like the past in the past, doesn’t mean that it will be like the past in the future.\nHume allowed that people have to reason inductively all the time, but he called it a “custom” or “habit” and challenged philosophers to justify it. Now almost 300 years later, there does not seem to have been a fully satisfactory answer; most philosophers of science (like Karl Popper, for example) admit that inductive reasoning is fallible, but think there are reasonable ways for scientists to deal with this.\n\n\nStatistical evasions of the problem of induction\nWe seem to be in trouble if we are trying to build a mathematical science of inductive reasoning, when the first thing we know about induction is that it is not mathematically valid. Statisticians have two main ways of evading this problem, which lead to the two main mathematical frameworks for statistical inference:\nEvasion 1: Bayesian reasoning. Bayesians respond to Hume that, whatever our a priori beliefs are about the world, we at least know how to update them in the light of experience using the mathematics of conditional probability. Our prior beliefs may not ultimately be justified, but there is only one rational way to update them (note this is somewhat disputed). We may hope that after enough experience they will “wash out” and observers with different prior beliefs will eventually converge in their beliefs after seeing enough data. We will study Bayesian statistics in a few weeks.\nEvasion 2: Inductive behavior (frequentist statistics). Another way of evading the problem is to design methods for inference whose fallibility can be quantified. As long as certain assumptions hold concerning the conditions under which the data were collected, we may be able to come up with methods that we can mathematically prove give correct conclusions with high probability."
  },
  {
    "objectID": "reader/introduction.html#statistical-evasions-in-practice-coin-flipping",
    "href": "reader/introduction.html#statistical-evasions-in-practice-coin-flipping",
    "title": "Course introduction",
    "section": "Statistical evasions in practice: coin flipping",
    "text": "Statistical evasions in practice: coin flipping\n\nAre coins really fair?\nAlthough physical randomizers like coins and dice seem to be the firmest ground on which we can build a theory of probability, recent work has made the surprising finding that most human flippers have a somewhat greater than 50% chance of seeing their coin land on the same side as it started, due to the physics of rotating objects. This was first hypothesized in a theoretical article by Diaconis, Holmes, and Montgomery in 2007, and confirmed in a large experiment by Bartos et al. (2023), in which 48 human coin flippers collectively flipped \\(n=350,757\\) coins — finding, indeed, that \\(178,079\\) (\\(50.77\\%\\)) of the coins landed on the same side as they started.\nIn fact, Bartos et al. collected a lot more data than this: each of the 48 flippers recorded the full sequence of all of their coin flips, including which type of coin they were using (coins minted in 46 different countries were used). All flippers even had to submit video of themselves flipping the coins. But the analysis in the next section will use only the summary statistic \\(X = 178,079\\), which records the number of flips that landed same-side-up.\n\n\nFrequentist analysis in the binomial model\nThe data set is simple to analyze if we make two seemingly innocuous simplifying assumptions: first, that the flips were statistically independent, and second, that every flip had an equal probability, which we will call \\(\\theta\\), of landing on the same side it started on. In that case, we can conclude (deductively) that the probability that \\(x\\) out of the \\(n\\) flips land on the same side is exactly \\(\\binom{n}{x} \\theta^x(1-\\theta)^{n-x}\\), for the realizable values \\(x=0,\\ldots,n\\).\nIf we accept these assumptions, we arrive at a one-parameter statistical model for the entire data set, called the binomial model. We abbreviate this by writing \\(X \\sim \\text{Binom}(n,\\theta)\\). If DHM’s prediction was right and a same-side bias exists, then we should have \\(\\theta &gt; 0.5\\); if they are wrong and the starting side makes no difference, we should have \\(\\theta = 0.5\\).\nFollowing the frequentist paradigm, we can estimate the parameter \\(\\theta\\) via the estimator \\(\\hat\\theta = X/n\\), in this case \\(0.5077\\). One thing we will prove in this class is that \\(X/n\\) is the best estimator of \\(\\theta\\), among all unbiased estimators, meaning all estimators for which \\(\\EE_\\theta \\hat\\theta = \\theta\\), for all possible values of the parameter \\(\\theta \\in [0,1]\\). This is an example of inductive behavior: if we use the estimator \\(X/n\\) to estimate \\(\\theta\\), we will get the answer right on average, and our estimate will be as precise (on average) as it could possibly be, for any unbiased estimator. We can also say how variable our estimator is: its standard error (the term of art we use for the standard deviation of an estimator) is \\(\\sqrt{\\theta(1-\\theta)}/n\\). Substituting our estimator \\(\\hat\\theta\\) for the true value \\(\\theta\\), we estimate that \\(\\hat\\theta\\) is typically off by about \\(0.00084\\), so we have good reason to believe \\(0.0577\\) is about that close to the true value of \\(\\theta\\). But we can’t necessarily say that it was close in this experiment; we could have gotten unlucky.\nCan we confidently conclude, inductively, that there is a same-side bias (\\(\\theta &gt; 0.5\\))? Naturally, even if DHM were wrong and there were no same-side bias (\\(\\theta = 0.5\\)), we would expect \\(X/n\\) to deviate somewhat from \\(0.5\\) in any given experiment, just by random chance. Could it be that that is what happened in this experiment?\nFrequentist analysis evades this question and substitutes another question in its place. Using the R function binom.test, we can test the null hypothesis that \\(\\theta = 0.5\\) and calculate a \\(95\\%\\) confidence interval for \\(\\theta\\):\n\nbinom.test(x = 178079, n = 350757, p = 0.5, alternative = \"greater\")\n\n\n    Exact binomial test\n\ndata:  178079 and 350757\nnumber of successes = 178079, number of trials = 350757, p-value &lt;\n2.2e-16\nalternative hypothesis: true probability of success is greater than 0.5\n95 percent confidence interval:\n 0.5063091 1.0000000\nsample estimates:\nprobability of success \n             0.5076991 \n\n\nThe \\(p\\)-value tells us the likelihood that we could observe as many same-side flips as we did, if \\(\\theta\\) really were \\(0.5\\). In this experiment the \\(p\\)-value is so small (\\(p &lt; 2.2\\times 10^{-16}\\)) that R doesn’t bother to tell us its exact value. The probability involved in calculating the \\(p\\)-value follows from our binomial assumptions, but most reasonable people would probably accept this as sufficient evidence to reject the null hypothesis; that is, to reach the inductive conclusion that \\(\\theta\\) is not really \\(0.5\\).\nThis is a risky conclusion! Even if we were so conservative as to reject the null hypothesis only when \\(p &lt; 10^{-10}\\), say, there would still be some chance of our making a mistake in any given experiment. But we have quantified how likely this is, and we can decide whether it is high enough to trouble us.\nBeyond just knowing that a same-side bias exists, it is interesting to have some sense of how large it is. Using very similar logic to the hypothesis test, the binom.test function also returns a \\(95\\%\\) confidence interval \\([50.6\\%, 50.9\\%]\\) for the parameter \\(\\theta\\). We will explore how to construct confidence intervals later in the semester, but for now it is sufficient to know that the interval is defined so that it has at least a $95$ chance of covering (i.e., including) \\(\\theta\\) in any given experiment, no matter what value \\(\\theta\\) takes. This is another example of inductive behavior: in producing a confidence interval, we take a risk that the corresponding inductive conclusion “\\(\\theta\\) lies between \\(0.056\\) and \\(0.0509\\)” is wrong, but we are behaving in such a way that we can quantify and limit this risk.\n\n\nBayesian analysis\nAnother route we could take, if we have a more Bayesian bent, is to introduce another assumption about the distribution that \\(\\theta\\) has; for example, that \\(\\theta \\sim \\text{Unif}[0,1]\\). This is a stronger assumption than we made before: in what sense does \\(\\theta\\) have this distribution? Compared with the other assumptions, it is very difficult to test: there is only one draw from the distribution of \\(\\theta\\), and it is observed only indirectly through the coin flips. However, it turns out in this case not to matter much what prior we picked, in the sense that many other prior distributions would result in almost the same posterior distribution.\nIf we make this assumption about the distribution of \\(\\theta\\), then we can directly calculate the conditional distribution after observing \\(X = 178,079\\). The posterior distribution for \\(\\theta\\) can be calculated analytically, giving probability density \\[\n\\theta \\mid X = x \\sim  \\frac{(n + 1)!}{x!(n-x)!} \\theta^x (1-\\theta)^{n-x}, \\quad \\text{ for } \\theta \\in [0,1].\n\\] This distribution is called the Beta distribution with parameters \\(\\alpha = x+1\\) and \\(\\beta = n-x+1\\). Note that this expression is a probability density for the parameter \\(\\theta\\), not the . Once we know the distribution of \\(\\theta\\) we can make claims about it: for example, after seeing the data, there is only a \\(3.8 \\times 10^{-20}\\) chance that \\(\\theta \\leq 0.5\\). We can also calculate a \\(95\\%\\) Bayesian credible interval, which includes \\(95\\%\\) of the posterior probability mass. In this case, the interval is \\([50.6\\%, 50.9\\%]\\), coinciding with the frequentist confidence interval to several decimal points. The credible interval is making a stronger claim than the confidence interval: it is saying, for this \\(\\theta\\), there is a \\(95\\%\\) chance that it falls in that range.\n\n\nQuestioning the binomial model\nBartos et al. stated in their paper that the binomial model is not quite correct: some human flippers had more same-side bias than others. We can modify the binomial model to allow for a different same-side probability \\(\\theta_i\\) for flipper \\(i\\) over their \\(n_i\\) coin flips (with \\(\\sum_i n_i = n\\)). If we retain the independence assumption, this leads to a more complex model where \\(X_i \\sim \\text{Binom}(n_i,\\theta_i)\\) independently for each \\(i = 1, \\ldots, 48\\).\nThere was also evidence that the flippers were improving over time. If we want to accommodate this information we can expand the model yet again, to allow \\(X_{i,t} \\sim \\text{Bernoulli}(\\theta_{i,t})\\) independently for \\(i = 1,\\ldots,48\\) and \\(t = 1,\\ldots, n_i\\). We might add the constraint that for each \\(i\\), we have \\(\\theta_{i,1} \\geq \\theta_{i,2} \\geq \\cdots \\geq \\theta_{i,n_i}\\). With \\(n = 350,757\\) parameters, one for every data point, this is effectively a nonparametric model. We will return to these models in future lectures.\n\n\nQuestions we’ll return to\nThis example is complex enough to give us a glimpse of some of the questions we’ll be interested in throughout the semester:\n\nBayesian vs frequentist frameworks: What are the pros and cons of each? Where does the prior come from, and how important is our choice of prior for the analysis?\nSufficiency: The first binomial analysis summarized the data as just the number \\(X\\) of total same-side flips, even though we have a lot more data than that (for one thing, we know the full sequence of flips for each flipper). As we’ll see, under the binomial model we lose nothing by summarizing the full data set by \\(X\\) alone and forgetting everything else about it. What is it about the structure of the binomial model that makes this a complete summary?\nEstimation: What is a good way to estimate the parameters in each of these models? In the second model with a different \\(\\theta_i\\) for each flipper, how can / should our estimate for one flipper be informed by the data from the other \\(47\\) flippers? In the third model, if we want to introduce a parametric functional form for the way \\(\\theta_{i,t}\\) changes with time, what would be a good model and how should we estimate it?\nTesting: In the basic binomial model, if we want to conclude with as much confidence as we can that there is some same-side bias, we might want to test the hypothesis \\(H_0:\\;\\theta \\leq 0.5\\) vs \\(H_1: \\;\\theta &gt; 0.5\\). As it turns out, there is a unique best way to do this: reject when \\(X\\) is large. In the second model, we might want to test whether we really need different \\(\\theta_i\\) values for the different flippers, by testing \\(H_0:\\; \\theta_1=\\theta_2=\\cdots=\\theta_{48}\\) against \\(H_1:\\; \\text{not all } \\theta_i \\text{ are equal}\\). That is, we test the null that the first model was adequate. This is a more complex testing problem for two reasons: our null hypothesis has a nuisance parameter, which may affect the null distribution of any test statistic; and our \\(48\\)-dimensional alternative distribution can vary from the \\(1\\)-dimensional null model in many, many different directions. As we’ll see, it can matter a lot which alternative directions are prioritized by the test we select.\nAsymptotics: In analyzing this data set we will never actually calculate anything like \\(350,757!\\) even though that quantity appears in the equations. In practice we replace the binomial model with an appropriate Normal model, in this case \\(X \\sim \\cN(n\\theta, n\\theta(1-\\theta))\\), and do the calculations with respect to that model. The normal approximation here is an obvious consequence of the central limit theorem, but we can make similar approximations in a lot of other problems where it is not so obvious a priori that this would be possible."
  },
  {
    "objectID": "reader/bayes-interpretation.html",
    "href": "reader/bayes-interpretation.html",
    "title": "Interpretations of Probability and Sources of Priors",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/bayes-interpretation.html#interpretations-of-probability",
    "href": "reader/bayes-interpretation.html#interpretations-of-probability",
    "title": "Interpretations of Probability and Sources of Priors",
    "section": "1 Interpretations of Probability",
    "text": "1 Interpretations of Probability\nWhy do we model anything as random? What does probability mean in the real world?\n\n1.1 1. Long-run Frequency Over Repeated Trials\n\nExamples:\n\nRepeatedly flipping a coin\nShooting electrons at a double slit\n\n\nNote: “You can never step into the same river twice.”\n\n\n1.2 2. Systematic Random Sampling from a Population\n\nExamples:\n\nSurvey of 500 random voters\nRandom assignment to treatment/control in controlled experiments\n\n\n\n\n1.3 3. Subjective Uncertainty About an Outcome\n\nExamples:\n\nChance that President Biden is re-elected\nHiggs boson having a given mass\nP = NP\n\n\nNotes: - Could be broad intersubjective agreement - These are often conflated with #1 - What if survey sampling is pseudo-random? - Probably relying on shared ignorance"
  },
  {
    "objectID": "reader/bayes-interpretation.html#where-does-the-prior-come-from",
    "href": "reader/bayes-interpretation.html#where-does-the-prior-come-from",
    "title": "Interpretations of Probability and Sources of Priors",
    "section": "2 Where Does the Prior Come From?",
    "text": "2 Where Does the Prior Come From?\nBayesian rejoinder: Where does \\(P\\) come from?\nFour main sources for prior on \\(\\theta\\):\n\n2.1 Source 1: Subjective Beliefs\nPros: - Brings all relevant info to bear - Straightforward interpretation of posterior\nCons: - Posterior is therefore subjective - Embarrassing to write “I think” in abstract - Hard if \\(\\theta\\) high-dimensional or \\(P\\) nonparametric\nExample: Flip coin 20 times, get 7 heads - 0.5 probably a better estimate than 0.35 - My subjective prior on coins:\n\\[\n\\pi(\\theta) \\propto \\theta^{10}(1-\\theta)^{10}\n\\]\n\n\n2.2 Source 2: Objective or Vague Prior\nPros: - Using default prior removes subjectivity\nCons: - What does the posterior mean?\nExamples:\n\nFlat prior: \\(\\pi(\\theta) \\propto 1\\) on \\([0,1]\\)\n\nIndifference in \\(\\theta\\) parameterization\nOften improper, e.g., \\(\\pi(\\theta) \\propto 1\\) on \\(\\mathbb{R}\\), but usually ok\n\n\\(\\theta \\in \\mathbb{R}\\), flat prior on \\(\\mathbb{R}\\)\n\n\\(X|\\theta \\sim N(\\theta, \\sigma^2)\\)\n\\(\\pi(\\theta|x) \\propto p(x|\\theta)\\)\n\\(\\mathbb{E}[\\theta|x] = \\bar{x}\\), \\(\\theta|x \\sim N(\\bar{x}, \\frac{\\sigma^2}{n})\\)\n\nJeffreys prior: \\(\\pi(\\theta) \\propto \\sqrt{I(\\theta)}\\)\n\nHigher density where \\(p_\\theta\\) changing faster\nInvariant to parameterization (HW 5)\n\n\\(X|\\theta \\sim \\text{Binom}(n, \\theta)\\)\n\n\\(\\pi(\\theta) \\propto \\theta^{-1/2}(1-\\theta)^{-1/2} = \\text{Beta}(1/2, 1/2)\\)\n\\(\\theta \\to 0\\) or \\(1\\) as \\(\\theta \\to 0\\) or \\(1\\)\n\n\n\n\n2.3 Intersubjective Agreement\nData may effectively rule out most \\(\\theta\\) values, making posterior uncontroversial.\nExample: \\(X \\sim \\text{Binom}(10^4, \\theta)\\), observe \\(X = 3000\\) - SD \\(\\approx \\sqrt{n} = 10 \\approx 0.005\\) - Likelihood \\(\\theta|X = 0.3\\) outside \\([0.29, 0.31]\\) - All reasonable priors may be \\(\\approx\\) flat on \\([0.29, 0.31]\\) - \\(\\pi(\\theta|x) \\propto \\text{Lik}(\\theta|x) \\approx \\exp(-\\frac{(\\theta - 0.3)^2}{2(0.005)^2})\\) - \\(\\theta|x \\approx N(0.3, (0.005)^2)\\) (HW 5)\nData swamps everyone’s prior.\n\n\n2.4 Gaussian Sequence Model\n\\(X|\\theta \\sim N(\\theta, I_d)\\), \\(\\theta \\in \\mathbb{R}^d\\)\n\nJeffreys prior is flat: \\(\\pi(\\theta) \\propto 1\\) on \\(\\mathbb{R}^d\\)\n\\(\\pi(\\theta|x) \\propto N(\\theta|x, I_d)\\), \\(\\mathbb{E}[\\theta|x] = x\\)\nSame as UMVU\nWhat about \\(\\|\\theta\\|_1\\)? Recall:\n\n\\(\\hat{\\mu} = \\arg\\min_d \\|\\theta - d\\|_1 \\Rightarrow \\hat{\\mu}_j = \\text{sign}(x_j)(|x_j| - \\lambda)_+\\)\nRecall James-Stein: \\(\\|\\hat{\\mu}\\|_2^2 \\leq \\|x\\|_2^2 - 2d\\lambda + d\\lambda^2\\)\n\\(\\text{MSE}(\\theta) = \\mathbb{E}[\\|\\hat{\\mu} - \\theta\\|_2^2] = \\text{Var}(\\hat{\\mu}) + \\text{Bias}^2\\)\n\\(\\text{Var}(\\hat{\\mu}) \\leq d\\), \\(\\text{Bias}^2 \\leq d\\lambda^2\\)\n\n\nWhat went wrong? Examine Jeffreys prior: - \\(P(\\|\\theta\\|_2 \\leq r) = \\text{Vol}(\\text{Ball of radius } r) \\propto r^d\\) - \\(P(\\|\\theta\\|_2 \\geq r) = 1 - cr^d\\) - \\(P(\\|\\theta\\|_2 \\geq \\gamma d^{1/2}) \\approx 1\\) for \\(\\gamma &lt; 1\\)\nGrows rapidly. Prior expects \\(\\|\\theta\\|\\) to be huge.\n\n\n2.5 Source 3: Prior or Concurrent Experience\n\nMay have many instances of same problem\nAssume true \\(\\theta\\) values drawn from a population\nHierarchical Bayes / empirical Bayes\nCan be hard to choose right reference class\n\nExample: Estimate batting average - Player \\(i\\) has \\(n_i\\) at-bats, true batting avg \\(\\theta_i\\) - Hierarchical model (players \\(i=1,\\ldots,m\\)): - Hyperparameter \\(\\alpha, \\beta \\sim \\pi_0(\\alpha, \\beta)\\) (hyperprior) - \\(\\theta_i|\\alpha, \\beta \\sim \\text{Beta}(\\alpha, \\beta)\\) - \\(X_i|\\theta_i, n_i \\sim \\text{Binom}(n_i, \\theta_i)\\) - \\(\\theta_i|X \\sim \\text{Beta}(\\alpha + X_i, \\beta + n_i - X_i)\\) - \\(\\mathbb{E}[\\theta_i|X] = \\frac{\\alpha + X_i}{\\alpha + \\beta + n_i}\\) - If \\(m\\) large, \\(\\alpha, \\beta\\) may be almost known - Choice of \\(\\pi_0\\) doesn’t matter much - Reference class problem: which players to include?\n\n\n2.6 Flexibility of Bayes\nAny \\((\\pi, P, L, g(\\theta))\\) defined straightforwardly: \\[\n\\delta(x) = \\arg\\min_d \\int L(\\theta, d) \\pi(\\theta|x) d\\theta\n\\]\n\nProblem reduced to (possibly hard) computation\nPosterior is one-stop shop for all answers\nNo need for:\n\nSpecial family structure (exp fam, completeness)\nSpecial estimator (U-estimable)\nConvex or nice \\(L\\)\n\nHighly expressive modeling & estimation\nCaveat: Limited by ability to do computations (topic of next lecture)\n\n\n\n2.7 Source 4: Convenience Priors\n\nChoosing conjugate or other nice priors\nMuch faster computations, esp. in high dim\nBut what does the posterior mean?\n\nExample: - \\(X_i \\stackrel{iid}{\\sim} p\\), \\(p\\) unknown density on \\(\\mathbb{R}\\) - Estimand: \\(m =\\) median\\((p)\\) - Estimator: \\(\\delta(x) =\\) median\\((X)\\) - Good estimator: robust, nonparametric - Large \\(n\\): \\(\\delta(x) \\sim N(m, \\frac{1}{4np(m)^2})\\) - Not Bayes for any realistic prior\nBayes approach: 1. Define prior over \\(p\\) (infinite dim) 2. Calculate posterior (horrific unless we pick special prior) 3. Return e.g., \\(\\mathbb{E}[m|X]\\)\nIf it differs substantially from median\\((X)\\), do we trust it?"
  },
  {
    "objectID": "reader/bayes-interpretation.html#gaussian-hierarchical-model",
    "href": "reader/bayes-interpretation.html#gaussian-hierarchical-model",
    "title": "Interpretations of Probability and Sources of Priors",
    "section": "3 Gaussian Hierarchical Model",
    "text": "3 Gaussian Hierarchical Model\n\\(\\theta_i \\sim N(\\mu, \\tau^2)\\), \\(X_i|\\theta_i \\sim N(\\theta_i, \\sigma^2)\\)\nPosterior mean: \\[\n\\mathbb{E}[\\theta_i|X] = \\mathbb{E}[\\mathbb{E}[\\theta_i|X, \\mu, \\tau^2]|X] = \\mathbb{E}[\\frac{\\tau^2}{\\tau^2 + \\sigma^2}X_i + \\frac{\\sigma^2}{\\tau^2 + \\sigma^2}\\mu|X]\n\\]\nLinear shrinkage estimator: - Bayes optimal shrinkage estimated from data - Likelihood for \\(\\mu, \\tau^2\\) (marginalize over \\(\\theta_i\\)): - \\(X_i|\\mu, \\tau^2 \\sim N(\\mu, \\tau^2 + \\sigma^2)\\) - \\(\\bar{X} \\sim N(\\mu, \\frac{\\tau^2 + \\sigma^2}{n})\\) - \\(S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\sim \\frac{\\tau^2 + \\sigma^2}{n-1}\\chi^2_{n-1}\\)\nDefine \\(B = \\tau^2 + \\sigma^2\\): \\[\n\\delta(x) = \\mathbb{E}[\\mathbb{E}[\\theta_i|X, B]|X] = \\mathbb{E}[\\frac{B - \\sigma^2}{B}X_i + \\frac{\\sigma^2}{B}\\bar{X}|X]\n\\]\nConjugate prior: \\[\n\\pi(B|\\lambda, \\nu) \\propto B^{-\\nu/2-2}\\exp(-\\frac{\\lambda}{2B})\n\\]\n\\[\nB|X \\sim \\text{InvGamma}(\\frac{n+\\nu}{2}, \\frac{\\lambda + (n-1)S^2}{2})\n\\]\n\\[\n\\mathbb{E}[\\frac{1}{B}|X] = \\frac{n+\\nu}{\\lambda + (n-1)S^2}\n\\]\n\\[\n\\delta_i(x) = \\frac{(n-3)S^2}{(n-1)S^2 + \\lambda}X_i + \\frac{\\lambda + 2S^2}{(n-1)S^2 + \\lambda}\\bar{X}\n\\]\nMight want to truncate prior to \\([\\sigma^2, \\infty)\\) if \\(\\lambda\\) small."
  },
  {
    "objectID": "reader/testing-interpretation.html",
    "href": "reader/testing-interpretation.html",
    "title": "p-Values, Confidence Regions, and Misinterpreting Tests",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/testing-interpretation.html#p-values",
    "href": "reader/testing-interpretation.html#p-values",
    "title": "p-Values, Confidence Regions, and Misinterpreting Tests",
    "section": "1 p-Values",
    "text": "1 p-Values\n\n1.1 Informal Definition\nSuppose \\(\\phi(x)\\) rejects for \\(T(x) &gt; c\\). The p-value is:\n\\[p(x) = \\mathbb{P}_0(T(X) \\geq T(x)_{\\text{observed}}) = \\mathbb{P}_0(T(X) \\geq t)\\]\nExample: \\(X \\sim N(\\theta, 1)\\), \\(H_0: \\theta = 0\\) vs \\(H_1: \\theta \\neq 0\\)\nTwo-sided test rejects for large \\(|T(X)| = |X|\\):\n\\[p(x) = \\mathbb{P}_0(|X| \\geq |x|) = 2(1 - \\Phi(|x|))\\]\nThe two-sided p-value is \\(p(X)\\), where:\n\\[p(x) = \\mathbb{P}_0(|X| \\geq |x|) = 2\\min\\{\\Phi(x), 1-\\Phi(x)\\}\\]\n\n\n1.2 Formal Definition\nAssume we have a test \\(\\phi_\\alpha\\) for each significance level \\(\\alpha\\): \\(\\mathbb{E}_0[\\phi_\\alpha(X)] \\leq \\alpha\\)\nIn the non-randomized case: \\(\\phi_\\alpha(x) = 1\\{x \\in R_\\alpha\\}\\)\nAssume tests are monotone in \\(\\alpha\\): if \\(\\alpha \\leq \\alpha'\\), then \\(\\phi_\\alpha(x) \\leq \\phi_{\\alpha'}(x)\\)\n(In non-randomized case: \\(R_\\alpha \\subseteq R_{\\alpha'}\\))\nThen:\n\\[p(x) = \\inf\\{\\alpha \\in [0,1]: \\phi_\\alpha(x) = 1\\} = \\inf\\{\\alpha: x \\in R_\\alpha\\}\\]\nIt’s possible to define randomized p-value, but not worth it.\nNote: \\(p(x) \\leq \\alpha \\iff \\phi_\\alpha(x) = 1\\)\nFor \\(\\theta = \\theta_0\\): \\(\\mathbb{P}_0(p(X) \\leq \\alpha) = \\mathbb{E}_0[\\phi_\\alpha(X)] \\leq \\alpha\\)\np-value stochastically dominates \\(U(0,1)\\)\nIf \\(\\phi_\\alpha\\) rejects for large \\(T(X)\\), reduces to original definition.\nNote: The p-value depends on: - The model - Null hypothesis - The data AND - The choice of test\nExample: \\(X \\sim N(\\theta, I_d)\\), \\(H_0: \\theta = 0\\) vs \\(H_1: \\theta \\neq 0\\)\nWe can use \\(T(x) = \\|x\\|_2^2\\) (\\(\\chi^2\\) test) or \\(T(x) = \\max_i |x_i|\\) (max test)\nVery different p-values, power if \\(d\\) large Choice reflects belief about whether \\(\\theta\\) is sparse\n\n\n1.3 Accept/Reject Decisions\nAccept/reject decisions are not interesting Usually, we care how big \\(\\theta\\) is Tiny p-value doesn’t imply big \\(\\theta\\) Big p-value doesn’t imply small \\(\\theta\\) either"
  },
  {
    "objectID": "reader/testing-interpretation.html#confidence-regions",
    "href": "reader/testing-interpretation.html#confidence-regions",
    "title": "p-Values, Confidence Regions, and Misinterpreting Tests",
    "section": "2 Confidence Regions",
    "text": "2 Confidence Regions\nDefinition: \\(C: \\cX \\to \\cP(\\Theta)\\) is a \\(1-\\alpha\\) confidence set for \\(g(\\theta)\\) if:\n\\[\\mathbb{P}_\\theta(C(X) \\ni g(\\theta)) \\geq 1-\\alpha \\quad \\forall \\theta \\in \\Theta\\]\nWe say \\(C(x)\\) covers \\(g(\\theta)\\) if \\(C(x) \\ni g(\\theta)\\)\nCoverage probability: \\(\\mathbb{P}_\\theta(C(X) \\ni g(\\theta))\\)\n\\(\\inf_\\theta \\mathbb{P}_\\theta(C(X) \\ni g(\\theta))\\) is confidence level\nNote: \\(C(X)\\) is random, not \\(g(\\theta)\\)\nOften misinterpreted as Bayesian guarantee Say “\\(C(x)\\) has a 95% chance of covering” Not “\\(g(\\theta)\\) has a 95% chance of being in \\(C\\)” NEVER “95% chance \\(g(\\theta) \\in [0.5, 1.5]\\)” e.g.\n\n2.1 Duality of Tests/Confidence Sets\nSuppose we have a level \\(\\alpha\\) test \\(\\phi(\\cdot, a)\\) of \\(H_0: g(\\theta) = a\\) vs \\(H_1: g(\\theta) \\neq a\\), \\(\\forall a \\in \\Theta\\)\nWe can use it to make a confidence set for \\(g(\\theta)\\):\nLet \\(C(X) = \\{a: \\phi(X, a) = 0\\}\\) (all non-rejected values of \\(a\\))\nThen \\(\\mathbb{P}_\\theta(C(X) \\ni g(\\theta)) = \\mathbb{P}_\\theta(\\phi(X, g(\\theta)) = 0) \\geq 1-\\alpha\\)\nAlternatively, suppose \\(C(X)\\) is a \\(1-\\alpha\\) confidence set for \\(g(\\theta)\\)\nWe can use \\(C\\) to construct a test \\(\\phi\\) of \\(H_0: g(\\theta) = a\\) vs \\(H_1: g(\\theta) \\neq a\\):\n\\(\\phi(x) = 1\\{a \\notin C(x)\\}\\)\nFor \\(\\theta\\) s.t. \\(g(\\theta) = a\\):\n\\[\\mathbb{E}_\\theta[\\phi(X)] = \\mathbb{P}_\\theta(a \\notin C(X)) = \\mathbb{P}_\\theta(C(X) \\not\\ni g(\\theta)) \\leq \\alpha\\]\nThis is called inverting a test.\n\n\n2.2 Confidence Intervals/Bounds\nIf \\(C(X) = [C_L(X), C_U(X)]\\), we say: - \\(C(X)\\) is a confidence interval (CI) - \\(C_L(X)\\) is a lower confidence bound (LCB) - \\(C_U(X)\\) is an upper confidence bound (UCB)\nWe usually get LCB, UCB by inverting a one-sided test in appropriate direction Called uniformly most accurate (UMA) if test UMP\nGet CI by inverting a two-sided test Called UMAU if test is UMPU\nExample: \\(X \\sim \\text{Exp}(\\theta)\\), \\(n=1\\), \\(\\mathbb{E}[X] = \\frac{1}{\\theta}\\), \\(\\theta &gt; 0\\)\nCDF: \\(\\mathbb{P}_\\theta(X \\leq x) = 1 - e^{-\\theta x}\\)\nLCB: Invert test for \\(H_0: \\theta \\leq \\theta_0\\) Solve \\(\\alpha = 1 - \\mathbb{P}_{\\theta_0}(X \\leq x) = e^{-\\theta_0 x}\\)\n\\(\\theta_0 = -\\frac{1}{x}\\log(\\alpha)\\)\n\\(C_L(X) = -\\frac{1}{X}\\log(\\alpha)\\), \\(\\mathbb{P}_\\theta(\\theta \\geq C_L(X)) = 1-\\alpha\\)\nUCB: Similar \\(C_U(X) = -\\frac{1}{X}\\log(1-\\alpha)\\)\nEqual-tailed: Invert equal-tailed test of \\(H_0: \\theta = \\theta_0\\)\n\\(\\theta_0 e^{-\\theta_0 x} = \\frac{\\alpha}{2}\\), \\(1 - e^{-\\theta_0 x} = 1 - \\frac{\\alpha}{2}\\)\n\\(C(X) = [\\frac{-\\log(\\alpha/2)}{X}, \\frac{-\\log(\\alpha/2)}{X}]\\)\nSimilar for UMPU 2-sided test"
  },
  {
    "objectID": "reader/testing-interpretation.html#misinterpreting-hypothesis-tests",
    "href": "reader/testing-interpretation.html#misinterpreting-hypothesis-tests",
    "title": "p-Values, Confidence Regions, and Misinterpreting Tests",
    "section": "3 Misinterpreting Hypothesis Tests",
    "text": "3 Misinterpreting Hypothesis Tests\nHypothesis tests ubiquitous in science Common misinterpretations:\n\np &lt; 0.05, therefore there is an effect (or the effect size = the estimate)\np &gt; 0.05, therefore there is no effect\np = 10^-6, therefore the effect is huge\np &lt; 10^-6, therefore the data are significant and everything about our model is correct (in most naive interpretation)\nEffect CI for men is [0.2, 3.2], for women is [-0.2, 2.8], therefore there is an effect for men and not for women\n\nDichotomous test doesn’t eliminate uncertainty CIs usually less misleading to novices\nInterpreting tests is not easy or automatic Hypothesis tests let us ask specific questions under specific modeling assumptions Interpreting them requires care and experience\nTop-tier medical journals let people publish claims reporting p-values without saying what model was used or what test was employed Pretty bad when you think about it\nHyp. tests can be a good companion to critical thinking, never a substitute All models are wrong, some are useful, but need experience and theory to understand when assumptions do or don’t cause real trouble\n\n3.1 Common Objections to Hypothesis Testing\n\nWhy should I test \\(\\theta = 0\\)? Is \\(\\theta\\) ever exactly 0?\nA: a. Test \\(H_0: |\\theta| &lt; \\epsilon\\) if you want If \\(\\sigma_{\\hat{\\theta}} \\ll \\epsilon\\), not much difference\n\nMost two-sided tests justify directional interest: If \\(T &gt; c\\), declare \\(\\theta &gt; 0\\), if \\(T &lt; -c\\), declare \\(\\theta &lt; 0\\) with \\(\\mathbb{P}(\\text{false claim}) &lt; \\alpha\\)\nHarder to answer in non-parametric problems e.g. \\(H_0: P = Q\\) vs \\(H_1: P \\neq Q\\) for perm test, but alternative frameworks like Bayes force very strong assumptions on us\n\nPeople only like frequentist results like p-values, CIs because they mistake them for Bayesian results “95% chance \\(\\theta &gt; 0\\)” is misinterpreted as a claim about \\(\\mathbb{P}(\\theta &gt; 0 | X)\\)\nA: True, but subjective Bayesian results often misinterpreted as the posterior dist. of \\(\\theta\\) when really should be “posterior opinion about \\(\\theta\\)”"
  },
  {
    "objectID": "reader/measure-theory-basics.html",
    "href": "reader/measure-theory-basics.html",
    "title": "Measure Theory Basics",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\simind}{\\stackrel{\\textrm{ind.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\DeclareMathOperator*{\\maxz}{minimize\\;}\n\\DeclareMathOperator*{\\argmin}{argmin\\;}\n\\DeclareMathOperator*{\\argmax}{argmax\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/measure-theory-basics.html#measure-theory-a-rigorous-grounding-for-probability",
    "href": "reader/measure-theory-basics.html#measure-theory-a-rigorous-grounding-for-probability",
    "title": "Measure Theory Basics",
    "section": "Measure theory: a rigorous grounding for probability",
    "text": "Measure theory: a rigorous grounding for probability\nMeasure theory is an area of mathematics concerned with measuring the “size” of subsets of a certain set. Soon after it was developed in the the early twentieth century, the great Soviet mathematician Kolmogorov realized it could be applied to give a rigorous grounding to probability theory, it was a major advance in understanding and resolving certain paradoxes in probability theory. David Aldous gives a nice discussion of this history.\nThis is not a course on measure-theoretic probability and we will not rigorously develop the subject. However, it will be useful to draw on some of the basics of measure theory, to simplify our notation throughout the course and to clarify certain concepts around integration and conditioning. Homework 0 illustrates some of the interesting aspects of measure theory and why it is useful."
  },
  {
    "objectID": "reader/measure-theory-basics.html#measures",
    "href": "reader/measure-theory-basics.html#measures",
    "title": "Measure Theory Basics",
    "section": "Measures",
    "text": "Measures\nGiven a set \\(\\cX\\), a measure\\(\\mu\\) is a certain kind of function mapping “nice enough” subsets \\(A \\subseteq \\cX\\) to non-negative numbers \\(\\mu(A) \\in [0,\\infty]\\).\nExample 1 (Counting measure): If \\(\\cX\\) is countable, e.g. \\(\\cX = \\mathbb{Z}\\), then a natural measure is the counting measure \\(\\#(A)\\), which simply counts the number of points in a subset \\(A\\). That is, \\(\\#(\\{0,1\\}) = 2\\), and \\(\\#(\\{2,4,6,8,\\ldots\\}) = \\infty\\) .\nExample 2 (Lebesgue measure): If \\(\\cX = \\RR^n\\) for some integer \\(n\\), a natural measure is the Lebesgue measure \\(\\lambda(A)\\), which returns the volume of a subset \\(A\\). Roughly speaking, we can write\n\\[\n\\lambda(A) = \\int \\cdots \\int_A \\td x_1\\td x_2\\cdots \\td x_n.\n\\]\nExample 3 (Gaussian measure): Now taking \\(\\cX = \\RR\\), we might instead want to define the “size” of a set as the probability that a standard Gaussian random variable \\(Z \\sim \\cN(0,1)\\) is observed to be in the set \\(A\\). That is, we can define the measure:\n\\[\nP_Z(A) = \\PP(Z \\in A) = \\int_A \\phi(x)\\td x, \\quad \\text{ where } \\;\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\n\\]\nis the probability density function of \\(Z\\).\nAs it turns out, it is not so obvious how to define what exactly we mean by the right-hand side of the previous two equations; in fact, it is not even possible to define the volume of every subset \\(A \\in \\mathbb{R}\\). In Homework 0, Problem 3, you will use the axiom of choice to construct pathological subsets (so-called non-measurable sets) to which we cannot sensibly assign any volume.\nOne of the original motivations for measure theory was to provide a framework for excluding these pathological sets and rigorously defining integrals over the other, nicer sets. In general, the domain of a measure is not all subsets of \\(\\cX\\) (called the power set and notated \\(2^{\\cX}\\)), but rather a collection of nice subsets \\(\\cF \\subseteq 2^{\\cX}\\).\nFormally, the collection \\(\\cF\\) must be a \\(\\sigma\\)-field, meaning that it satisfies certain closure properties. We say \\(\\cF\\) is a \\(\\sigma\\)-field (or \\(\\sigma\\)-algebra) if\n\nThe full set \\(\\cX\\) is in \\(\\cF\\).\nIf \\(A\\) is in \\(\\cF\\) then its complement \\(\\cX \\setminus A\\) is also in \\(\\cF\\) (i.e., \\(\\cF\\) is closed under complementation)\nIf \\(A_1,A_2,\\ldots \\in \\cF\\) then \\(\\bigcup_{i=1}^\\infty A_i\\) is also in \\(\\cF\\) (i.e. \\(\\cF\\) is closed under countable unions)\n\nNote: The details of this definition are not important for purposes of this course.\nExample: If \\(\\cX\\) is countable we can take \\(\\cF\\) to be the entire power set.\nExample: If \\(\\cX = \\mathbb{R}^n\\) we will typically use the Borel \\(\\sigma\\)-field \\(\\cB\\), defined as the smallest \\(\\sigma\\)-field that includes all open rectangles \\((a_1,b_1)\\times (a_2,b_2) \\times \\cdots \\times (a_n, b_n)\\), where \\(a_i &lt; b_i\\) for all \\(i\\). That is, we start with the open rectangles and recursively apply the closure properties to obtain a very large collection of sets, which informally we can think of as containing all non-pathological subsets of \\(\\mathbb{R}^n\\).\nWe are now ready to define a measure. We call a pair of a set \\(\\cX\\) and an associated \\(\\sigma\\)-field \\(\\cF \\subseteq 2^{\\cX}\\) a measurable space. Given a measurable space \\((\\cX, \\cF)\\), a measure is a function \\(\\mu: \\cF \\to \\mathbb{R}\\) satisfying three properties:\n\nNon-negativity: \\(\\mu(A) \\geq 0\\) for all \\(A\\in \\cF\\).\nEmpty set maps to zero: \\(\\mu(\\emptyset) = 0\\)\nCountable additivity: If \\(A_1,A_2,\\ldots\\in \\cF\\) are all disjoint, then\n\n\\[\n\\mu\\left(\\bigcup_{i=1}^\\infty A_i\\right) = \\sum_{i=1}^\\infty \\mu(A_i)\n\\]\nIf \\(\\mu\\) is a measure on \\((\\cX, \\cF)\\) we call \\((\\cX, \\cF, \\mu)\\) a measure space.\nIn the special case \\(\\mu(\\cX) = 1\\), we call \\(\\mu\\) a probability measure and \\((\\cX, \\cF, \\mu)\\) is called a probability space."
  },
  {
    "objectID": "reader/measure-theory-basics.html#integrals",
    "href": "reader/measure-theory-basics.html#integrals",
    "title": "Measure Theory Basics",
    "section": "Integrals",
    "text": "Integrals\nOne very nice thing about measures is that they let us define integrals of (nice enough) real-valued functions on \\(\\cX\\) with respect to the measure \\(\\mu\\), meaning the integral is “weighted” in a way that assigns total weight \\(\\mu(A)\\) to each set \\(A\\). We will use the notation \\(\\int f(x)\\,\\td\\mu(x)\\), or just \\(\\int f \\td\\mu\\).\nTo construct this integral, we begin by defining it for indicator functions, and then extend to more general functions by linearity and limits, in a few steps:\nFirst, for an indicator function \\(1_A(x) = 1\\{x \\in A\\}\\) of a set \\(A \\in \\cF\\), it is straightforward to define the integral as \\(\\int 1_A \\td\\mu = \\mu(A)\\) (note if \\(A \\notin \\cF\\) this does not work, but we are only defining the integral for a class of “nice” functions determined by our \\(\\sigma\\)-field)\nNext, consider a simple function \\(f(x) = \\sum_{i=1}^\\infty c_i 1_{A_i}(x)\\), with all \\(c_i \\geq 0\\) and \\(A_i \\in \\cF\\). Because the integral should be linear, we should have\n\\[\n\\int f\\td\\mu = \\sum_{i=1}^\\infty c_i \\int 1_{A_i}\\td\\mu = \\sum_{i=1}^\\infty c_i \\mu(A_i)\n\\]\nThird, we can extend to all sufficiently nice non-negative functions by approximating them from below with a series of simple functions:\n\\[\n\\int f\\td\\mu = \\lim_{i=1}^\\infty \\int f_i\\td\\mu.\n\\]\nThis idea is illustrated in the picture below:\n\n\n\nApproximating a non-negative function from below by a series of simple (piecewise constant) functions. The red function is an indicator, the blue function is a simple function, and the dashed orange function is a simple function that approximates the orange curve.\n\n\nFinally, we can write any real-valued function as the sum of its positive and negative parts, \\(f(x) = f^+(x) - f^-(x)\\), where \\(f^+(x) = \\max\\{f(x), 0\\}\\) and \\(f^-(x) = \\max\\{-f(x), 0\\}\\). Then both \\(f^+\\) and \\(f^-\\) have non-negative (possibly infinite) integrals. Then we simply take\n\\[\n\\int f\\td\\mu = \\int f^+\\td\\mu - \\int f^-\\td\\mu \\in [-\\infty, \\infty],\n\\]\ncalling the difference undefined if the integrals of both \\(f^+\\) and \\(f^-\\) are infinite.\nAs a result, we have \\(\\int f\\td\\mu\\) for any function \\(f\\) whose positive and negative parts can both be approximated from below by simple functions. Note that we have left out some important details in this presentation (for example we have not characterized which functions \\(f\\) are nice enough to be approximated well by simple functions) but these details are unimportant for this class. The important thing to know is that to any measure \\(\\mu\\) there corresponds a well-defined integral \\(\\int \\cdot \\td\\mu\\), which behaves as we would expect it to.\nWe can now return to our previous examples of measures and ask what the corresponding integrals are:\nExample 1, continued (Counting measure): An integral with respect to \\(\\#\\) just adds up all the values of \\(f(x)\\):\n\\[\n\\int f\\td\\# = \\sum_{x\\in \\cX} f(x)\n\\]\nExample 2, continued (Lebesgue measure): An integral with respect to the Lebesgue measure is called a Lebesgue integral, which is essentially just the usual integral you are used to from calculus class:\n\\[\n\\int f\\td\\lambda = \\int\\cdots \\int f(x) \\td x_1 \\cdots \\td x_n.\n\\]\nThe Lebesgue integral extends the Riemann integral to a more general class of functions, in the sense that if the Riemann integral of \\(f\\) is defined then the Lebesgue integral is also well defined and the two integrals coincide. But the Lebesgue integral is also well-defined for functions like \\(f(x) = 1\\{x \\in \\mathbb{Q}\\}\\), for which the Riemann integral is not well-defined (Exercise: what is the Lebesgue integral of \\(1\\{x \\in \\mathbb{Q}\\}\\)?)\nExample 3, continued (Gaussian measure): Note that \\(P_Z(A)\\) is defined as the (Lebesgue) integral of \\(1_A(x)\\phi(x)\\). By extension, the integral of \\(f\\) with respect to \\(P_Z\\) is the Lebesgue integral of \\(f(x) \\phi(x)\\), which is nothing more than the expectation of \\(f(Z)\\):\n\\[\n\\int f\\td P_Z = \\int_{-\\infty}^\\infty f(x) \\phi(x) \\td x = \\EE[f(Z)].\n\\]"
  },
  {
    "objectID": "reader/measure-theory-basics.html#densities",
    "href": "reader/measure-theory-basics.html#densities",
    "title": "Measure Theory Basics",
    "section": "Densities",
    "text": "Densities\nWe have just seen in the last two examples that there is a special relationship between the Lebesgue measure \\(\\lambda\\) on \\(\\RR\\) and the Gaussian measure \\(P_Z\\), allowing us to evaluate integrals with respect to \\(P\\) by turning them into integrals with respect to \\(\\lambda\\), namely \\(\\int f(x)\\td P_Z(x) = \\int f(x)\\phi(x) \\td\\lambda(x)\\).\nThis is a happy fact, since mathematicians have gone to a lot of trouble figuring out how to calculate integrals with respect to the usual (Lebesgue) measure. Most of the expectations we want to calculate in statistics are integrals with respect to some joint probability measure over random variables, and we certainly wouldn’t want to have to reinvent the wheel of integration every time we want to do calculations with respect to a new random variable.\nNote that we can’t turn integrals for every random variable into Lebesgue integrals. If \\(Y\\) follows a binomial distribution, for example, we can just as well define \\(P_Y(A) = \\PP(Y \\in A)\\), but there is no counterpart to \\(\\phi\\) that would let us turn \\(P_Y\\) integrals into Lebesgue integrals in the same way.\nFormally, consider a measurable space \\((\\cX, \\cF)\\), with two measures \\(P\\) and \\(\\mu\\). We say \\(P\\) is absolutely continuous with respect to \\(\\mu\\) if \\(P(A) = 0\\) whenever \\(\\mu(A) = 0\\). In notation, we write \\(P \\ll \\mu\\).\nIf \\(P \\ll \\mu\\) then, under mild conditions, we can always define a density function \\(p:\\cX \\mapsto [0,\\infty)\\) such that\n\\[\nP(A) = \\int 1_A(x) p(x)\\td\\mu(x), \\quad \\text{ for all } A \\in \\cF,\n\\]\nand by extension \\(\\int f(x)\\td P(x) = \\int f(x) p(x) \\td\\mu(x)\\).\nThe function \\(p\\) is called the density function or Radon-Nikodym derivative of \\(P\\) with respect to \\(\\mu\\). It is sometimes written using the suggestive notation \\(\\frac{\\td P}{\\td\\mu}(x)\\). Whenever we have a density function we can turn integrals with respect to \\(P\\) into integrals with respect to \\(\\mu\\) simply by multiplying the integrand by \\(p\\).\nIf we do not specify what \\(\\mu\\) is, it is assumed to be the Lebesgue measure; that is, if we say \\(P\\) is absolutely continuous with no further elaboration, we mean \\(P \\ll \\lambda\\).\nIf \\(P\\) is a probability measure, we call \\(p(x)\\) its probability density function (with respect to \\(\\mu\\)). If \\(\\mu\\) is a counting measure, we call \\(p(x)\\) its probability mass function. These are abbreviated pdf and pmf, respectively."
  },
  {
    "objectID": "reader/measure-theory-basics.html#probability-spaces-and-random-variables",
    "href": "reader/measure-theory-basics.html#probability-spaces-and-random-variables",
    "title": "Measure Theory Basics",
    "section": "Probability spaces and random variables",
    "text": "Probability spaces and random variables\nA typical statistics problem involves many, random variables of various types (e.g. some discrete and some continuous random variables), some of which may be functions of others. The overall joint distribution is defined implicitly by specifying the variables’ relationships to one another, or giving some sequence of rules for how they are all generated. We will want to ask about probabilities of events that involve functions of multiple random variables, e.g. a natural question to ask in a simple variance estimation problem with i.i.d. random variables \\(X_1,\\ldots,X_n\\) might be “what is the probability that \\(\\left|\\frac{1}{n-1}\\sum_{i=1}^n \\left(X_i - \\overline X\\right)^2 - \\sigma^2\\right| &lt; \\delta\\) ?”\nThe notation in the previous section doesn’t allow us to ask questions like this without, e.g., massaging the above event into the set of \\((X_1,\\ldots,X_n)\\) vectors for which the event would hold. This would become even more difficult in more complicated setups.\nInstead of trying to work directly with the measure corresponding to the joint distribution of all of the random variables involved, it can be convenient to instead think of the variables as all being functions of some abstract “outcome” \\(\\omega\\) that encompasses all of the randomness in the problem. We introduce an abstract probability space \\((\\Omega, \\cF, \\PP)\\) where\n\n\\(\\omega \\in \\Omega\\) is called an outcome,\n\\(A \\in \\cF\\) is called an event,\n\\(\\PP(A)\\) is called the probability of \\(A\\).\n\nThen a random variable is any (nice enough) function \\(X:\\; \\Omega \\to \\cX\\). We say \\(X\\) has distribution \\(P\\), and write \\(X \\sim P\\), if\n\\[\n\\PP(X \\in B) = \\PP(\\{\\omega:\\; X(\\omega) \\in B\\}) = P(B).\n\\]We say the real-valued random variable \\(X\\) is continuous if its distribution is absolutely continuous (with respect to the Lebesgue meaure). If \\(X\\) is a random variable, then \\(f(X)\\) is also a random variable for any (nice enough) function \\(f\\).\nLikewise, the expectation of a random variable is defined as an integral with respect to \\(\\PP\\):\n\\[\n\\EE[X] = \\int X(\\omega) \\td\\PP(\\omega), \\quad \\text{ and } \\quad \\EE[f(X,Y)] = \\int f(X(\\omega), Y(\\omega)) \\td\\PP(\\omega).\n\\]\nUsually, to do real calculations we will eventually boil \\(\\PP\\) or \\(\\EE\\) into a composition of integrals and/or sums."
  },
  {
    "objectID": "reader/measure-theory-basics.html#conditional-probability",
    "href": "reader/measure-theory-basics.html#conditional-probability",
    "title": "Measure Theory Basics",
    "section": "Conditional probability",
    "text": "Conditional probability\nWhile it is beyond the scope of this course, measure theory also allows us to patch the definition of conditional probability and conditional expectation. Given two events \\(A\\) and \\(B\\), if \\(\\PP(B) &gt; 0\\), we can unproblematically define the conditional probability of \\(A\\) given \\(B\\) as \\(\\PP(A \\mid B) = \\PP(A \\cap B) / \\PP(B)\\), but this definition obviously fails when \\(\\PP(B) = 0\\).\nGenerally speaking, we cannot necessarily define \\(\\PP(A \\mid B)\\) for measure zero events \\(B\\) (Homework 0 includes a problem illustrating the inherent ambiguity of this definition). But, for example, if \\(X\\) and \\(Y\\) are both continuous random variables with some dependence between them we would like to be able to discuss, e.g., the distribution or expectation of \\(Y\\) given that \\(X\\) takes on some specific value \\(x\\). We can do this by defining the conditional expectation \\(\\EE(Y \\mid X)\\) as a random variable \\(g(X)\\), which has the property \\(\\EE[(Y - g(X)) 1_A(X)] = 0\\) for all (nice) subsets \\(A\\). By evaluating this function \\(g\\) at \\(x\\) we can answer the question we asked earlier. However, note this explanation is informal and brushes many important points under the rug; for a more complete explanation, take Stat 205A.\nHaving defined the conditional expectation, we can also ask about the conditional distribution of \\(Y\\) by evaluating the conditional expectation on new random variables defined with indicator functions: \\(\\PP(Y \\in A \\mid X) = \\EE[1_A(Y) \\mid X]\\) ."
  },
  {
    "objectID": "reader/measure-theory-basics.html#more-definitions",
    "href": "reader/measure-theory-basics.html#more-definitions",
    "title": "Measure Theory Basics",
    "section": "More definitions",
    "text": "More definitions\nFinally, this section includes some scattered definitions to remind you of a few more useful definitions which you likely have already seen in your previous probability courses.\nIf \\(\\PP(A) = 1\\), we say \\(A\\) occurs almost surely.\nThe variance of a random variable \\(X\\) is \\(\\textrm{Var}(X) = \\EE[X^2] - \\EE [X]\\)\nSee Keener Ch. 1 for more on probability."
  },
  {
    "objectID": "test-viz.html",
    "href": "test-viz.html",
    "title": "Sufficiency Visualization",
    "section": "",
    "text": "html`\n&lt;style&gt;\n  .plot-container {\n    display: flex;\n    justify-content: space-around;\n    flex-wrap: wrap;\n  }\n  .plot {\n    flex: 0 1 450px;\n    margin: 10px;\n  }\n  .plot-title {\n    text-align: center;\n    font-size: 20px;\n    font-weight: bold;\n    margin-bottom: 10px;\n  }\n&lt;/style&gt;\n`\n\n\n\n\n\n\n\nfunction binomialProbability(n, k, p) {\n  return combination(n, k) * Math.pow(p, k) * Math.pow(1 - p, n - k);\n}\n\nfunction combination(n, k) {\n  return factorial(n) / (factorial(k) * factorial(n - k));\n}\n\nfunction factorial(n) {\n  if (n === 0 || n === 1) return 1;\n  return n * factorial(n - 1);\n}\n\n// Input parameters\nviewof n = Inputs.range([2, 10], {step: 1, value: 5, label: \"n:\"})\nviewof theta = Inputs.range([0, 1], {step: 0.01, value: 0.5, label: \"θ:\"})\nviewof t = Inputs.range([0, 2 * n], {step: 1, value: n, label: \"T(X) = X₁ + X₂:\"})\n\n// Calculate probabilities\nprobabilities = {\n  let probs = [];\n  for (let x1 = 0; x1 &lt;= n; x1++) {\n    for (let x2 = 0; x2 &lt;= n; x2++) {\n      let prob = binomialProbability(n, x1, theta) * binomialProbability(n, x2, theta);\n      probs.push({x1, x2, prob, sum: x1 + x2});\n    }\n  }\n  return probs;\n}\n\n// Calculate conditional probabilities\nconditionalProbs = {\n  let totalProb = probabilities.filter(p =&gt; p.sum === t).reduce((sum, p) =&gt; sum + p.prob, 0);\n  return Array.from({length: n + 1}, (_, x1) =&gt; {\n    let x2 = t - x1;\n    let prob = (x2 &gt;= 0 && x2 &lt;= n) ? probabilities.find(p =&gt; p.x1 === x1 && p.x2 === x2)?.prob || 0 : 0;\n    return {x1, prob: prob / totalProb};\n  });\n}\n\n// Create plot container\nhtml`&lt;div class=\"plot-container\"&gt;\n  &lt;div class=\"plot\"&gt;\n    &lt;div class=\"plot-title\"&gt;P(X₁, X₂)&lt;/div&gt;\n    ${\n      Plot.plot({\n        width: 450,\n        height: 450,\n        margin: 60,\n        style: {\n          fontSize: \"14px\",\n          fontWeight: \"normal\"\n        },\n        x: {\n          label: \"X₂\",\n          domain: [-1, n],\n          tickFormat: d =&gt; d &gt;= 0 ? d : \"\",\n        },\n        y: {\n          label: \"X₁\",\n          domain: [-1, n],\n          tickFormat: d =&gt; d &gt;= 0 ? d : \"\",\n        },\n        marks: [\n          Plot.dot(probabilities, {\n            x: \"x2\",\n            y: \"x1\",\n            r: d =&gt; Math.sqrt(d.prob) * 200,\n            fill: d =&gt; d.sum === t ? \"red\" : \"black\",\n          }),\n          Plot.text([{x: n/2, y: -1.5}], {\n            text: [\"Distribution of (X₁, X₂)\"],\n            fontSize: 16,\n          }),\n        ]\n      })\n    }\n  &lt;/div&gt;\n  &lt;div class=\"plot\"&gt;\n    &lt;div class=\"plot-title\"&gt;P(X₁ | T(X))&lt;/div&gt;\n    ${\n      Plot.plot({\n        width: 450,\n        height: 450,\n        margin: 60,\n        style: {\n          fontSize: \"14px\",\n          fontWeight: \"normal\"\n        },\n        x: {\n          label: \"X₁\",\n        },\n        y: {\n          label: \"Probability\",\n          domain: [0, Math.max(...conditionalProbs.map(d =&gt; d.prob)) * 1.1],\n        },\n        marks: [\n          Plot.barY(conditionalProbs, {x: \"x1\", y: \"prob\", fill: \"red\"}),\n        ]\n      })\n    }\n  &lt;/div&gt;\n&lt;/div&gt;`"
  },
  {
    "objectID": "test-viz.html#visualization-of-the-sufficient-statistic-tx-x1-x2",
    "href": "test-viz.html#visualization-of-the-sufficient-statistic-tx-x1-x2",
    "title": "Sufficiency Visualization",
    "section": "",
    "text": "html`\n&lt;style&gt;\n  .plot-container {\n    display: flex;\n    justify-content: space-around;\n    flex-wrap: wrap;\n  }\n  .plot {\n    flex: 0 1 450px;\n    margin: 10px;\n  }\n  .plot-title {\n    text-align: center;\n    font-size: 20px;\n    font-weight: bold;\n    margin-bottom: 10px;\n  }\n&lt;/style&gt;\n`\n\n\n\n\n\n\n\nfunction binomialProbability(n, k, p) {\n  return combination(n, k) * Math.pow(p, k) * Math.pow(1 - p, n - k);\n}\n\nfunction combination(n, k) {\n  return factorial(n) / (factorial(k) * factorial(n - k));\n}\n\nfunction factorial(n) {\n  if (n === 0 || n === 1) return 1;\n  return n * factorial(n - 1);\n}\n\n// Input parameters\nviewof n = Inputs.range([2, 10], {step: 1, value: 5, label: \"n:\"})\nviewof theta = Inputs.range([0, 1], {step: 0.01, value: 0.5, label: \"θ:\"})\nviewof t = Inputs.range([0, 2 * n], {step: 1, value: n, label: \"T(X) = X₁ + X₂:\"})\n\n// Calculate probabilities\nprobabilities = {\n  let probs = [];\n  for (let x1 = 0; x1 &lt;= n; x1++) {\n    for (let x2 = 0; x2 &lt;= n; x2++) {\n      let prob = binomialProbability(n, x1, theta) * binomialProbability(n, x2, theta);\n      probs.push({x1, x2, prob, sum: x1 + x2});\n    }\n  }\n  return probs;\n}\n\n// Calculate conditional probabilities\nconditionalProbs = {\n  let totalProb = probabilities.filter(p =&gt; p.sum === t).reduce((sum, p) =&gt; sum + p.prob, 0);\n  return Array.from({length: n + 1}, (_, x1) =&gt; {\n    let x2 = t - x1;\n    let prob = (x2 &gt;= 0 && x2 &lt;= n) ? probabilities.find(p =&gt; p.x1 === x1 && p.x2 === x2)?.prob || 0 : 0;\n    return {x1, prob: prob / totalProb};\n  });\n}\n\n// Create plot container\nhtml`&lt;div class=\"plot-container\"&gt;\n  &lt;div class=\"plot\"&gt;\n    &lt;div class=\"plot-title\"&gt;P(X₁, X₂)&lt;/div&gt;\n    ${\n      Plot.plot({\n        width: 450,\n        height: 450,\n        margin: 60,\n        style: {\n          fontSize: \"14px\",\n          fontWeight: \"normal\"\n        },\n        x: {\n          label: \"X₂\",\n          domain: [-1, n],\n          tickFormat: d =&gt; d &gt;= 0 ? d : \"\",\n        },\n        y: {\n          label: \"X₁\",\n          domain: [-1, n],\n          tickFormat: d =&gt; d &gt;= 0 ? d : \"\",\n        },\n        marks: [\n          Plot.dot(probabilities, {\n            x: \"x2\",\n            y: \"x1\",\n            r: d =&gt; Math.sqrt(d.prob) * 200,\n            fill: d =&gt; d.sum === t ? \"red\" : \"black\",\n          }),\n          Plot.text([{x: n/2, y: -1.5}], {\n            text: [\"Distribution of (X₁, X₂)\"],\n            fontSize: 16,\n          }),\n        ]\n      })\n    }\n  &lt;/div&gt;\n  &lt;div class=\"plot\"&gt;\n    &lt;div class=\"plot-title\"&gt;P(X₁ | T(X))&lt;/div&gt;\n    ${\n      Plot.plot({\n        width: 450,\n        height: 450,\n        margin: 60,\n        style: {\n          fontSize: \"14px\",\n          fontWeight: \"normal\"\n        },\n        x: {\n          label: \"X₁\",\n        },\n        y: {\n          label: \"Probability\",\n          domain: [0, Math.max(...conditionalProbs.map(d =&gt; d.prob)) * 1.1],\n        },\n        marks: [\n          Plot.barY(conditionalProbs, {x: \"x1\", y: \"prob\", fill: \"red\"}),\n        ]\n      })\n    }\n  &lt;/div&gt;\n&lt;/div&gt;`"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework",
    "section": "",
    "text": "You may disregard measure-theoretic niceties about conditioning on measure-zero sets, almost-sure equality vs. actual equality, “all functions” vs. “all measurable functions,” etc. (unless the problem is explicitly asking about such issues).\nIf you need to write code to answer a question, show your code. If you need to include a plot, make sure the plot is readable, with appropriate axis labels and a legend if necessary. Points will be deducted for very hard-to-read code or plots.\nAnytime I ask you to calculate things numerically, it is implicit that Monte Carlo integration (calculating an expectation by repeatedly sampling data from an appropriate distribution and taking the average) is a valid numerical method. There is no need to ask permission, but please use good judgment about how many samples to take so that your numerical error is not too high: if you report a number it should be correct to a few significant digits; if you are comparing two numbers, the precision of your calculation should be high enough for the difference to be meaningful; and plots of smooth functions should appear smooth enough for the plot to be readable.\nUnless otherwise stated, assume asymptotic limits are taken as \\(n\\to \\infty\\). If I ask for a “limiting distribution,” I mean do an appropriate centering and scaling to find a limiting distribution that is non-degenerate (not converging in probability to a constant). That is, find sequences \\(a_n\\) and \\(b_n\\) such that \\(b_n(X_n−a_n)\\) converges to a non-degenerate limiting distribution."
  },
  {
    "objectID": "homework.html#standing-homework-instructions",
    "href": "homework.html#standing-homework-instructions",
    "title": "Homework",
    "section": "",
    "text": "You may disregard measure-theoretic niceties about conditioning on measure-zero sets, almost-sure equality vs. actual equality, “all functions” vs. “all measurable functions,” etc. (unless the problem is explicitly asking about such issues).\nIf you need to write code to answer a question, show your code. If you need to include a plot, make sure the plot is readable, with appropriate axis labels and a legend if necessary. Points will be deducted for very hard-to-read code or plots.\nAnytime I ask you to calculate things numerically, it is implicit that Monte Carlo integration (calculating an expectation by repeatedly sampling data from an appropriate distribution and taking the average) is a valid numerical method. There is no need to ask permission, but please use good judgment about how many samples to take so that your numerical error is not too high: if you report a number it should be correct to a few significant digits; if you are comparing two numbers, the precision of your calculation should be high enough for the difference to be meaningful; and plots of smooth functions should appear smooth enough for the plot to be readable.\nUnless otherwise stated, assume asymptotic limits are taken as \\(n\\to \\infty\\). If I ask for a “limiting distribution,” I mean do an appropriate centering and scaling to find a limiting distribution that is non-degenerate (not converging in probability to a constant). That is, find sequences \\(a_n\\) and \\(b_n\\) such that \\(b_n(X_n−a_n)\\) converges to a non-degenerate limiting distribution."
  },
  {
    "objectID": "homework.html#assignments",
    "href": "homework.html#assignments",
    "title": "Homework",
    "section": "Assignments",
    "text": "Assignments\n\nHomework 1 (LaTeX source)\nHomework 2 (LaTeX source)\nHomework 3 (LaTeX source)\nHomework 4 (LaTeX source)\nHomework 5 (Data for Problem 5) (LaTeX source)\nHomework 6 (LaTeX source)\nHomework 7 (LaTeX source)\nHomework 8 (LaTeX source)\nHomework 9 (LaTeX source)\nHomework 10 (Data for Problem 3) (LaTeX source)\nHomework 11 (LaTeX source)"
  },
  {
    "objectID": "units/macros.html",
    "href": "units/macros.html",
    "title": "",
    "section": "",
    "text": "\\[\n\\newcommand{\\trans}{^\\mathsf{T}}\n\\newcommand{\\eps}{\\epsilon}\n\\]"
  },
  {
    "objectID": "units/unit3.html",
    "href": "units/unit3.html",
    "title": "Unit 3: More",
    "section": "",
    "text": "This is an example of using qmd as the source document with pdf as one target. I’ve taken out the qmd stuff that doesn’t seem to render to pdf."
  },
  {
    "objectID": "units/unit3.html#evaluated-python-code-chunk-with-a-plot",
    "href": "units/unit3.html#evaluated-python-code-chunk-with-a-plot",
    "title": "Unit 3: More",
    "section": "Evaluated Python code chunk, with a plot",
    "text": "Evaluated Python code chunk, with a plot\n\n\nCode\nimport numpy as np\nx = np.random.normal(size=100)\nimport matplotlib.pyplot as plt\nplt.hist(x)\nplt.show()\nnp.mean(x)\n\n\n\n\n\n-0.08893663052001283"
  },
  {
    "objectID": "units/unit3.html#latex",
    "href": "units/unit3.html#latex",
    "title": "Unit 3: More",
    "section": "LaTeX",
    "text": "LaTeX\n\\[\n\\theta = \\int_0^\\infty f(x,\\theta)d\\theta\n\\]"
  },
  {
    "objectID": "units/unit3.html#latex-macro",
    "href": "units/unit3.html#latex-macro",
    "title": "Unit 3: More",
    "section": "LaTeX macro",
    "text": "LaTeX macro\n\nWarning: need to look back at this as having include-before-body in the yaml causes extra space at top of page.\n\n\\[\nA = X \\trans Y\n\\]"
  },
  {
    "objectID": "old-exams.html",
    "href": "old-exams.html",
    "title": "Old exams",
    "section": "",
    "text": "Fall 2024 (Solutions)\nFall 2023 (Solutions)\nFall 2021 (Solutions)\nFall 2020 (Solutions)\nFall 2019 (Solutions)"
  },
  {
    "objectID": "old-exams.html#exams-from-previous-semesters",
    "href": "old-exams.html#exams-from-previous-semesters",
    "title": "Old exams",
    "section": "",
    "text": "Fall 2024 (Solutions)\nFall 2023 (Solutions)\nFall 2021 (Solutions)\nFall 2020 (Solutions)\nFall 2019 (Solutions)"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Stat 210A is an introductory Ph.D.-level course in theoretical statistics. It is a fast-paced and demanding course intended to prepare students for research careers in statistics."
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "Syllabus",
    "section": "Course information",
    "text": "Course information\n\nInstructors\n\nPrimary Instructor Prof. Will Fithian\n\nOffice Hours: Tuesday 3:30-4:30pm on Zoom, Thursday 9:30-10:30am in 301 Evans\nEmail: wfithian@berkeley.edu\n\nGSI Dohyeong Ki\n\nOffice Hours: Wednesday 9-10am, Friday 4:30-5:30pm in 444 Evans\nEmail: dohyeong_ki@berkeley.edu\n\n\n\n\nCourse schedule\n\nLectures: Tuesday and Thursday 11am-12:30pm, Evans 60\nRecitation sections: Every second F 3:30-4:30pm in 444 Evans, starting September 6\nElection week:\n\nBlanket 2-day extension on homework 9 (due Friday November 8, 11:59pm)\n\nThanksgiving week:\n\nTuesday November 26: lecture 11-12:30 on Zoom, 3:30-4:30 OH on Zoom\nNo lecture Thursday November 28\nNo in-person office hours\n\nFinal exam review: (a.k.a. last recitation section) Friday, December\nFinal exam: Wed December 18, 8-11am\n\n\n\nCourse communications\n\nLecture videos and homework solutions at [https://bcourses.berkeley.edu bCourses]\nEmail policy: You can email me or Dohyeong about administrative questions, with “[Stat 210A]” in the subject line. No math over email, please.\nEd page for announcements and technical discussion (no homework spoilers!)\nGradescope for turning in homework"
  },
  {
    "objectID": "syllabus.html#about-stat-210a",
    "href": "syllabus.html#about-stat-210a",
    "title": "Syllabus",
    "section": "About Stat 210A",
    "text": "About Stat 210A\n\nWhat is the theory of statistics?\nStatistics is the study of methods that use data to understand the world. Statistical methods are used throughout the natural and social sciences, in machine learning and artificial intelligence, and in engineering. Despite the ubiquitous use of statistics, its practitioners are perpetually accused of not actually understanding what they are doing. Statistics theory is, broadly speaking, the subject of what exactly we are doing when we apply statistical methods.\nWhile there are many possible ways to analyze data, most (but certainly not all) statistical methods are based on statistical modeling: treating the data as a realization of some random data-generating process with attributes, usually called parameters, that are a priori unknown. The goal of the analyst, then, is to use the data to draw accurate inferences about these parameters and/or to make accurate predictions about future data. If the modeling has been done well (a very big “if”) then these unknown parameters will correspond well to whatever real-world questions initially motivated the analysis. Applied statistics courses like Stat 215A and B delve deeply into questions about how to ensure that the statistical modeling exercise successfully captures something interesting about reality.\nIn this course we will instead focus on how the analyst can use the data most effectively within the context of a given mathematical setup. We will discuss the structure of statistical models, how to evaluate the quality of a statistical method, how to design good methods for new settings, and the philosophy of Bayesian vs frequentist modeling frameworks. We will cover estimation, confidence intervals, and hypothesis testing, in parametric and nonparametric methods, in finite samples and asymptotic regimes.\n\n\nTopics\nStatistical decision theory (frequentist and Bayesian), exponential families, point estimation, hypothesis testing, resampling methods, estimating equations and maximum likelihood, empirical Bayes, large-sample theory, high-dimensional testing, multiple testing and selective inference.\n\n\nPrerequisites\nThe course prerequisites are linear algebra, analysis, probability, and statistics.\n\n\nRelationship of Stat 210A to other Berkeley courses\nStat 210A focuses on classical statistical contexts: either inference in finite samples, or in fixed-dimensional asymptotic regimes. Stat 210B (for which 210A is a prerequisite) is more technical and covers topics like empirical process theory and high-dimensional statistics.\nBerkeley’s graduate course on Statistical Learning Theory (CS 281A / Stat 241A) is also very popular and has some overlap in its topics. Roughly speaking, it is more tilted toward “machine learning”: it spends more time on topics in predictive modeling (i.e. classification and regression, which are covered in Stat 215A), optimization, and signal processing, but spends less time on inferential questions and (I believe) does not cover topics like hypothesis testing, confidence intervals, and causal inference. Both courses cover estimation and exponential families."
  },
  {
    "objectID": "syllabus.html#references",
    "href": "syllabus.html#references",
    "title": "Syllabus",
    "section": "References",
    "text": "References\nThe online notes for this course are self-contained, however it can be helpful to see a different presentation in the following supplementary texts (all links are to public websites or Springer Link):\n\nKeener, Theoretical Statistics: Topics for a Core Course, Springer 2010. The textbook that is closest in technical level and presentation style to our course reader.\nLehmann and Casella, Theory of Point Estimation, Springer 1998. A highly-detailed reference text that covers much of the estimation material in this course.\nLehmann and Romano, Testing Statistical Hypotheses, Springer 2005. A highly-detailed reference text covering much of the material on testing and confidence estimation.\nHacking, Probability and Inductive Logic, Cambridge University Press, 2001. A beautifully written book that treats probability and statistics from a philosophical point of view.\nCandes, Stats 300C Lecture notes, Stanford 2016. The course notes for a great course at Stanford that covers some of the later material in this course.\n\nUndergrad-level review texts for prerequisites:\n\nAxler, Linear Algebra Done Right\nAbbott, Understanding Analysis\nAdhikari & Pitman, Probability for Data Science"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nYour final grade is based on:\n\nWeekly problem sets: 50%\nFinal exam: 50%\n\nLateness policy: Homework must be submitted to Gradescope at 11:59pm on Wednesday nights. Late problem sets will not be accepted, but we will drop your lowest two grades.\nCollaboration policy: For homework, you are welcome to work with each other or consult articles or textbooks online, with the following caveats:\n\nYou must write up your solution by yourself.\nYou may NOT consult any solutions from previous iterations of this course.\nNo generative AI allowed for problem sets.\nIf you collaborate or use any resources other than course texts, you must acknowledge your collaborators and the resources you used.\n\nAcademic integrity: You are expected to abide by the Berkeley honor code. Violating the collaboration policy, or cheating in any other way, will result in a failing grade for the semester and you will be reported to the University Office of Student Conduct.\nWhile the final exam is nominally one half of the grade, it is quite difficult and typically accounts for most of the variance in final course grades. If you take shortcuts on the homework, you will save yourself time in the short run, but you may do quite poorly on the final."
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\nStudents with disabilities: Please see me as soon as possible if you need particular accommodations, and we will work out the necessary arrangements.\nScheduling conflicts: Please notify me in writing by the second week of the semester about any known or potential extracurricular conflicts (such as religious observances, graduate or medical school interviews, or team activities). I will try my best to help you with making accommodations, but cannot promise them in all cases. In the event there is no mutually-workable solution, you may be dropped from the class.\nExam accommodations: If you need accommodations on the final exam due to disability, or unavoidable travel or time conflict, please fill out the exam exam accommodation form by Friday, October 4 so that I can make arrangements. To ensure exam integrity I much prefer for all students to take the exam in Berkeley at the regularly scheduled time (8am December 18th), but will try to work with you if you have a conflict."
  }
]