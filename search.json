[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "We’ll use data from several real world situations in class."
  },
  {
    "objectID": "data.html#data-1",
    "href": "data.html#data-1",
    "title": "Data",
    "section": "Data 1",
    "text": "Data 1"
  },
  {
    "objectID": "data.html#data-2",
    "href": "data.html#data-2",
    "title": "Data",
    "section": "Data 2",
    "text": "Data 2"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Who can / should take this course?\nThis is a fast-paced and demanding course designed to prepare PhD students for research careers in statistics. Undergraduates and PhD students in other fields are very welcome to take the class, and many have succeeded in the past. But you should be prepared to work hard.\nCan I take this course if I have a conflict with the lecture time?\nAttendance is not taken at lectures but attending is an important part of taking the course. If you want to take another course at the same time, you can do it as long as the other professor is OK with your not attending their lectures.\nHow can I prepare for the course?\nThe course prerequisites are undergraduate-level linear algebra, real analysis, and a year of upper-division probability and statistics. If you are unsure of your background in one or more of these, the texts below can be useful review materials. All books linked below, except Gelman & Hill, should be available for free with a Berkeley library subscription. Email me if you have trouble accessing them.\n\nLinear algebra: Fluency with undergrad-level abstract linear algebra is essential to understanding the course content (numerical tools like LU or Cholesky decompositions are not essential). Chapters 1-3 and 5-6 of Linear Algebra Done Right (Axler) are good review materials.\nReal analysis: Familiarity with and intuition for ideas of infinite sequences, convergence, Taylor approximations, etc. should be enough. Chapters 1-3 of Understanding Analysis (Abbott) are good review materials.\nProbability: The whole course involves probability. If Chapters 1-6, 8-9, 13-17, and 23 of Probability for Data Science (Adhikari and Pitman) aren’t mostly review for you, I strongly recommend studying them before the course begins.\nStatistics: We will derive all statistical results from first principles, but at a fairly technical level. If you have never seen them in an applied context, Chapters 1-6 of Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman & Hill) should give you some frame of reference."
  },
  {
    "objectID": "units/unit3.html",
    "href": "units/unit3.html",
    "title": "Unit 3: More",
    "section": "",
    "text": "This is an example of using qmd as the source document with pdf as one target. I’ve taken out the qmd stuff that doesn’t seem to render to pdf."
  },
  {
    "objectID": "units/unit3.html#evaluated-python-code-chunk-with-a-plot",
    "href": "units/unit3.html#evaluated-python-code-chunk-with-a-plot",
    "title": "Unit 3: More",
    "section": "Evaluated Python code chunk, with a plot",
    "text": "Evaluated Python code chunk, with a plot\n\n\nCode\nimport numpy as np\nx = np.random.normal(size=100)\nimport matplotlib.pyplot as plt\nplt.hist(x)\nplt.show()\nnp.mean(x)\n\n\n\n\n\n-0.04367339374561691"
  },
  {
    "objectID": "units/unit3.html#latex",
    "href": "units/unit3.html#latex",
    "title": "Unit 3: More",
    "section": "LaTeX",
    "text": "LaTeX\n\\[\n\\theta = \\int_0^\\infty f(x,\\theta)d\\theta\n\\]"
  },
  {
    "objectID": "units/unit3.html#latex-macro",
    "href": "units/unit3.html#latex-macro",
    "title": "Unit 3: More",
    "section": "LaTeX macro",
    "text": "LaTeX macro\n\nWarning: need to look back at this as having include-before-body in the yaml causes extra space at top of page.\n\n\\[\nA = X \\trans Y\n\\]"
  },
  {
    "objectID": "units/macros.html",
    "href": "units/macros.html",
    "title": "",
    "section": "",
    "text": "\\[\n\\newcommand{\\trans}{^\\mathsf{T}}\n\\newcommand{\\eps}{\\epsilon}\n\\]"
  },
  {
    "objectID": "handwritten-notes.html",
    "href": "handwritten-notes.html",
    "title": "Handwritten notes",
    "section": "",
    "text": "Lecture 1\nLecture 2"
  },
  {
    "objectID": "handwritten-notes.html#handwritten-lecture-notes",
    "href": "handwritten-notes.html#handwritten-lecture-notes",
    "title": "Handwritten notes",
    "section": "",
    "text": "Lecture 1\nLecture 2"
  },
  {
    "objectID": "reader/introduction.html",
    "href": "reader/introduction.html",
    "title": "Course introduction",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/introduction.html#what-is-the-theory-of-statistics",
    "href": "reader/introduction.html#what-is-the-theory-of-statistics",
    "title": "Course introduction",
    "section": "What is the theory of statistics?",
    "text": "What is the theory of statistics?\nStatistics is the study of methods that use data to understand the world. Statistical methods are used throughout the natural and social sciences, in machine learning and artificial intelligence, and in engineering. Despite the ubiquitous use of statistics, its practitioners are perpetually accused of not actually understanding what they are doing. Statistics theory is, broadly speaking, about trying to understand what we are doing when we use statistical methods.\nWhile there are many possible ways to analyze data, most (but certainly not all) statistical methods are based on statistical modeling: treating the data as a realization of some random data-generating process with attributes, usually called parameters, that are a priori unknown. The goal of the analyst, then, is to use the data to draw accurate inferences about these parameters and/or to make accurate predictions about future data. If the modeling has been done well (a very big “if”) then these unknown parameters will correspond well to whatever real-world questions initially motivated the analysis. Applied statistics courses like Stat 215A and B delve deeply into questions about how to ensure that the statistical modeling exercise successfully captures something interesting about reality.\nIn this course we will instead focus on how the analyst can use the data most effectively within the context of a given mathematical setup. We will discuss the structure of statistical models, how to evaluate the quality of a statistical method, how to design good methods for new settings, and the philosophy of Bayesian vs frequentist modeling frameworks. We will cover estimation, confidence intervals, and hypothesis testing, in parametric and nonparametric methods, in finite samples and asymptotic regimes."
  },
  {
    "objectID": "reader/introduction.html#relationship-of-stat-210a-to-other-berkeley-courses",
    "href": "reader/introduction.html#relationship-of-stat-210a-to-other-berkeley-courses",
    "title": "Course introduction",
    "section": "Relationship of Stat 210A to other Berkeley courses",
    "text": "Relationship of Stat 210A to other Berkeley courses\nStat 210A focuses on classical statistical contexts: either inference in finite samples, or in fixed-dimensional asymptotic regimes. Stat 210B (for which 210A is a prerequisite) is more technical and covers topics like empirical process theory and high-dimensional statistics.\nBerkeley’s graduate course on Statistical Learning Theory (CS 281A / Stat 241A) is also very popular and has some overlap in its topics. Roughly speaking, it is more tilted toward “machine learning”: it spends more time on topics in predictive modeling (i.e. classification and regression, which are covered in Stat 215A), optimization, and signal processing, but spends less time on inferential questions and (I believe) does not cover topics like hypothesis testing, confidence intervals, and causal inference. Both courses cover estimation and exponential families."
  },
  {
    "objectID": "reader/completeness.html",
    "href": "reader/completeness.html",
    "title": "Completeness, Ancillarity, and Basu’s Theorem",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/completeness.html#completeness",
    "href": "reader/completeness.html#completeness",
    "title": "Completeness, Ancillarity, and Basu’s Theorem",
    "section": "1 Completeness",
    "text": "1 Completeness\nAs we have seen, for a given statistical problem we may have many different sufficient statistics, some of which reduce the data more than others. We usually want to look for one that is minimal sufficient, meaning that it strips away as much irrelevant information as possible and only retains the information that is relevant to estimating the parameter.\nIn some cases the minimal sufficient statistic has an additional property called completeness. The definition of completeness is initially counterintuitive, but it has a number of useful implications we will explore throughout the semester.\n\n1.1 Definition of completeness\nA statistic \\(T(X)\\) is complete for a family of distributions \\(\\cP = \\{P_\\theta: \\theta \\in \\Theta\\}\\) if no nontrivial function of \\(T\\) can have expectation zero for every distribution in the family:\n\\[\\EE_\\theta \\,f(T(X)) = 0 \\quad \\forall \\theta \\in \\Theta \\implies f(T) \\eqPas 0\\] ::: callout-note The name for complete statistics comes from a prior notion that \\(\\cP^T = \\{P_\\theta^T:\\; \\theta \\in \\Theta\\}\\) is ``complete’’ as a model if its linear span includes all possible distributions on \\(T(X)\\); see Homework 3. :::\nIf \\(X\\) itself is complete, then the definition immediately implies that there can be at most one unbiased estimator for any estimand: if \\(\\EE_\\theta \\delta_1(X) = \\EE_\\theta \\delta_2(X) = g(\\theta)\\) for all \\(\\theta \\in \\Theta\\), then \\(f(X) = \\delta_1(X) - \\delta_2(X) = 0\\) almost surely. More generally, if \\(T(X)\\) is a complete statistic then there can be at most one unbiased estimator that runs through \\(T\\). We will return to this fact when we discuss unbiased estimation.\nWe will be especially interested in statistics that are both complete and sufficient. If \\(T(X)\\) is complete and sufficient we call it a complete sufficient statistic.\n\n\n\n\n\n\nWarning\n\n\n\nA complete statistic need not be sufficient: the constant “statistic” \\(T(X) \\equiv 0\\) is complete in any model. In general, to show that \\(T(X)\\) is complete sufficient we must establish both properties.\n\n\n\n\n1.2 Examples\nExample 1 (Laplace location family): Let \\(X_1,\\ldots,X_n \\simiid \\text{Lap}(\\theta)\\) for \\(\\theta \\in \\RR\\), and recall that the vector of order statistics \\(S(X) = (X_{(1)},\\ldots,X_{(n)})\\) is a minimal sufficient statistic. Is \\(S(X)\\) complete?\n\n\n\n\n\n\nExpand to see answer\n\n\n\n\n\nNo, \\(S(X)\\) is not complete.\nBoth the sample median \\(\\text{Med}(S)\\) (as defined in Homework 2 Problem XX) and sample mean \\(\\overline{X}(S)\\) can be calculated using \\(S(X)\\) alone, and both are unbiased estimators for \\(\\theta\\). Hence \\(f(S) = \\text{Med}(S) - \\overline{X}(S)\\) has expectation zero, but is not almost surely equal to zero because the median and mean are not equal to each other.\n\n\n\nExample 1 (Uniform scale family): Let \\(X_1, \\ldots, X_n \\simiid U[0, \\theta]\\), for \\(\\theta &gt; 0\\). We showed previously that the maximum \\(T(X) = X_{(n)}\\) is minimal sufficient. Is it complete?\n\n\n\n\n\n\nExpand to see answer\n\n\n\n\n\nYes, \\(T(X)\\) is complete.\nAs we showed previously, the density of \\(T(X)\\) for \\(t &gt; 0\\) is \\[\np_\\theta(t) =  \\frac{nt^{n-1}}{\\theta^n}\\,\\cdot \\;1\\{t \\leq \\theta\\}.\n\\]\nSuppose we could find \\(f(t)\\) such that \\[\n0 = \\EE_\\theta f(T) = \\frac{n}{\\theta^n}\\int_0^\\theta f(t) t^{n-1} \\,dt, \\quad \\text{ for all } \\theta &gt; 0.\n\\] Dividing the last expression by \\(n/\\theta^n\\) and then differentiating with respect to \\(\\theta\\), we obtain \\[\n0 = f(\\theta) \\theta^{n-1}, \\quad \\text{ for all } \\theta &gt; 0,\n\\] hence \\(f \\equiv 0\\).\n\n\n\n\n\n1.3 Full-rank exponential families\nIn the general case where \\(T(X)\\) can take on infinitely many values, it is hard to show completeness because the space of possible counterexample functions \\(f\\) is infinite-dimensional. But there is an important class of examples where we can quickly verify complete sufficiency, as we see next.\nDefinition: Let \\(\\cP = \\{P_\\eta:\\; \\eta \\in \\Xi\\}\\) be an \\(s\\)-parameter exponential family with densities \\[\np_\\eta(x) = e^{\\eta'T(x) - A(\\eta)} h(x),\n\\] with respect to some carrier measure \\(\\mu\\). Assume further that the sufficient statistic \\(T(X)\\) satisfies no affine constraint: that is, there is no \\(\\alpha \\in \\RR\\) and nonzero \\(\\beta \\in \\RR^s\\) with \\(\\beta'T(x) \\eqPas \\alpha\\).\nIf \\(\\Xi\\) contains an open set we say \\(\\cP\\) is full-rank; otherwise we say it is curved.\n\n\n\n\n\n\nNote\n\n\n\nIf \\(T(X)\\) does satisfy a linear constraint, that means \\(\\cP\\) can be defined equivalently as an \\(r\\)-parameter exponential family for some \\(r &lt; s\\). It may be full-rank or curved depending on the parameter space in a lower-dimensional parameterization.\n\n\nTheorem (Complete sufficiency in full-rank exponential families): If \\(\\cP\\) is a full-rank \\(s\\)-parameter exponential family, then \\(T(X)\\) is complete sufficient.\nThe proof is somewhat technical and uses the uniqueness of moment-generating functions.\n\n\n\n\n\n\nExpand for proof\n\n\n\n\n\n\\(T(X)\\) is sufficient by the factorization theorem, so it remains only to prove completeness.\nAssume without loss of generality that \\(0\\) is in the interior of \\(\\Xi\\); otherwise we can reparameterize. Assume also that \\(\\cP\\) is in canonical form, i.e. \\(T(X) = X\\) and \\(p_\\eta(x) = e^{\\eta'x - A(\\eta)}\\); in general we can always reduce to this case by making a sufficiency reduction and taking \\(P_0^T\\) as the carrier measure.\nAny measurable function \\(f\\) can be decomposed as \\(f(x) = f^+(x) - f^-(x)\\) where \\(f^+\\) and \\(f^-\\) are non-negative measurable functions. If \\(\\EE_\\eta f(X) = \\int (f^+-f^-)p_\\eta \\,d\\mu = 0\\) for all \\(\\eta\\in\\Xi\\), then we have \\[\n\\int e^{\\eta'x} f^+(x) \\,d\\mu(x) = \\int e^{\\eta'x} f^-(x) \\,d\\mu(x), \\quad \\text{ for all } \\eta \\in \\Xi.\n\\tag{1}\\] By assumption, \\(\\Xi\\) contains an open neighborhood that includes \\(0\\) on which both integrals are finite. Let \\(c = \\int f^+(x)\\,d\\mu(x) \\geq 0\\).\nIf \\(c&gt;0\\) then we can define the random variables \\(Y^+\\) and \\(Y^-\\) with probability densities \\(f^+(x)/c\\) and \\(f^-(x)/c\\) respectively; then Equation 1 implies that \\(Y^+\\) and \\(Y^-\\) have equal MGFs in a neighborhood of \\(0\\); hence they have the same distribution and their densities must be a.s. equal to each other. But \\(f^+(x)=f^-(x)\\) only when both are zero, so we have \\(f^+, f^- \\eqmuas 0\\).\n\n\n\nThe next figure shows three cases for exponential families with the same sufficient statistic. The set \\(\\Xi_1\\) indicates the full natural parameter space for a generic 2-parameter exponential family, and (A), (B), and (C) denote parameter spaces for three different subfamilies. The subfamily described by the shaded circle (A) is a full-rank exponential family, because it contains an open set. The subfamily described by the curve (B) is a typical example of a curved family, because it does not contain an open set. The subfamily described by the line segment (C) is technically curved according to the definition above, but we could view it as a full-rank \\(1\\)-parameter exponential family after reparameterizing it.\n\n\n\nThis figure shows three cases:\n\n\n\n\n1.4 Complete sufficient statistics are minimal\nA second convenient property of completeness is that complete sufficient statistics are always minimal sufficient.\nTheorem: If \\(T(X)\\) is complete sufficient for the family \\(\\mathcal{P}\\), then \\(T(X)\\) is minimal sufficient for \\(\\cP\\).\nThe way that we usually use completeness in proofs is to show that two quantities are almost surely equal by showing that they have the same expectation. The next proof is an example.\nProof: Let \\(S(X)\\) represent any minimal sufficient statistic, and define the conditional expectation of \\(T\\) given \\(S\\): \\[\n\\overline{T}(S(X)) = \\EE[T(X) \\mid S(X)].\n\\] Note that this conditional expectation does not depend on the parameter \\(\\theta\\), because \\(S(X)\\) is sufficient, so \\(\\overline{T}\\) is a valid statistic. If we can show that \\(\\overline{T} \\eqas T(X)\\), that means we can calculate \\(T(X)\\) from \\(S(X)\\), so \\(T(X)\\) is also minimal sufficient.\nBecause \\(S(X)\\) is minimal sufficient, we can write it as \\(f(T(X))\\) for some function \\(f\\), and use \\(f\\) to define a function \\(g\\) giving the difference between \\(T\\) and \\(\\overline{T}\\): \\[\ng(t) = t - \\overline{T}(f(t)).\n\\] The expectation of \\(g(T)\\) is always zero, because \\[\n\\begin{aligned}\n\\EE_\\theta\\; g(T(X)) &= \\EE_\\theta\\; T(X) - \\EE_\\theta\\; \\overline{T}(S(X)) \\\\[5pt]\n&= \\EE_\\theta\\; T(X) - \\EE_\\theta\\left[\\EE[T(X) \\mid S(X)]\\right]\\\\\n&= 0.\n\\end{aligned}\n\\] As a result, \\(g(T) \\eqas 0\\) and hence \\(T \\eqas \\overline{T}\\), as desired."
  },
  {
    "objectID": "reader/completeness.html#ancillarity",
    "href": "reader/completeness.html#ancillarity",
    "title": "Completeness, Ancillarity, and Basu’s Theorem",
    "section": "2 Ancillarity",
    "text": "2 Ancillarity\nWe’ve spent a lot of time discussing sufficient statistics, which are statistics that carry all of the information about \\(\\theta\\). Our next definition describes a type of statistic that carry no information about the parameter.\nDefinition: We say \\(V(X)\\) is ancillary for the model \\(\\cP = \\{P_\\theta: \\theta \\in \\Theta\\}\\) if its distribution does not depend on \\(\\theta\\).\nJust as the sufficiency principle tells us that our inference procedures should depend only on information from sufficient statistics, an analogous principle suggests in effect that procedures should depend as little as possible on ancillary statistics. Specifically, it recommends treating \\(V(X)\\) as a fixed value and evaluating the rest of the data set according to its distribution conditional on \\(V(X)\\):\nConditionality Principle: If \\(V(X)\\) is ancillary, then all inference should be conditional on \\(V(X)\\).\nIt may not be immediately clear why conditioning on \\(V(X)\\) “removes” it from the problem, but we will return to the idea of conditional inference during our unit on hypothesis testing and interval estimation, where it will play an important role."
  },
  {
    "objectID": "reader/completeness.html#basus-theorem",
    "href": "reader/completeness.html#basus-theorem",
    "title": "Completeness, Ancillarity, and Basu’s Theorem",
    "section": "3 Basu’s Theorem",
    "text": "3 Basu’s Theorem\nBasu’s Theorem gives us a simple way to prove that statistics are independent of one another using the definitions introduced above.\nTheorem (Basu): If \\(T(X)\\) is complete sufficient and \\(V(X)\\) ancillary for the model \\(\\cP\\), then \\(V(X) \\indep T(X)\\) for all \\(\\theta \\in \\Theta\\).\nAgain, for this proof our strategy will be to show that two quantities are almost surely equal to each other, by showing that they have the same expectation for all \\(\\theta\\).\nProof: Define the following two quantities representing the marginal and conditional probabilities that \\(V\\) falls into a generic set \\(A\\). \\[\n\\begin{aligned}\np_A &= \\PP(V \\in A)\\\\[5pt]\nq_A(T(X)) &= \\PP(V \\in A \\mid T(X))\n\\end{aligned}\n\\] Note that \\(p_A\\) does not depend on \\(\\theta\\) by ancillarity of \\(V\\), while \\(q_A\\) does not depend on \\(\\theta\\) by sufficiency of \\(T\\).\nThe expectation of their difference is \\[\n\\EE_\\theta\\left[q_A(T) - p_A\\right] = p_A - p_A = 0, \\quad \\text{ for all } \\theta.\n\\] By completeness of \\(T\\), this implies that \\(q_A(T) \\eqas p_A\\): the conditional probability equals the marginal probability. Hence, for any \\(B\\), we have \\[\n\\begin{aligned}\n\\PP_\\theta(V \\in A, T \\in B) &= \\int q_A(t) 1\\{t \\in B\\}\\,dP_\\theta^T(t)\\\\\n&= \\int p_A 1\\{t \\in B\\}\\,dP_\\theta^T(t)\\\\\n&= \\PP_\\theta(V \\in A) \\PP_\\theta(T \\in B).\n\\end{aligned}\n\\]\n\n3.1 Using Basu’s Theorem\nBasu’s Theorem can be helpful in proving independence. To use it, remember that the hypotheses of the theorem (sufficiency, completeness, and ancillarity) are all defined with respect to a family \\(\\cP\\). The conclusion, however, is defined with respect to individual distributions. As a result, when we apply the theorem we can often benefit from being a little clever about how to define \\(\\cP\\). The following example should make this clear:\nExample (Independence of sample mean and sample variance for Gaussian): Assume \\(X_1,\\ldots,X_n \\simiid \\cN(\\mu, \\sigma^2)\\) for \\(\\mu \\in \\RR\\) and \\(\\sigma^2 &gt; 0\\). Define the sample mean and sample variance as \\[\n\\begin{aligned}\n\\overline{X} &= \\frac{1}{n}\\sum_{i=1}^n X_i\\\\[5pt]\nS^2 &= \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\overline{X})^2\n\\end{aligned}\n\\] We would like to show \\(\\overline{X} \\indep S^2\\).\nInitially the approach of applying Basu’s Theorem appears hopeless because, in the model with \\(\\mu\\) and \\(\\sigma^2\\) unknown, neither of these two statistics is ancillary or sufficient. However, we can nevertheless apply Basu’s Theorem if we are just a bit more clever:\n\n\n\n\n\n\nExpand for answer\n\n\n\n\n\nConsider the model \\(\\cP\\) with known \\(\\sigma^2 &gt; 0\\) and unknown \\(\\mu\\in \\RR\\). This \\(\\cP\\) is a one-parameter full-rank exponential family with complete sufficient statistic \\(\\overline{X}\\). Moreover, \\(S^2\\) is ancillary, since we can write \\[\nS^2 = \\sum_{i=1}^n (Z_i - \\overline{Z})^2, \\quad \\text{ for } Z_i = X_i - \\mu.\n\\] Because the distribution of \\(Z_1,\\ldots,Z_n \\simiid N(0,\\sigma^2)\\) is known, it follows that the distribution of \\(S^2\\) is known as well (specifically, \\(S^2/\\sigma^2\\) is a \\(\\chi^2\\) random variable with \\(n-1\\) degrees of freedom). Since \\(\\mu\\) is the only unknown parameter, \\(S^2\\) is therefore ancillary in \\(\\cP\\). Applying Basu’s theorem, we have \\(\\overline{X} \\indep S^2\\) for any \\(\\mu \\in \\RR\\). But \\(\\sigma^2\\) was arbitrary, so we have the result for all \\(\\mu\\) and \\(\\sigma^2\\)."
  },
  {
    "objectID": "reader/exponential-families.html",
    "href": "reader/exponential-families.html",
    "title": "Exponential Families",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/exponential-families.html#exponential-family-structure",
    "href": "reader/exponential-families.html#exponential-family-structure",
    "title": "Exponential Families",
    "section": "Exponential family structure",
    "text": "Exponential family structure\nThus far we have discussed statistical models in the abstract without making any assumptions about them. Exponential families, the topic of this lecture, are models with a special structure that makes them especially easy to work with.\n\nWe say the model \\(\\cP = \\{P_\\eta:\\eta \\in \\Xi\\}\\) is an \\(s\\)-parameter exponential family if it is defined by a family of densities of the form:\n\\[\np_\\eta(x) = e^{\\eta'T(x) - A(\\eta)}h(x),\n\\tag{1}\\]\nall with respect to a common dominating measure \\(\\mu\\), i.e. a measure \\(\\mu\\) such that \\(P_\\eta \\ll \\mu\\) for all \\(\\eta\\in \\Xi\\).\nThe different parts of the expression in Equation 1 have distinct names referring to the role they play in defining the density:\n\\[\n\\begin{aligned}\nT&:\\; \\cX \\to \\RR^s &\\qquad &\\text{ is called the {\\it sufficient statistic}}\\\\\nh&:\\; \\cX \\to [0,\\infty) &\\qquad &\\text{ is called the {\\it carrier density} or {\\it base density}}\\\\\n\\eta &\\in \\Xi \\subseteq \\RR^s &\\qquad &\\text{ is called the {\\it natural parameter}}\\\\\nA&:\\; \\Xi \\to \\RR &\\qquad &\\text{ is called the {\\it log-partition function}}\n\\end{aligned}\n\\]\nThe function \\(A(\\eta)\\) is totally determined by \\(T\\) and \\(h\\), and plays the role of normalizing the density so that \\(P_\\eta(\\cX) = 1\\):\n\\[\nA(\\eta) = \\log\\left( \\int_\\cX e^{\\eta'T(x)}h(x)\\td\\mu(x)\\right) \\leq \\infty\n\\tag{2}\\]\nIf \\(A(\\eta) = \\infty\\) then there is no way to normalize \\(p_\\eta\\), so \\(\\eta\\) is not an allowed value for the natural parameter to take. The natural parameter space, which we denote \\(\\Xi_1\\), is the set of all \\(\\eta\\) values for which the family is normalizable:\n\\[\n\\Xi_1 = \\{\\eta:\\; A(\\eta) &lt; \\infty\\} \\subseteq \\RR^s.\n\\]\nWe will show in Homework 1 that \\(A(\\eta)\\) is a convex function, so \\(\\Xi_1\\) is a convex set.\nNote that, because \\(\\mu\\) is allowed to be an arbitrary measure, \\(h(x)\\) also plays an inessential role since we could always absorb it into the base measure \\(\\mu\\). More precisely, suppose we define a new dominating measure \\(\\nu\\) whose density with respect to \\(\\mu\\) is \\(h\\) (informally we can write \\(\\td\\nu = h\\td\\mu\\)). Then \\(P_\\eta\\), which had density \\(e^{\\eta'T(x)}h(x)\\) with respect to \\(\\mu\\), now has density \\(e^{\\eta'T(x)}\\) with respect to \\(\\nu\\).\nAs a result, if we were aiming for maximal parsimony we could assume without loss of generality that \\(h(x) = 1\\), removing it from the definition Equation 1 and replacing \\(\\mu\\) with our new, bespoke measure \\(\\nu\\). However, it is convenient to leave Equation 1 as is if it allows us to take \\(\\mu\\) to be some simple default measure like a counting measure or Lebesgue measure. In that case, \\(p_\\eta\\) will be a standard pmf or pdf, so we can discuss it without going over the head of anyone who lacks a background in measure theory.\nExample (Poisson): The Poisson distribution \\(\\textrm{Pois}(\\lambda)\\) has probability mass function\n\\[\np_\\lambda(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad \\textrm{ on } x = 0, 1, 2, \\ldots.\n\\]\nFormally this pmf is a density over the counting measure on the set of non-negative integers \\(\\ZZ_+ = \\{0,1,\\ldots\\}\\).\nLetting \\(\\lambda\\) range over \\([0, \\infty)\\) yields an exponential family, but it is not immediately obvious from the form of the density. To see this, we need to massage \\(p_\\lambda(x)\\) a bit, by observing that\n\\[\np_\\lambda(x) = \\exp\\{(\\log \\lambda) x - \\lambda\\}\\frac{1}{x!}.\n\\]\nNow we see that we can reparameterize the family by setting \\(\\eta = \\log\\lambda\\), leading to\n\\[\np_\\eta(x) = \\exp\\{\\eta x - e^\\eta\\} \\frac{1}{x!}.\n\\] At this point, we immediately recognize \\(p_\\eta\\) as an exponential family with sufficient statistic \\(T(x) = x\\), and base density \\(h(x) = \\frac{1}{x!}\\), and log-partition function \\(A(\\eta) = e^{\\eta}\\).\nNote that this is not the only way to decompose the Poisson distribution as an exponential family. For example, we could just as well take \\(T(x) = x/2\\); then we would have \\(\\eta = 2\\log\\lambda\\) and \\(A(\\eta) = e^{\\eta/2}\\). Or, we could take \\(T(x) = x + 1\\) and \\(A(\\eta) = e^{\\eta} + \\eta\\).\nThis is not just a property of the Poisson distribution; the decomposition is generally non-unique for exponential families. For any exponential family of the form Equation 1 , if \\(U \\in \\RR^{s\\times s}\\) is invertible and \\(v \\in \\RR^s\\) then we can write the same density as \\(e^{\\zeta'S(x) - B(\\zeta)}h(x)\\), for new sufficient statistic \\(S(x) = U T(x) + v\\), new natural parameter \\(\\zeta = (U^{-1})'\\eta\\), and and new log-partition function \\(B(\\zeta) = A(U'\\zeta) + \\zeta'U^{-1}v\\). So “the” sufficient statistic of an exponential family is defined only up to invertible affine transformations."
  },
  {
    "objectID": "reader/exponential-families.html#differential-identities",
    "href": "reader/exponential-families.html#differential-identities",
    "title": "Exponential Families",
    "section": "Differential identities",
    "text": "Differential identities\nExponentiating Equation 2, we obtain the equation\n\\[\ne^{A(\\eta)} = \\int_\\cX e^{\\eta'T(x)}h(x)\\td\\mu(x)\n\\tag{3}\\]\nWe can derive many interesting identities by differentiating this function, and related functions, with respect to \\(\\eta\\). We will always evaluate derivatives by differentiating under the integral sign. This is not always a correct operation, but by Theorem 2.4 in Keener, it is correct on the interior of the natural parameter space \\(\\Xi_1\\). We refer the reader to Keener for details.\n\nMean of \\(T(X)\\)\nPartially differentiating Equation 3 once with respect to a generic coordinate \\(\\eta_j\\), for \\(j =1, \\ldots, s\\), we obtain\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\eta_j} e^{A(\\eta)} &= \\int_\\cX \\frac{\\partial}{\\partial \\eta_j} e^{\\eta'T(x)}h(x)\\td\\mu(x)\\\\[7pt]\ne^{A(\\eta)} \\frac{\\partial A}{\\partial \\eta_j}(\\eta) &= \\int_\\cX T_j(x) e^{\\eta'T(x)}h(x)\\td\\mu(x)\\\\[7pt]\n\\frac{\\partial A}{\\partial \\eta_j}(\\eta) &= \\int_\\cX T_j(x) e^{\\eta'T(x) - A(\\eta)}h(x)\\td\\mu(x)\\\\[7pt]\n&= \\EE_\\eta \\left[\\,T_j(X)\\,\\right]\\\\\n\\end{aligned}\n\\]\nArranging these partial derivatives into a vector, we obtain\n\\[\n\\nabla A(\\eta) = \\EE_\\eta\\left[\\,T(X)\\,\\right],\n\\]\nwhich gives us a very convenient method for evaluating the expectation of the sufficient statistic.\n\n\nVariance of \\(T(X)\\)\nPushing our luck further, we can take a second partial derivative:\n\\[\n\\begin{aligned}\n\\frac{\\partial^2}{\\partial \\eta_j\\partial \\eta_k} e^{A(\\eta)} &= \\int_\\cX \\frac{\\partial^2}{\\partial \\eta_j\\partial \\eta_k} e^{\\eta'T(x)}h(x)\\td\\mu(x)\\\\[7pt]\ne^{A(\\eta)}\\left(\\frac{\\partial^2 A}{\\partial \\eta_j\\partial \\eta_k} + \\frac{\\partial A}{\\partial \\eta_j} \\frac{\\partial A}{\\partial \\eta_k} \\right)&= \\int_\\cX T_j(x) T_k(x)e^{\\eta'T(x)}h(x)\\td\\mu(x)\\\\[7pt]\n\\frac{\\partial^2 A}{\\partial \\eta_j\\partial \\eta_k} + \\EE_\\eta[T_j(X)]\\EE_\\eta[T_k(X)] &= \\EE_\\eta \\left[\\,T_j(X) T_k(X)\\,\\right]\\\\\n\\frac{\\partial^2 A}{\\partial \\eta_j\\partial \\eta_k} &= \\text{Cov}_\\eta\\left(T_j(X), T_k(X)\\right).\n\\end{aligned}\n\\]\nAgain, collecting these second partials into a Hessian matrix gives us\n\\[\n\\nabla^2 A(\\eta) = \\text{Var}_\\eta(T(X)),\n\\]\nwhere the right-hand side denotes the \\(s\\times s\\) variance-covariance matrix of the random vector \\(T(X)\\).\nExample (Poisson, continued): As we showed above, in the Poisson exponential family the sufficient statistic is \\(T(X)=X\\), the natural parameter is \\(\\eta = \\log\\lambda\\), and the log-partition function is \\(A(\\eta) = e^\\eta \\;(=\\lambda)\\).\nNote: This calculation would not have worked correctly if we had instead said \\(A(\\eta) = \\lambda\\), and differentiated that expression with respect to \\(\\lambda\\). We would then get \\(\\EE_\\eta[X] = 1\\) and \\(\\text{Var}_\\eta(X) = 0\\), which are clearly incorrect.\n\n\nMoment-generating function and cumulant-generating function\nThe moment generating function (MGF) of a \\(d\\)-dimensional random vector \\(X\\sim P\\) is defined as \\(M^X(u) = \\EE[e^{u'X}]\\), for \\(u\\in \\RR^d\\). If the MGF is well-defined in a neighborhood of \\(u=0\\), then we can use it to calculated moments of \\(X\\) by evaluating its derivatives at 0.\nWe can show this using manipulations very similar to the ones we saw above. To evaluate the first moment of \\(X_j\\), we can differentiate once with respect to \\(u_j\\), since\n\\[\n\\frac{\\partial}{\\partial u_j} M^X(u) = \\int_\\cX \\frac{\\partial}{\\partial u_j} e^{u'x}\\td P(x) = \\int_\\cX x_j e^{u'x}\\td P(x) = \\int_\\cX x_j e^{u'x} \\td P(x).\n\\]\nHere we have again assumed that we can differentiate under the integral sign; this is a technical condition that we would check if we were being more careful.\nEvaluating the derivative at \\(u=0\\), we obtain \\(\\frac{\\partial}{\\partial u_j} M^X(0) = \\int_\\cX x_j \\td P(x) = \\EE[X_j]\\). Moreover, we can repeat this trick as many times as we want, leading to a formula for mixed partial derivatives of any order:\n\\[\n\\left.\\frac{\\partial^{m_1 + \\cdots + m_d}}{\\partial u_1^{m_1}\\cdots\\partial u_d^{m_d}} M^X(u)\\right|_{u=0} = \\left.\\int_{\\cX} x_1^{m_1}\\cdots x_d^{m_d} e^{u'x}\\td P(x)\\right|_{u=0} = \\EE\\left[\\,X_1^{m_1} \\cdots X_d^{m_d}\\,\\right].\n\\tag{4}\\]\nThe MGF is therefore very useful for evaluating moments of \\(X\\). It is also useful for finding distributions of sums of independent random variables, because \\(M^{X + Y}(u) = M^X(u)M^Y(u),\\) if \\(X\\) and \\(Y\\) are independent. If two random variables have the same MGF then they have the same distribution.\nIn an exponential family, the MGF of \\(T(X)\\), under sampling from \\(P_\\eta\\), is simple to evaluate:\n\\[\n\\begin{aligned}\nM^{T(X)}_\\eta(u) &= \\EE_\\eta\\left[\\,e^{u'T(X)}\\,\\right]\\\\[5pt]\n&= \\int_\\cX e^{u'T(x)}e^{\\eta'T(x) - A(\\eta)}h(x)\\td\\mu(x) \\\\[5pt]\n&= e^{-A(\\eta)}\\int_\\cX e^{(u+\\eta)'T(x)} h(x)\\td\\mu(x)\\\\[5pt]\n&= e^{A(\\eta+u)-A(\\eta)}\n\\end{aligned}\n\\]\nExample (Poisson, continued): The MGF for \\(X \\sim \\text{Pois}(\\lambda)\\), with \\(\\eta = \\log\\lambda\\), is\n\\[\nM^{X}_\\eta(u) = \\exp\\{e^{\\eta + u} - e^{\\eta}\\} = \\exp\\{\\lambda (e^u - 1)\\}\n\\]\nTo see how the MGF is useful, suppose we have \\(X_i \\sim \\text{Pois}(\\lambda_i)\\), independently for \\(i = 1,2,\\ldots,n\\), and we want to know the distribution of \\(X_+ = \\sum_i X_i\\). Then we can multiply the MGF’s for \\(X_1,\\ldots,X_n\\) together to obtain the MGF for \\(X_+\\):\n\\[\nM^{X_+}(u) = \\prod_i M_{\\eta_i}^{X_i}(u) = \\exp\\left\\{\\sum_i \\lambda_i (e^u-1)\\right\\}.\n\\]\nAs a result, we have \\(X_+ \\sim \\text{Pois}(\\lambda_+)\\), for \\(\\lambda_+ = \\sum_j \\lambda_i\\).\nLikewise, the closely related cumulant-generating function (CGF) is defined as the log of the MGF:\n\\[\nK^{T(X)}_\\eta(u) = \\log M^{T(X)}_\\eta(u) = A(\\eta+u)-A(\\eta)\n\\]\nEvaluating the CGF’s derivatives at \\(u=0\\) gives us the distribution’s cumulants instead of its moments (the first two cumulants are the mean and variance). \\(K_\\eta^{T(X)}\\) and \\(A\\) are closely related. In particular, note that\n\\[\n\\left.\\frac{\\partial}{\\partial \\eta_j}K_\\eta^{T}(u)\\right|_{u=0} = \\frac{\\partial}{\\partial \\eta_j} A(\\eta),\n\\]\nThis relationship explains why we can also get cumulants for \\(T(X)\\) under \\(P_\\eta\\) by differentiating \\(A(\\eta)\\). It also partly explains why \\(A(\\eta)\\) is sometimes referred to as the CGF even though it is generally not the CGF for \\(T(X)\\)."
  },
  {
    "objectID": "reader/exponential-families.html#other-parameterizations",
    "href": "reader/exponential-families.html#other-parameterizations",
    "title": "Exponential Families",
    "section": "Other parameterizations",
    "text": "Other parameterizations\nSometimes instead of parameterizing \\(\\cP\\) by the natural parameter \\(\\eta\\), it is more convenient to parameterize the family by another parameter \\(\\theta\\). Then we can write the density in terms of this alternative parameterization as\n\\[\np_\\theta(x) = e^{\\eta(\\theta)'T(x) - B(\\theta)}h(x), \\quad \\text{ where } B(\\theta) = A(\\eta(\\theta)).\n\\]\nThe Poisson distribution, if indexed by the mean \\(\\lambda\\), is an example of such an alternative parameterization, with \\(\\eta(\\lambda) = \\log\\lambda\\) and \\(B(\\lambda) = \\lambda\\). Another example is the normal family:\nExample (Normal): Consider the model \\(X\\sim \\cN(\\mu, \\sigma^2)\\), for \\(\\mu\\in\\RR\\) and \\(\\sigma^2&gt;0\\). The usual parameter vector for this problem is \\(\\theta = (\\mu, \\sigma^2)\\). The density in that parameterization is\n\\[\np_\\theta(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-(\\mu-x)^2/2\\sigma^2\\right\\}.\n\\]\nExpanding the square and raising the \\(\\sqrt{2\\pi\\sigma^2}\\) into the exponent, we can massage the density into the exponential family structure we are looking for:\n\\[\np_\\theta(x)\n= \\exp\\left\\{\\frac{\\mu}{\\sigma^2} x - \\frac{1}{2\\sigma^2}x^2 - \\frac{\\mu}{2\\sigma^2} - \\frac{1}{2}\\log\\left(2\\pi\\sigma^2\\right)\\right\\},\n\\]\nwhich we can recognize as an exponential family with sufficient statistic \\(T(x) = (x,x^2)\\), natural parameter \\(\\eta(\\theta) = (\\mu/\\sigma^2,-1/2\\sigma^2)\\), carrier density \\(h(x)=1\\), and log-partition function\n\\[\nB(\\theta) = \\frac{\\mu^2}{2\\sigma^2} + \\frac{1}{2}\\log\\left(2\\pi\\sigma^2\\right).\n\\]\nWe can rewrite the log-partition function in terms of \\(\\eta_1 = \\mu^2/2\\sigma^2\\) and \\(\\eta_2=-1/2\\sigma^2\\) to complete the natural parameterization:\n\\[\np_\\eta(x) = e^{\\eta'T(x) - A(\\eta)}, \\quad \\text{ for } A(\\eta) = \\frac{-\\eta_1^2}{4\\eta_2} + \\frac{1}{2}\\log(-\\pi/\\eta_2)\n\\]\nHence, the Gaussian is the (only) exponential family with sufficient statistic \\(T(x) = (x,x^2)\\), and carrier density \\(h(x)=1\\), with respect to the Lebesgue measure on \\(\\RR\\).\nExample (Binomial): Next, consider the model \\(X \\sim \\text{Binom}(n,\\theta)\\), which has pmf\n\\[\np_\\theta(x) = \\theta^x (1-\\theta)^{n-x}\\binom{n}{x}.\n\\]\nWe can again massage this into canonical form by raising the parameters into the exponent and collecting terms\n\\[\n\\begin{aligned}\np_\\theta(x)\n&= \\exp\\left\\{x\\log\\theta + (n-x)\\log(1-\\theta)\\right\\}\\binom{n}{x}\\\\\n&= \\exp\\left\\{x\\log\\left(\\frac{\\theta}{1-\\theta}\\right)-n\\log(1-\\theta)\\right\\}\\binom{n}{x},\n\\end{aligned}\n\\]\nwhich we recognize as an exponential family structure with \\(T(x)=x\\), natural parameter \\(\\eta=\\log\\left(\\frac{\\theta}{1-\\theta}\\right)\\) The natural parameter \\(\\eta\\) is called the log-odds or logit, which is used in classification models such as logistic regression and the many extensions thereof.\nExample (Beta): The Beta distribution is a common family of distributions on the unit interval. If \\(X \\sim \\text{Beta}(\\alpha,\\beta)\\) then \\(X\\) has pdf\n\\[\n\\begin{aligned}\np_{\\alpha,\\beta}(x)\n&= \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\\\[5pt]\n&= \\exp\\{\\alpha \\log x + \\beta\\log (1-x) - \\log B(\\alpha,\\beta)\\}\\cdot \\frac{1}{x(1-x)},\n\\end{aligned}\n\\]\nwhere \\(B(\\alpha,\\beta) = \\int_0^1 t^{\\alpha-1}(1-t)^{\\beta-1}\\td t\\) is called the beta function. We recognize this as an exponential family with \\(T(x) = (\\log x, \\log(1-x))\\), \\(\\eta = (\\alpha,\\beta)\\), and \\(h(x) = \\frac{1}{x(1-x)}\\), though as always there are other ways to decompose the density.\nIn addition to these examples, we could add most of the distributional families detailed on Wikipedia: the Gamma, multinomial, Dirichlet, Pareto, Wishart, and many others. We will see more exponential family examples throughout the course."
  },
  {
    "objectID": "reader/exponential-families.html#exponential-tilting",
    "href": "reader/exponential-families.html#exponential-tilting",
    "title": "Exponential Families",
    "section": "Exponential tilting",
    "text": "Exponential tilting\nTo help interpret what it means for a model to have an exponential family structure, we can think of \\(p_\\eta(x) = e^{\\eta'T(x) - A(\\eta)} h(x)\\) as an exponential tilt of the carrier density \\(h(x)\\). That is, beginning with \\(h(x)\\), we first multiply by \\(e^{\\eta'T(x)}\\), increasing the density of points in the sample space for which \\(\\eta'T(x)\\) is largest relative to those for which \\(\\eta'T(x)\\) is smaller. Then, we re-normalize by \\(e^{-A(\\eta)}\\) to obtain a probability distribution.\nThis is easiest to understand in a one-parameter family with sufficient statistic \\(T(X) = X\\)\n\nPlot = require(\"@observablehq/plot\")\nd3 = require(\"d3\")\n\nkatex = require(\"https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js\")\n\n// Function to render LaTeX\nfunction tex(string) {\n  let span = document.createElement('span');\n  katex.render(string, span, {output: \"mathml\", throwOnError: false});\n  return span;\n}\n\n// Create sliders for mean and standard deviation\nviewof eta1 = Inputs.range([-5, 5], {step: 0.1, label: tex(\"\\\\eta_1\"), value: 0})\nviewof eta2 = Inputs.range([-5, 5], {step: 0.1, label: tex(\"\\\\eta_2\"), value: 0})\nviewof eta3 = Inputs.range([-0.3, 0.3], {step: 0.01, label: tex(\"\\\\eta_3\"), value: 0})\n\n// Function to generate normal distribution data\nfunction explDistribution(eta1, eta2, eta3, n = 2000) {\n  const x = d3.range(-1, 1, 2/n);\n  const unnorm = x.map(x =&gt; ({\n    x: x,\n    y: Math.exp(eta1 * x + eta2 * x * x + eta3 * Math.cos(30 * x * Math.PI)) * (1 - Math.abs(x))\n  }));\n  const normConst = d3.sum(unnorm, point =&gt; point.y) * 2 / n;\n  return unnorm.map(point =&gt; ({\n    x: point.x,\n    y: point.y / normConst\n  }));\n}\n\n// Create the plot\nPlot.plot({\n  width: 640,\n  height: 400,\n  x: {label: \"X\"},\n  y: {label: \"Density\"},\n  marks: [\n    Plot.line(explDistribution(eta1, eta2, eta3), {x: \"x\", y: \"y\", stroke: \"steelblue\"}),\n    Plot.ruleY([0])\n  ]\n})"
  },
  {
    "objectID": "reader/exponential-families.html#repeated-sampling-from-exponential-families",
    "href": "reader/exponential-families.html#repeated-sampling-from-exponential-families",
    "title": "Exponential Families",
    "section": "Repeated sampling from exponential families",
    "text": "Repeated sampling from exponential families\nOne of the most important properties of exponential families is that a large sample can be summarized by a low-dimensional statistic. Suppose we observe a vector of observations \\(X = (X_1,\\ldots,X_n)\\) representing an independent and identically distributed (i.i.d.) sample from an exponential family. Let \\(p_\\eta^{(1)\\) denote the density for a single observation:\n\\[\nX_1\\ldots,X_n \\simiid p_\\eta^{(1)}(x) = e^{\\eta'T(x) - A(\\eta)}h(x).\n\\]\nThen the random vector \\(X = (X_1,\\ldots,X_n)\\) follows another closely related exponential family:\n\\[\n\\begin{aligned}\np_\\eta(x)\n&= \\prod_{i=1}^n e^{\\eta'T(x_i) - A(\\eta)}h(x_i)\\\\[7pt]\n&= \\exp\\left\\{\\eta'\\sum_{i=1}^n T(x_i) - nA(\\eta)\\right\\} \\prod_{i=1}^n h(x_i).\n\\end{aligned}\n\\]\nThis new density \\(p_\\eta\\), which governs the distribution of the entire sample, is an exponential family with the same natural parameter as before, sufficient statistic \\(\\sum_i T(X_i)\\), carrier density \\(\\prod_i h(x_i)\\), and log-partition function \\(nA(\\eta)\\).\nFor reasons that will become clearer in the next lecture, it is very significant that the sufficient statistic does not increase in dimension as the sample size grows. This means that the \\(s\\)-dimensional vector \\(\\sum_i T(X_i)\\) is for all intents and purposes a complete summary of the entire sample, no matter how large \\(n\\) is."
  },
  {
    "objectID": "reader/unbiased-estimation.html",
    "href": "reader/unbiased-estimation.html",
    "title": "Unbiased Estimation",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/unbiased-estimation.html#outline",
    "href": "reader/unbiased-estimation.html#outline",
    "title": "Unbiased Estimation",
    "section": "1 Outline",
    "text": "1 Outline\n\nConvex Loss\nRao-Blackwell Theorem\nUMVU Estimators\nExamples"
  },
  {
    "objectID": "reader/unbiased-estimation.html#unbiased-estimation",
    "href": "reader/unbiased-estimation.html#unbiased-estimation",
    "title": "Unbiased Estimation",
    "section": "2 Unbiased Estimation",
    "text": "2 Unbiased Estimation\nRecall from Lecture 2 that we had two primary strategies to choose an estimator:\n\nSummarize the risk function by a scalar (average or supremum)\nRestrict attention to a smaller class of estimators\n\nToday we’ll discuss unbiased estimation, which is an example of the second strategy. That is, if \\(g(\\theta)\\) is our estimand, we will require that \\(\\EE_\\theta \\delta = g(\\theta)\\) for all \\(\\theta\\)\nUnbiased estimation is especially convenient in models with a complete sufficient statistic \\(T(X)\\). In that case:\n\nThere is at most one unbiased \\(\\delta(T(X))\\) (if \\(\\delta_1, \\delta_2(T)\\) are both unbiased, then \\(\\delta_1 \\eqas \\delta_2\\))\nIf an unbiased estimator exists, it uniformly minimizes risk for any convex loss function"
  },
  {
    "objectID": "reader/unbiased-estimation.html#convex-loss-functions",
    "href": "reader/unbiased-estimation.html#convex-loss-functions",
    "title": "Unbiased Estimation",
    "section": "3 Convex Loss Functions",
    "text": "3 Convex Loss Functions\nRecall \\(f(y)\\) is convex if for all \\(x_1, x_2\\) and all \\(\\gamma \\in [0,1]\\):\n\\[f(\\gamma x_1 + (1-\\gamma)x_2) \\leq \\gamma f(x_1) + (1-\\gamma) f(x_2)\\] \\(f\\) is strictly convex if the inequality is strict unless \\(x_1 = x_2\\).\nAn key fact about convex functions is Jensen’s Inequality: If \\(f\\) is convex, then for any random variable \\(X\\), we have\n\\[f(\\EE[X]) \\leq \\EE[f(X)]\\] If \\(f\\) is strictly convex, then the inequality is strict unless \\(X\\) is constant.\nWe say a loss function \\(L(\\theta, d)\\) is (strictly) convex if is (strictly) convex as a function of the estimate \\(d\\), its second argument, holding the parameter \\(\\theta\\) fixed.\nExample: The best-known example of a convex loss function is the squared error loss. Recall that the corresponding risk, the MSE, can be decomposed as the sum of the bias squared and the variance:\n\\[\n\\begin{aligned}\n\\text{MSE}_\\theta(\\delta) &= \\EE_\\theta[(\\delta(X) - g(\\theta))^2] \\\\[5pt]\n&= \\text{Bias}_\\theta(\\delta)^2 + \\text{Var}_\\theta(\\delta(X))\n\\end{aligned}\n\\] If \\(\\delta(X)\\) is unbiased, then its MSE is exactly its variance, so minimizing the risk among unbiased estimators just amounts to finding one with the least variance."
  },
  {
    "objectID": "reader/unbiased-estimation.html#the-rao-blackwell-theorem",
    "href": "reader/unbiased-estimation.html#the-rao-blackwell-theorem",
    "title": "Unbiased Estimation",
    "section": "4 The Rao-Blackwell Theorem",
    "text": "4 The Rao-Blackwell Theorem\nIntuitively, convex losses punish us for using noisy estimators: we would always improve the risk if we could replace an estimator \\(\\delta(X)\\) with its expectation: \\[L(\\theta, \\EE_\\theta [\\delta(X)]) \\leq \\EE_\\theta \\left[L(\\theta, \\delta(X))\\right].\\]Generally, this is not feasible in real problems because \\(\\EE_\\theta [\\delta(X)]\\) depends on \\(\\theta\\), so it is not an estimator. But for any sufficient statistic \\(T(X)\\), the conditional expectation of \\(\\delta(X)\\) given \\(T(X)\\) really is an estimator, and it is always at least as good as \\(\\delta(X\\)) if the loss is convex.\nThe next result formalizes this fact, and thereby gives decision-theoretic teeth to the sufficiency principle:\nTheorem (Rao-Blackwell): Let \\(T(X)\\) be sufficient for \\(\\cP = \\{P_\\theta:\\;\\theta\\in\\Theta\\}\\), and let \\(\\delta(X)\\) be any estimator for \\(g(\\theta)\\). Define the new estimator:\n\\[\\bar{\\delta}(T(X)) = \\EE[\\delta(X) \\mid T(X)]\\]\nThen for any convex loss \\(L(\\theta, d)\\), we have \\(R(\\theta, \\bar{\\delta}) \\leq R(\\theta, \\delta)\\) for all \\(\\theta\\). If \\(L\\) is strictly convex, \\(\\bar{\\delta}\\) strictly dominates \\(\\delta\\) as an estimator unless \\(\\delta(X) \\eqPas \\bar{\\delta}(T(X))\\).\nProof:\n\\[\\begin{aligned}\nR(\\theta, \\bar{\\delta}) &= \\EE_\\theta\\left[\\,L(\\theta, \\;\\EE[\\delta \\mid T])\\,\\right]\\\\[5pt]\n&\\leq \\EE_\\theta\\left[\\,\\EE[L(\\theta, \\delta) \\mid T]\\,\\right]\\\\[5pt]\n&= \\EE_\\theta[\\,L(\\theta, \\delta)\\,] \\\\[5pt]\n&= R(\\theta, \\bar{\\delta})\n\\end{aligned}\\]\n\\(\\bar{\\delta}\\) is called the Rao-Blackwellization of \\(\\delta\\). Note that the condition $\\delta(X) \\eqPas \\bar{\\delta}(T(X))$ is equivalent to the condition that \\(\\delta\\) depends only on \\(X\\) through \\(T(X)\\).\nWhenever we are dealing with a convex loss, the Rao-Blackwell theorem lets us restrict our attention only to estimators that run through \\(T(X)\\), because any other estimator could be improved (or at least not worsened) by Rao-Blackwellization. The theorem even gives us a recipe for **constructing** the improved estimator."
  },
  {
    "objectID": "reader/unbiased-estimation.html#umvu-estimators",
    "href": "reader/unbiased-estimation.html#umvu-estimators",
    "title": "Unbiased Estimation",
    "section": "5 UMVU estimators",
    "text": "5 UMVU estimators\nIn this section we will combine two key facts from this lecture and last concerning unbiased estimation of any estimand \\(g(\\theta)\\).\n\nIf \\(T(X)\\) is complete sufficient, there can be at most one unbiased estimator based on \\(T(X)\\).\nIf the loss is convex, then we can restrict our attention only to estimators that are based on \\(T(X)\\).\n\nTogether these facts imply that, if any unbiased estimator exists at all, then there is a unique best unbiased estimator.\nNot all estimands have unbiased estimators. We say \\(g(\\theta)\\) is U-estimable if there exists any \\(\\delta(X)\\) with \\(\\EE_\\theta \\delta(X) = g(\\theta)\\) for all \\(\\theta\\). This leads to the following theorem:\nTheorem: For model \\(\\mathcal{P} = \\{P_\\theta : \\theta \\in \\Theta\\}\\), assume \\(T(X)\\) is a complete sufficient statistic. Then\n\nFor any U-estimable \\(g(\\theta)\\) there exists a unique unbiased estimator of the form \\(\\delta(T(X))\\).\nFor a (strictly) convex loss, that estimator (strictly) dominates any other unbiased estimator \\(\\tilde{\\delta}(X)\\) unless \\(\\tilde{\\delta}(X) \\eqas \\delta(T(X))\\).\n\nAs usual the “uniqueness” here is only up to \\(\\eqPas\\).\nProof:\n(1) Since \\(g(\\theta)\\) is U-estimable, there exists some unbiased estimator \\(\\delta_0(X)\\). Then its Rao-Blackwellization \\(\\delta(T(X)) = \\EE[\\delta_0 \\mid T]\\) is also unbiased, since\n\\[\n\\EE_\\theta \\delta(T) = \\EE_\\theta[\\EE[\\delta_0 | T]] = \\EE_\\theta \\delta_0 = g(\\theta).\n\\]\nAny other estimator of the form \\(\\tilde\\delta(T)\\) must be almost surely equal to \\(\\delta(T)\\), by completeness: if \\(f(t) = \\delta(t)-\\tilde\\delta(t)\\), then both estimators being unbiased means \\(\\EE_\\theta f(T) = g(\\theta)-g(\\theta) = 0\\), so \\(f(T(X)) \\eqas 0\\). Thus, \\(\\delta(T)\\) is unique.\n(2) The first result implies that every unbiased estimator has the same Rao-Blackwellization, namely \\(\\delta(T)\\). Thus, by the Rao-Blackwell theorem, \\(\\delta(T)\\) (strictly) dominates every other unbiased estimator for any (strictly) convex loss function, unless the estimator is almost surely identical to \\(\\delta\\). \\(\\blacksquare\\)\nThe estimator from this theorem is usually called the UMVU (Uniformly Minimum Variance Unbiased) Estimator. We say \\(\\delta(X)\\) is UMVU if:\n\n\\(\\delta(X)\\) is unbiased\n\\(\\text{Var}_\\theta \\,\\delta(X) \\leq \\text{Var}_\\theta \\,\\tilde{\\delta}(X)\\) for all \\(\\theta\\) and all unbiased \\(\\tilde{\\delta}(X)\\)\n\nSince \\(\\text{MSE}(\\theta; \\delta) = \\text{Var}_\\theta(\\delta(X))\\) for any unbiased estimator, and the squared error loss is strictly convex, Theorem XXX immediately implies the existence of a unique UMVU estimator for any U-estimable \\(g(\\theta)\\), whenever we have a complete sufficient statistic.\nNote that in problems where no complete sufficient statistic exists, there can be multiple unbiased estimators based on the minimal sufficient statistic; for example, both the mean and the median are unbiased for the Laplace location parameter, but they are not almost surely equal to each other, and they do not have the same risk function."
  },
  {
    "objectID": "reader/unbiased-estimation.html#finding-the-umvue",
    "href": "reader/unbiased-estimation.html#finding-the-umvue",
    "title": "Unbiased Estimation",
    "section": "6 Finding the UMVUE",
    "text": "6 Finding the UMVUE\nTheorem XXX suggests two strategies for finding the UMVUE:\n\nSolve directly for an unbiased estimator based on \\(T\\)\nFind any unbiased estimator at all, then Rao-Blackwellize it\n\nWe give examples of both strategies below:\nExample (Poisson): Let \\(X_1, \\ldots, X_n \\sim \\text{Pois}(\\theta)\\), \\(g(\\theta) = e^{-\\theta}\\) and consider unbiased estimation for \\(g(\\theta) = \\theta^2\\).\nThe complete sufficient statistic for the model is\n\\[T(X) = \\sum X_i \\sim \\text{Pois}(n\\theta),\\] and its probability mass function for \\(t \\geq 0\\) is \\[\np_\\theta(t) = \\frac{e^{-n\\theta} (n\\theta)^t}{t!}\n\\] Strategy 1\nIf there is some unbiased estimator \\(\\delta(t)\\), we can try to solve for it by setting its expectation equal to \\(\\theta^2\\): \\[\n\\theta^2 = \\EE_\\theta \\delta(T) = \\sum_{t=0}^\\infty \\delta(t) \\frac{e^{-n\\theta} (n\\theta)^t}{t!}.\n\\] Rearranging factors, we obtain matching power series: \\[\n\\sum_{t=0}^\\infty \\delta(t) \\frac{n^t \\theta^t}{t!} = e^{n\\theta}\\theta^2 =  \\sum_{k=0}^\\infty \\frac{n^k\\theta^{k+2}}{k!}.\n\\] We will choose the coefficients on the left-hand side to match terms. First, change the index for the left-hand sum to \\(t = k+2\\): \\[\n\\sum_{t=0}^\\infty \\delta(t) \\frac{n^t \\theta^t}{t!} = e^{n\\theta}\\theta^2 =  \\sum_{t=2}^\\infty \\frac{n^{t-2}\\theta^{t}}{(t-2)!}.\n\\] To match the terms, we can set \\(\\delta(0)=\\delta(1)=0\\), and for \\(t\\geq 2\\), set \\(\\delta(t)=\\frac{t!}{n^2(t-2)!}=\\frac{t(t-1)}{n^2}\\). The same expression works for both, so we obtain the estimator \\[\n\\delta(T) = \\frac{T(T-1)}{n^2}\n\\]\nStrategy 2:\nAlternatively, we can find an unbiased estimator and Rao-Blackwellize it. If $n$, we can use the fact that\n\\[\n\\EE_\\theta [X_1 X_2] = \\EE_\\theta [X_1] \\;\\cdot\\; \\EE_\\theta [X_2] = \\theta^2\n\\] to obtain an initial unbiased estimator \\(\\delta_0(X) = X_1X_2\\), which we will Rao-Blackwellize.\nConditional on $\nto match the terms\n\\(\\delta(T) = (1 - 1/n)^T\\) unbiased:\n\\[\\begin{aligned}\n\\EE_\\theta \\delta(T) &= \\sum_{t=0}^\\infty (1-1/n)^t e^{-n\\theta} (n\\theta)^t / t! \\\\\n&= e^{-n\\theta} \\sum_{t=0}^\\infty ((n-1)\\theta)^t / t! \\\\\n&= e^{-n\\theta} e^{(n-1)\\theta} = e^{-\\theta}\n\\end{aligned}\\]\nAlternatively, we could Rao-Blackwellize \\(\\delta_0(X) = I(X_1 = 0)\\):\n\\[\\begin{aligned}\n\\EE[I(X_1 = 0) | T] &= \\PP(X_1 = 0 | T) \\\\\n&= \\frac{\\PP(X_1 = 0, X_2 + \\cdots + X_n = T)}{\\PP(X_2 + \\cdots + X_n = T-1) + \\PP(X_2 + \\cdots + X_n = T)} \\\\\n&= \\frac{\\binom{n-1}{T} (1/n)^0 (1-1/n)^T}{\\binom{n-1}{T-1} (1/n) (1-1/n)^{T-1} + \\binom{n-1}{T} (1-1/n)^T} \\\\\n&= \\frac{(1-1/n)^T}{T/n + (1-1/n)^T} \\\\\n&= (1-1/n)^T\n\\end{aligned}\\]\nExample: \\(X_1, \\ldots, X_n \\sim U[0, \\theta]\\), \\(\\theta &gt; 0\\)\n\\(T = X_{(n)}\\) complete sufficient\n\\(p_\\theta(t) = n t^{n-1} / \\theta^n \\cdot I(0 &lt; t &lt; \\theta)\\)\n\\(\\EE_\\theta[T] = \\frac{n}{n+1} \\theta\\)\n\\(T \\cdot \\frac{n+1}{n}\\) is UMVUE\nAlternatively, \\(2X_1\\) is unbiased:\n\\[\\EE[2X_1 | T] = 2T \\cdot \\frac{n+1}{2n} = T \\cdot \\frac{n+1}{n}\\]\nActually, \\(T\\) is inadmissible too! Keener shows \\(\\frac{n-1}{n} T\\) has better MSE for any estimator \\(c \\cdot T\\).\nThis raises the question: why do we require zero bias?\nThe UMVUE is often inefficient, inadmissible, or just dumb in cases where another approach makes much more sense.\nExample: \\(X \\sim \\text{Bin}(1000, \\theta)\\)\nEstimate \\(g(\\theta) = I(\\theta &gt; 0.5)\\)\nUMVUE is \\(I(X &gt; 500)\\). Why?\n\n\\(X = 500\\): Conclude \\(g(\\theta) = 1\\)\n\\(X = 499\\): Conclude \\(g(\\theta) = 0\\)\n\nThis is not epistemically reasonable. Could do much better with e.g. MLE or a Bayes estimator.\nIn fact, our theorem should make us suspicious of UMVUEs: every idiotic function of \\(T\\) is a UMVUE of its own expectation!\nExample: \\(X_1, \\ldots, X_n \\sim N(\\mu, 1)\\), estimate \\(g(\\mu) = \\|\\mu\\|\\)\n\\(\\bar{X}\\) is complete sufficient\n\\(\\|\\bar{X}\\|\\) is unbiased: \\(\\EE[\\|\\bar{X}\\|] = \\EE[\\|N(\\mu, 1/n)\\|] = \\|\\mu\\|\\)\nSo \\(\\|\\bar{X}\\|\\) is UMVUE\nIf \\(\\mu = 0\\), \\(\\delta(\\bar{X}) = 0\\) about half the time\n\\(\\|\\bar{X}\\| + d \\cdot \\max(0, \\|\\bar{X}\\| - d)\\) strictly dominates UMVUE"
  },
  {
    "objectID": "reader/models.html",
    "href": "reader/models.html",
    "title": "Statistical decision theory",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/models.html#statistical-models",
    "href": "reader/models.html#statistical-models",
    "title": "Statistical decision theory",
    "section": "Statistical models",
    "text": "Statistical models\nUntil now, we have been discussing the topic of probability. Roughly speaking, in probability we fully specify the distribution of some random variables, and then ask what we can say about the distribution. For example, given a complete description of the rules for generating a random walk, we might ask how long, in expectation, it will take to reach a certain threshold. This is an essentially deductive exercise: while the mathematics might be very hard, the questions we ask generally have unambiguous answers.\nIn statistics, we do essentially the opposite: beginning with the data — the Latin word for “given” — we work backwards to draw inferences about the data-generating distribution. This is an inductive exercise, for which the answers will inevitably be more ambiguous.\nWe will generally use the letter \\(X\\) to denote the full data set, which we assume is drawn randomly from some unknown distribution \\(P\\) over the sample space \\(\\cX\\). Let \\(\\cP\\) denote a family of candidate probability distributions, called the statistical model. We assume the analyst knows that one of the elements of \\(\\cP\\) is the true data-generating distribution \\(P\\), but does not know which one. The set \\(\\cX\\) in which \\(X\\) is\nExample (Binomial): As a simple example, we can imagine an analyst who flips a biased coin \\(n\\) times, getting \\(X\\) heads and \\(n-X\\) tails. If we assume the successive flips are independent, and each has a common probability \\(\\theta\\) of landing heads, we can write the model as\n\\[\nX \\sim \\text{Binom}(n, \\theta), \\quad \\text{ for some } \\theta \\in [0,1].\n\\]\nFormally, we could say the family of distributions is \\(\\cP = \\{\\text{Binom}(n, \\theta):\\; \\theta \\in [0,1]\\}\\), a set of distributions indexed by the real parameter \\(\\theta\\).\nNote that in the previous example, the integer \\(n\\) is another important variable in the problem, but we implicitly assumed that it was “known” by the analyst, meaning that it is the same for all \\(P \\in \\cP\\). The parameter \\(\\theta\\), by contrast, is termed “unknown” in the sense that it varies over the family \\(\\cP\\).\n\nParametric vs nonparametric models\nMany of the models we will consider in this class are parametric, typically meaning that they are indexed by finitely many real parameters. That is, we have \\(\\cP = \\{P_\\theta:\\; \\theta \\in \\Theta\\}\\), typically for some parameter space \\(\\Theta \\subseteq \\RR^d\\). Then \\(\\theta\\) is called the parameter or parameter vector.\nIn other models, there is no natural way to index \\(\\cP\\) using \\(d\\) real numbers. We call these nonparametric models. Sometimes excited authors referred to their methods as “assumption-free,” but essentially all nonparametric models still make some assumptions about the data distribution. For example, we might assume independence between multiple observations, or shape constraints such as unimodality.\nExample (Nonparameric model): Suppose we observe an i.i.d. sample of size \\(n\\) from a distribution \\(P\\) on the real line. Even if we do not want to assume anything about \\(P\\), the i.i.d. assumption will play an important role in the analysis. We might write this model as\n\\[\nX_1,\\ldots,X_n \\simiid P, \\quad \\text{ for some distribution } P \\text{ on } \\RR.\n\\]\nFormally, if \\(X = (X_1,\\ldots,X_n)\\), we can write the family as \\(\\cP = \\{P^n:\\; P \\text{ is a distribution on } \\RR\\}\\), where \\(P^n\\) represents the \\(n\\)-fold product of \\(P\\) on \\(\\RR^n\\).\nNotation: Much of what we will learn in this course applies to parametric and nonparametric models alike, and indeed there is no crisp demarcation between parametric and nonparametric models in practice. It will often be convenient to use notation \\(\\cP = \\{P_\\theta :\\; \\theta \\in \\Theta\\}\\), without specifying what kind of set \\(\\Theta\\) is; in particular there is nothing to stop \\(\\theta\\) from being an infinite-dimensional object such as a density function. We can work in this notation without any loss of generality, since we could always take \\(\\theta = P\\) and \\(\\Theta = \\cP\\).\n\n\nBayesian vs Frequentist inference\nThus far we have assumed the data \\(X\\) follows a distribution \\(P_\\theta\\), for some unknown parameter \\(\\theta\\) which can be any arbitrary member of the set \\(\\Theta\\). In some contexts we will introduce an additional assumption we can call the Bayesian assumption: that \\(\\theta\\) is itself random, drawn from some known distribution \\(\\Lambda\\) that we call the prior.\nA major advantage of this assumption is that it reduces the problem of inference about \\(\\theta\\) to simply calculating the conditional distribution of \\(\\theta\\) given \\(X\\).\nThe philosophical ramifications of this assumption, as well as its practical advantages and disadvantages, will be a major theme later in the course, but for now we will simply say it is an assumption we are sometimes, but not always, willing to make. From a mathematical perspective, it makes no more or less sense to assume \\(\\theta\\) is random than it does to assume \\(\\theta\\) is fixed and unknown.\nFor the remainder of this lecture, and until our unit on Bayesian inference, we will refrain from making this assumption, instead regarding \\(\\theta\\) as taking an arbitrary fixed value in \\(\\Theta\\)."
  },
  {
    "objectID": "reader/models.html#estimation-in-statistical-models",
    "href": "reader/models.html#estimation-in-statistical-models",
    "title": "Statistical decision theory",
    "section": "Estimation in statistical models",
    "text": "Estimation in statistical models\nHaving observed \\(X \\sim P_\\theta\\), an unknown distribution in the model \\(\\cP = \\{P_\\theta:\\; \\theta \\in \\Theta\\}\\), we will be interested in learning something about \\(\\theta\\). In estimation, we guess the value of some quantity of interest \\(g(\\theta)\\), called the estimand. Our guess is called the estimate \\(\\delta(X)\\), calculated based on the data. The method \\(\\delta(\\cdot)\\) that we use to calculate the estimate is called the estimator.\nExample (Binomial, continued): Returning to our binomial example from above, we may want to estimate \\(g(\\theta) = \\theta\\), the probability of the coin landing heads. A natural estimator is \\(\\delta_0(X) = X/n\\), the fraction of coins landing heads in any given trial. One favorable property of this estimator is that it is unbiased, meaning that \\(\\EE_\\theta \\delta_0(X) = g(\\theta)\\), for all \\(\\theta \\in \\Theta\\).\nThere are many potential estimators for any given problem, so our goal will generally be to find a good estimator. To evaluate and compare estimators, we must have a way of evaluating how successful an estimator is in any given realization of the data. To this end we introduce the loss function \\(L(\\theta, d)\\), which measures how bad it is to guess that \\(g(\\theta) = d\\) when \\(\\theta\\) is the true parameter value. Typically loss functions are non-negative, with \\(L(\\theta, d) = 0\\) if and only if \\(g(\\theta) = d\\) (no loss from a perfect guess) but this is not required.\nIn any given problem, we should ideally choose the loss that best measures our own true (dis)utility function, but in practice people fall back on simple defaults. One loss function that is especially popular for its mathematical convenience is the squared-error loss, defined by \\(L(\\theta, d) = (d-g(\\theta))^2\\).\nWhereas the loss function measures how (un)successful an estimator is in one realization of the data, we would really like to evaluate an estimator’s performance over the whole range of possible data sets \\(X\\) that we might observe. This is measured by the risk function, defined as\n\\[\nR(\\theta; \\delta(\\cdot)) = \\EE_\\theta [\\, L(\\theta, \\delta(X)) \\,] = \\int L(\\theta, \\delta(x)) \\td P_\\theta(x)\n\\]\nRemark on notation: The subscript in the previous expression tells us which of our candidate probability distributions to use in evaluating the expectation. In some other fields, people may use the subscript to indicate “what randomness to integrate over,” with the implication that any random variable that does not appear in the subscript should be held fixed. In our course, it should generally be assumed that any expectation or probability is integrating over the joint distribution of the entire data set; if we want to hold something fixed we will condition on it. Recall that, for now, the parameter \\(\\theta\\) is fixed unless otherwise specified.\nThe semicolon in the risk function is meant to indicate we are viewing it primarily as a function of \\(\\theta\\). That is, we should think of and estimator \\(\\delta\\) as having a risk function \\(R(\\theta)\\), and the second input in \\(R(\\theta; \\delta)\\) is telling us which estimator’s risk function to evaluate at \\(\\theta\\).\nThe risk for the squared-error loss is called the mean squared error (MSE):\n\\[\n\\textrm{MSE}(\\theta; \\delta) = \\EE_\\theta\\left[\\,(\\delta(X) - g(\\theta))^2\\,\\right]\n\\]\nExample (Binomial, continued): To calculate the MSE of our estimator \\(\\delta_0 = X/n\\), note that \\(\\EE_\\theta[X/n] = \\theta\\) (the estimator is unbiased). As a result, we have\n\\[\n\\begin{aligned}\n\\textrm{MSE}(\\theta; \\delta_0) &= \\EE_\\theta\\left[ \\left(\\frac{X}{n} - \\theta\\right)^2\\right] \\\\[7pt]\n&= \\text{Var}_\\theta(X/n)\\\\[3pt]\n&= \\frac{1}{n}\\theta(1-\\theta)\n\\end{aligned}\n\\]\nOne reason why we might consider estimators other than \\(\\delta_0\\) is that, if \\(n\\) is small, our estimate could be quite noisy. As an extreme example, if \\(n=1\\) we will always estimate either \\(\\theta = 0\\) or \\(\\theta = 1\\), both of which would be extreme conclusions to draw after a single trial. One simple way of reducing the variance is to pretend that we flipped the coin an additional \\(m\\) times resulting in \\(a\\) heads and \\(m-a\\) tails. This will tend to shade our estimate toward \\(a/m\\), reducing the risk if \\(\\theta = a/m\\) but possibly increasing the risk for other values of \\(\\theta\\).\n\nWe show the risk function for several alternative estimators of this form below:\n\\[\n\\delta_1(X) = \\frac{X + 1}{n + 2}, \\quad \\delta_2(X) = \\frac{X + 2}{n + 4}, \\quad \\delta_3(X) = \\frac{X + 1}{n}\n\\]\nThe last estimator, \\(\\delta_3\\), is another example where we add something to \\(X\\) in the numerator but nothing \\(n\\) in the denominator.\n\nlibrary(RColorBrewer)\n\nn = 16\n\n## risk function of estimator (X + synth.heads) / (n + synth.flips)\nbinom.mse &lt;- function(theta, n, synth.heads, synth.flips) {\n  binom.var &lt;- theta * (1 - theta) * n / (n + synth.flips)^2\n  binom.bias &lt;- (n * theta + synth.heads) / (n + synth.flips) - theta\n  return(binom.var + binom.bias^2)\n}\n\n\npalette &lt;- c(\"black\",brewer.pal(4, \"Set1\"))\ncurve(binom.mse(x, n, 0, 0), from=0, to=1, ylim=c(0,0.35/n), lwd=2, col=palette[1],\n      main = \"Mean squared error for binomial estimators (n=16)\", \n      ylab=expression(MSE(theta)), \n      xlab=expression(theta))\ncurve(binom.mse(x, n, 1, 2), add=TRUE, col=palette[2], lwd=2) \ncurve(binom.mse(x, n, 2, 4), add=TRUE, col=palette[3], lwd=2) \ncurve(binom.mse(x, n, 1, 0), add=TRUE, col=palette[4], lwd=2) \nlegend(\"topright\", col=palette, lwd=2,bty=\"n\",\n       legend=c(expression(delta[0]), expression(delta[1]), expression(delta[2]), expression(delta[3])))\n\n\n\n\nIf we look at the vertical axis, the MSE may appear to be very small, especially considering we only have 16 flips. But recall that an MSE of \\(0.01\\) means that we are typically missing by about \\(0.1\\), while estimating a parameter that is between \\(0\\) and \\(1\\).\n\nComparing estimators\nIn comparing the risk functions of these estimators, we can notice a few things. As expected, both \\(\\delta_1\\) and \\(\\delta_2\\) outperform \\(\\delta_0\\) for values of \\(\\theta\\) close to \\(1/2\\), but underperform for more extreme values of \\(\\theta\\). The estimator \\(\\delta_3\\), however, performs worse than \\(\\delta_0\\) throughout the entire parameter space; this is because we have added bias without doing anything to reduce the variance. While it is difficult to choose between the other three estimators, we can at least rule out \\(\\delta_3\\) on the grounds that we have no reason to ever prefer it over \\(\\delta_0\\).\nFormally, we say an estimator \\(\\delta\\) is inadmissible if there is some other estimator \\(\\delta^*\\) for which\n\n\\(R(\\theta; \\delta^*) \\leq R(\\theta; \\delta)\\) for all \\(\\theta\\in\\Theta\\), and\n\\(R(\\theta; \\delta^*) &lt; R(\\theta; \\delta)\\) for some \\(\\theta\\in\\Theta\\).\n\nIn this case we say \\(\\delta^*\\) strictly dominates \\(\\delta\\); more generally we can say \\(\\delta^*\\) *dominates* \\(\\delta\\) if we only have (1). An estimator is admissible if it is not inadmissible. We can see from our plot that \\(\\delta_3\\) is inadmissible because \\(\\delta_0\\) strictly dominates it.\nComparing the other three estimators is more difficult, however, because no one of them dominates any other. In most estimation problems, including this one, we can never hope to come up with an estimator that uniformly attains the smallest risk among all estimators. That is because, for example, we can always choose the constant estimator \\(\\delta(X) \\equiv 1/2\\) that simply ignores the data and always guesses that \\(\\theta = 1/2\\). This estimator may perform poorly for other values of \\(\\theta\\), but it is the only estimator that has exactly zero MSE for \\(\\theta = 1/2\\).\nIf we cannot hope to minimize the risk for every value of \\(\\theta\\) simultaneously then we must come up with some other way to resolve the inherent ambiguity in comparing all of the many estimators that we must choose among.\nIn our unit on estimation, we will consider two main strategies for resolving this ambiguity.\n\n\nStrategy 1: Summarizing the risk function by a scalar\nIf we can find a way to summarize the risk function for each estimator by a single real number that we want to minimize, then we can find an estimator that is optimal in this summary sense. The two main ways to summarize the risk are to examine the average-case risk and the worst-case risk.\n\nAverage-case risk (Bayes estimation)\nThe first option is to minimize some (weighted) average of the risk function over the parameter space \\(\\Theta\\) :\n\\[\n\\minz_{\\delta(\\cdot)} \\int_\\theta R(\\theta; \\delta)\\td \\Lambda(\\theta)\n\\]\nThe average is taken with respect to some measure \\(\\Lambda\\) of our choosing. If \\(\\Lambda(\\Theta) &lt; \\infty\\) we can assume without loss of generality that \\(\\Lambda\\) is a probability measure, since we could always normalize it without changing the minimization problem. Then, this average is simply the estimator’s expected risk, called the Bayes risk, or equivalently the expected loss averaging over the joint distribution of \\(\\theta\\) and \\(X\\). An estimator that minimizes the Bayes risk is called a Bayes estimator.\nIn the binomial problem above, \\(\\delta_1(X) = \\frac{X + 1}{n + 2}\\) is a Bayes estimator that minimizes the average-case risk with respect to the Lebesgue measure on \\(\\Theta = [0,1]\\). \\(\\delta_2(X) = \\frac{X+2}{n+4}\\) is also a Bayes estimator with respect to a different prior, specifically the \\(\\textrm{Beta}(2,2)\\) distribution. We will show this later.\nNote that minimizing the average-case risk may be a natural thing to do regardless of whether we “really believe” that \\(\\theta \\sim \\Lambda\\). Hence Bayes estimators are well-motivated even from a purely frequentist perspective; using them does not have to imply one has any specific position on the philosophical interpretation of probability.\nIf \\(\\Lambda(\\Theta) = \\infty\\) then we call \\(\\Lambda\\) an improper prior, and we can no longer interpret the corresponding Bayes risk as an expectation. But, as we will see, working with improper priors can sometimes be convenient and often leads to good estimators in practice.\n\n\nWorst-case risk (Minimax estimation)\nIf we are reluctant to average over the parameter space, we can instead seek to minimize the worst-case risk over the entire parameter space:\n\\[\n\\minz_{\\delta(\\cdot)} \\sup_{\\theta\\in\\Theta} R(\\theta; \\delta)\n\\]\nThis minimization problem has a game-theoretic interpretation if we imagine that, after we choose our estimator, Nature will adversarially choose the least favorable parameter value.\nAs we will see, minimax estimation is closely related to Bayes estimation and the minimax estimator is commonly a Bayes estimator.\nThe minimax perspective pushes us to choose estimators with flat risk functions, and indeed \\(\\delta_2(X) = \\frac{X + 2}{X + 4}\\) is the minimax estimator when \\(n = 16\\).\n\n\n\nStrategy 2: Restricting the choice of estimators\nThe second main strategy for resolving ambiguity is to restrict ourselves to choose an estimator that satisfies some additional side constraint.\n\nUnbiased estimation\nOne property we might want to demand of an estimator is that it be unbiased, meaning that \\(\\EE_\\theta [\\delta_0(X)] = g(\\theta)\\), for all \\(\\theta\\in\\Theta\\). This rules out, for example, estimators that ignore the data and always guess the same value.\nAs we will see, once we requiring unbiasedness there will often be a clear winner among all remaining estimators under consideration, called the uniformly minimum variance unbiased (UMVU) estimator, which uniformly minimizes the risk for any convex loss function.\nOf the four estimators we considered above, only \\(\\delta_0(X) = X/n\\) is unbiased, and it is indeed the UMVU for this problem."
  },
  {
    "objectID": "reader/sufficiency.html",
    "href": "reader/sufficiency.html",
    "title": "Sufficiency",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/sufficiency.html#sufficiency",
    "href": "reader/sufficiency.html#sufficiency",
    "title": "Sufficiency",
    "section": "Sufficiency",
    "text": "Sufficiency\nSufficiency is a central concept in statistics that allows us to focus on the essential aspects of the data set while ignoring details that are irrelevant to the inference problem. If \\(X\\sim P_\\theta\\) represents the entire data set, drawn from a model \\(\\cP = \\{P_\\theta:\\; \\theta \\in \\Theta\\}\\), then this lecture will concern the idea of a sufficient statistic \\(T(X)\\) that carries all of the information in the data that can help us learn about \\(\\theta\\).\nA statistic \\(T(X)\\) is any random variable which is a function of the data \\(X\\), and which does not depend on the unknown parameter \\(\\theta\\). We say the statistic \\(T(X)\\) is sufficient for the model \\(\\cP\\) if \\(P_\\theta(X \\mid T)\\) does not depend on \\(\\theta\\). This lecture will be devoted to interpreting this definition and giving examples.\nExample (Independent Bernoulli sequence): We introduced the binomial example from Lecture 2 by telling a story about an investigator who flips a biased coin \\(n\\) times and records the total number of heads, which has a binomial distribution. All of the estimators we considered were functions only of the (binomially-distributed) count of heads.\nBut if the investigator had actually performed this experiment, they would have observed more than just the total number of heads: they would have observed the entire sequence of \\(n\\) heads and tails. If we let \\(X_i\\) denote a binary indicator of whether the \\(i\\)th throw is heads, for \\(i=1,\\ldots,n\\), then we have assumed that these indicators are i.i.d. Bernoulli random variables:\n\\[\nX_1,\\ldots,X_n \\simiid \\text{Bern}(\\theta).\n\\]\nLet \\(T(X) = \\sum_i X_i \\sim \\text{Binom}(n,\\theta)\\) denote the summary statistic that we previously used to represent the entire data set. It is undeniable that we have lost some information by only recording \\(T(X)\\) instead of the entire sequence \\(X = (X_1,\\ldots,X_n)\\). As a result, we might wonder whether we could have improved the estimator by considering all functions of \\(X\\), not just functions of \\(T(X)\\).\nThe answer is that, no, we did not really lose anything by summarizing the data by \\(T(X)\\) because \\(T(X)\\) is sufficient. The joint pmf of the data set \\(X \\in \\{0,1\\}^n\\) (i.e., the density wrt the counting measure on \\(\\{0,1\\}^n\\)) is\n\\[\np_\\theta(x) = \\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i} = \\theta^{\\sum_i x_i}(1-\\theta)^{n-\\sum_i x_i}.\n\\]\nNote that this pmf depends only on \\(T(x)\\): it assigns probability \\(\\theta^t (1-\\theta)^{n-t}\\) to every sequence with \\(T(X)=t\\) total heads. As a result, the conditional distribution given \\(T(X)=t\\) should be uniform on all of the \\(\\binom{n}{t}\\) sequences with \\(t\\) heads. We can confirm this by calculating the conditional pmf directly:\n\\[\n\\begin{aligned}\n\\PP_\\theta(X = x \\mid T(X) = t)\n&= \\frac{\\PP_\\theta(X=x, \\sum_i X_i = t)}{\\PP_\\theta(T(X) = t)} \\\\[7pt]\n&= \\frac{\\theta^t (1-\\theta)^{n-t}1\\{\\sum_i x_i = t\\}}{\\theta^t(1-\\theta)^{n-t}\\binom{n}{t}}\\\\[5pt]\n&= \\binom{n}{t}^{-1}1\\{T(x) = t\\}.\n\\end{aligned}\n\\]\nSince the conditional distribution does not depend on \\(\\theta\\), \\(T(X)\\) is sufficient for the model \\(\\cP\\).\nAs we will see next, we didn’t really need to go to the trouble of calculating the conditional distribution. Once we noticed that the density depends on \\(x\\) only through \\(T(x)\\), we could have concluded that \\(T(X)\\) was sufficient."
  },
  {
    "objectID": "reader/sufficiency.html#factorization-theorem",
    "href": "reader/sufficiency.html#factorization-theorem",
    "title": "Sufficiency",
    "section": "Factorization theorem",
    "text": "Factorization theorem\nThe easiest way to verify that a statistic is sufficient is to show that the density \\(p_\\theta\\) factorizes into a part that involves only \\(\\theta\\) and \\(T(x)\\), and a part that involves only \\(h(x)\\).\nFactorization Theorem: Let \\(\\cP\\) be a model having densities \\(p_\\theta(x)\\) with respect to a common dominating measure \\(\\mu\\). Then \\(T(X)\\) is sufficient for \\(\\cP\\) if and only if there exist non-negative functions \\(g_\\theta\\) and \\(h\\) for which\n\\[\np_\\theta(x) = g_\\theta(T(x)) h(x),\n\\]\nfor almost every \\(x\\) under \\(\\mu\\).\nThe “almost every \\(x\\)” qualification means that\n\\[\n\\mu\\left(\\{x:\\; p_\\theta(x) \\neq g_\\theta(T(x))h(x)\\}\\right) = 0.\n\\]\nIt is needed to avoid counterexamples with continuous distributions where we could arbitrarily change the value of \\(p_{\\theta_0}(x_0)\\) for a single \\(\\theta_0\\) and \\(x_0\\) without actually changing any of the distributions.\nProof: The proof is easiest in the discrete case, so we don’t have to deal with conditioning on measure-zero events.\nFirst, assume that there exists a factorization \\(p_\\theta(x) = g_\\theta(T(x)) h(x)\\). Then we\n\\[\n\\begin{aligned}\n\\PP_\\theta(X = x \\mid T(X) = t)\n&= \\frac{p_\\theta(x)1\\{T(x) = t\\}}{\\displaystyle\\sum_{z:\\;T(z) = t} p_\\theta(z)}\\\\[7pt]\n&= \\frac{g_\\theta(t) h(x) 1\\{T(x) = t\\}}{g_\\theta(t)\\displaystyle\\sum_{z:\\;T(z) = t} h(z)}\\\\[7pt]\n&= \\frac{h(x) 1\\{T(x) = t\\}}{\\displaystyle\\sum_{z:\\;T(z) = t} h(z)},\n\\end{aligned}\n\\]\nwhich we see does not depend on \\(\\theta\\).\nNext consider the opposite direction. If \\(T(X)\\) is sufficient, then we can construct a factorization by writing\n. First, define\n\\[\ng_\\theta(t) = \\PP_\\theta(T(X)=t) = \\sum_{x:\\;T(x) = t} p_\\theta(x).\n\\]\nThen, let\n\\[\nh(x) = \\PP(X = x \\mid T(X) = T(x)),\n\\]\nwhich does not depend on \\(\\theta\\) by sufficiency. Then we have\n\\[\n\\PP_\\theta(X = x) = \\PP_\\theta(T(X) = T(x)) \\;\\cdot\\;\\PP_\\theta(X = x \\mid T(X) = T(x)) = g_\\theta(T(x)) h(x)\n\\]"
  },
  {
    "objectID": "reader/sufficiency.html#sufficient-statistics-in-exponential-families",
    "href": "reader/sufficiency.html#sufficient-statistics-in-exponential-families",
    "title": "Sufficiency",
    "section": "Sufficient statistics in exponential families",
    "text": "Sufficient statistics in exponential families"
  },
  {
    "objectID": "reader/score-fisher.html",
    "href": "reader/score-fisher.html",
    "title": "Score Function and Fisher Information",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/score-fisher.html#outline",
    "href": "reader/score-fisher.html#outline",
    "title": "Score Function and Fisher Information",
    "section": "1 Outline",
    "text": "1 Outline\n\nScore function\nFisher information\nCramér-Rao Lower Bound\nExamples"
  },
  {
    "objectID": "reader/score-fisher.html#motivation-tangent-family",
    "href": "reader/score-fisher.html#motivation-tangent-family",
    "title": "Score Function and Fisher Information",
    "section": "2 Motivation: Tangent Family",
    "text": "2 Motivation: Tangent Family\nConsider a family of densities:\n\\[p(x; \\theta) = e^{\\theta'T(x) - A(\\theta)}h(x)\\]\nwhere \\(\\theta \\in \\RR^d\\) and \\(A(\\theta) = \\log \\int e^{\\theta'T(x)}h(x)dx\\).\nFor this family:\n\n\\(T(X)\\) is complete sufficient\n\\(T(X)\\) is minimal\n\\(\\PP_\\theta(T(X) = t) = e^{\\theta't - A(\\theta)}\\)\n\\(\\EE_\\theta[T(X)] = A'(\\theta)\\)\n\nLet \\(\\theta_0 \\in \\RR^d\\) be fixed. Define the tangent family\n\\[q(x; t) = e^{t'\\nabla l_{\\theta_0}(x) - k(t)}p_{\\theta_0}(x)\\]\nwhere \\(k(t) = \\log \\int e^{t'\\nabla l_{\\theta_0}(x)}p_{\\theta_0}(x)dx\\).\nThen \\(\\nabla l_{\\theta_0}(X)\\) is complete sufficient for the tangent family at \\(\\theta_0\\).\nThis is called the Score function."
  },
  {
    "objectID": "reader/score-fisher.html#score-function",
    "href": "reader/score-fisher.html#score-function",
    "title": "Score Function and Fisher Information",
    "section": "3 Score Function",
    "text": "3 Score Function\nAssume a family \\(\\cP\\) has densities \\(p_\\theta\\) with respect to a measure \\(\\mu\\), for \\(\\theta \\in \\Theta \\subseteq \\RR^d\\). Assume additionally that these densities have common support: that \\(\\{x: p_\\theta(x) &gt; 0\\}\\) is the same for all \\(\\theta\\).\nRecall the log-likelihood is \\(l(\\theta;X) = \\log p_\\theta(X)\\) (thought of as a random function of \\(\\theta\\))\nDefinition: The Score function is \\(\\nabla l_\\theta(X)\\).\nIt plays a key role in many areas of statistics, especially in asymptotics. We can think of it as a “local complete sufficient statistic.” For \\(\\eta \\approx 0\\), and \\(\\theta_0 \\in \\Theta^\\circ\\), we have\n\\[p_{\\theta_0+\\eta}(x) = e^{\\ell(\\theta_0 + \\eta; x)} \\approx e^{\\eta'\\nabla \\ell(\\theta_0;x)}p_{\\theta_0}(x).\\]"
  },
  {
    "objectID": "reader/score-fisher.html#differential-identities-and-the-fisher-information",
    "href": "reader/score-fisher.html#differential-identities-and-the-fisher-information",
    "title": "Score Function and Fisher Information",
    "section": "4 Differential Identities and the Fisher Information",
    "text": "4 Differential Identities and the Fisher Information\nAssuming enough regularity, we can arrive at some important differential identities by differentiating both sides of the equation\n\\[1 = \\int_\\cX e^{\\ell(\\theta;x)}\\,d\\mu(x).\\]\nDifferentiating both sides with respect to \\(\\theta_j\\), we obtain \\[0 = \\int_\\cX \\frac{\\partial}{\\partial \\theta_j} \\ell(\\theta; x) e^{\\ell(\\theta; x)}\\,d\\mu(x) = \\EE_\\theta \\left[\\frac{\\partial}{\\partial\\theta_j}\\ell(\\theta;X)\\right].\\] Collecting these identities into a vector, we obtain \\[\\EE_\\theta [\\nabla \\ell(\\theta; X)] = 0.\\] Importantly, note that this identity only holds if the \\(\\theta\\) in the subscript (defining the distribution with respect to which the expectation is taken) matches the \\(\\theta\\) at which the gradient is being evaluated.\nIf we differentiate the identity a second time with respect to \\(\\theta_k\\), we obtain \\[0 = \\int_\\cX \\left(\\frac{\\partial^2\\ell}{\\partial \\theta_j\\partial\\theta_k} + \\frac{\\partial \\ell}{\\partial \\theta_j}\\frac{\\partial \\ell}{\\partial\\theta_k}\\right) e^{\\ell}\\,d\\mu = \\EE_\\theta\\left[\\frac{\\partial^2\\ell}{\\partial \\theta_j\\partial\\theta_k}\\right] + \\EE_\\theta\\left[\\frac{\\partial \\ell}{\\partial \\theta_j}\\frac{\\partial \\ell}{\\partial \\theta_k}\\right]\n%= \\EE_\\theta\\left[\\frac{\\partial^2\\ell}{\\partial \\theta_j\\partial\\theta_k}\\right] + \\Cov_\\theta\\left(\\frac{\\partial \\ell}{\\partial \\theta_j},\\frac{\\partial \\ell}{\\partial \\theta_k}\\right).\n\\] Again collecting these identities into a matrix, and noting that \\[\\EE_\\theta\\left[\\frac{\\partial \\ell}{\\partial \\theta_j}\\frac{\\partial \\ell}{\\partial \\theta_k}\\right] = \\Cov_\\theta\\left(\\frac{\\partial \\ell}{\\partial \\theta_j},\\frac{\\partial \\ell}{\\partial \\theta_k}\\right),\\] we obtain \\[\\Var_\\theta\\left(\\nabla\\ell(\\theta;X)\\right) = \\EE_\\theta\\left[-\\nabla^2\\ell(\\theta;X)\\right],\\] again with the important observation that the \\(\\theta\\) in both subscripts must match the \\(\\theta\\) where the first and second derivatives are evaluated.\nThe left-hand side of the last equation, the variance of the score, is called the Fisher Information matrix \\[ J(\\theta) := \\Var_\\theta(\\nabla\\ell(\\theta;X)). \\] Note \\(J(\\theta)\\) is always positive semidefinite. It is possible to extend this definition to certain models where \\(\\ell(\\theta;x)\\) is not differentiable with respect to \\(\\theta\\), such as the Laplace location family. However we will not explore these generalizations."
  },
  {
    "objectID": "reader/score-fisher.html#cramér-rao-lower-bound",
    "href": "reader/score-fisher.html#cramér-rao-lower-bound",
    "title": "Score Function and Fisher Information",
    "section": "5 Cramér-Rao Lower Bound",
    "text": "5 Cramér-Rao Lower Bound\nLet \\(\\delta(X)\\) be any real-valued statistic. Let \\(g(\\theta) = \\EE_\\theta[\\delta]\\), so \\(\\delta\\) is an unbiased estimator for \\(g(\\theta)\\). If we repeat the idea of differentiating \\(g(\\theta) = \\int \\delta(x) e^{\\ell(\\theta;x)}\\,d\\mu(x)\\) with respect to \\(\\theta_j\\) for each \\(j\\), and collect the resulting partial derivatives into a vector, we obtain\n\\[\\nabla g(\\theta) = \\int \\delta(x) \\nabla \\ell(\\theta;x) e^{\\ell(\\theta;x)}\\,d\\mu(x) = \\EE_\\theta\\left[\\delta(X) \\nabla\\ell(\\theta;X)\\right] = \\Cov_\\theta\\left(\\delta(X), \\nabla\\ell(\\theta;X)\\right).\\] Combining these results with the Cauchy-Schwarz inequality gives us the Cramér-Rao Lower Bound, also known as the Information lower bound. For a single parameter (\\(d=1\\)), we have \\[\\Var_\\theta(\\delta(X)) \\cdot \\Var_\\theta(\\dot{\\ell}(\\theta;X)) \\geq \\Cov_\\theta(\\delta(X), \\dot{\\ell}(\\theta; X))^2, \\] so after rearranging terms and applying identities, \\[\\Var_\\theta(\\delta(X)) \\geq \\frac{\\dot{g}(\\theta)^2}{J(\\theta)}.\\]\nFor the multivariate case (\\(d&gt;1\\)), we have more generally \\[ \\Var_\\theta(\\delta(X) \\geq \\nabla g(\\theta)'J(\\theta)^{-1}\\nabla g(\\theta).\\] The interpretation of this identity is that no unbiased estimator for \\(g(\\theta)\\) can have variance smaller than \\(\\nabla g(\\theta)'J(\\theta)^{-1}\\nabla g(\\theta)\\). In particular, if \\(g(\\theta) = \\theta_j\\), no estimator can have variance smaller than \\((J(\\theta)^{-1})_{jj}\\).\n\\[\\Var_\\theta(\\delta) \\geq \\Var_\\theta(\\delta(X)) \\Cov_\\theta(\\delta, \\nabla l_\\theta(X))I(\\theta)^{-1}\\Cov_\\theta(\\delta, \\nabla l_\\theta(X))' = g'(\\theta)I(\\theta)^{-1}g'(\\theta)'\\]\n\n\n\n\n\n\nExpand to see proof\n\n\n\n\n\nFor any \\(a \\in \\RR^d\\), we can write \\[\\begin{aligned}\n\\Var_\\theta(\\delta(X)) \\cdot a'J(\\theta)a &= \\Var_\\theta(\\delta)\\Var_\\theta(a'\\nabla\\ell(\\theta;X))\\\\\n&\\geq \\Cov_\\theta(\\delta(X), a'\\nabla\\ell(\\theta;X))^2\\\\\n&= \\left(a'\\nabla \\Cov_\\theta(\\delta, \\nabla\\ell(\\theta))\\right)^2\\\\\n&= (a'\\nabla g(\\theta))^2.\n\\end{aligned}\\]\nThus we obtain for all nonzero \\(a \\in \\RR^d\\),\n\\[\\Var_\\theta(\\delta(X)) \\geq \\frac{(a'\\nabla g(\\theta))^2}{a'J(\\theta)a}.\\]\nWe obtain the result by optimizing the bound, with \\(a = J(\\theta)^{-1}\\nabla g(\\theta)\\) (show this as an exercise)."
  },
  {
    "objectID": "reader/score-fisher.html#examples",
    "href": "reader/score-fisher.html#examples",
    "title": "Score Function and Fisher Information",
    "section": "6 Examples",
    "text": "6 Examples\nExample: i.i.d. sample\nAssume \\(X_1, \\ldots, X_n \\simiid p_\\theta^{(1)}(x)\\), for \\(\\theta \\in \\Theta \\subseteq \\RR^d\\).\nAssume additionally that \\(p_\\theta^{(1)}\\) is “regular:” it has common support, and finite derivative w.r.t. \\(\\theta\\).\nThen the full data density is \\(p_\\theta(x) = \\prod_i p_\\theta^{(1)}(x_i)\\).\nDefine the single-sample log-likelihood \\(\\ell_1(\\theta;x_i) = \\log p_\\theta^{(1)}(x_i)\\); then we have \\(\\ell(\\theta;x) = \\sum_i \\ell_1(\\theta;x_i)\\).\nThen the Fisher information for the full sample is \\[J(\\theta) = \\Var_\\theta(\\nabla \\ell(\\theta; X)) = \\sum_{i=1}^n \\Var_\\theta(\\nabla \\ell_1(\\theta; X_i)) = n J_1(\\theta),\\] where \\(J_1(\\theta) = \\Var_\\theta(\\nabla\\ell(\\theta; X_1))\\) is the Fisher information for a single sample.\nAs a result, we see that the Information bound scales like \\(n^{-1}\\) for regular families; in other words, the standard deviation of an estimator should scale roughly like \\(1/\\sqrt{n}\\).\nExample: exponential family\nSuppose we have an exponential family of the form \\[ p_\\eta(x) = e^{\\eta'T(x) - A(\\eta)} h(x).\\]\nThe log-likelihood is \\(\\ell(\\eta;X) = \\eta'T(X) - A(\\eta) + \\log h(X)\\), and its gradient (the score) is \\[\\nabla \\ell(\\eta;X) = T(X) - \\nabla A(\\eta) = T(X) - \\EE_\\eta T(X).\\] Since \\(\\EE_\\eta T(X)\\) is nonrandom, the variance is \\[ J(\\eta) = \\Var_\\eta (T(X)) = \\nabla^2 A(\\eta).\\]\nWe could alternatively derive the Fisher information from taking a second derivative with respect to \\(\\eta\\), giving \\[ \\nabla^2\\ell(\\eta;X) = -\\nabla^2 A(\\eta),\\] which is deterministically equal to \\(-\\Var_\\eta(T(X))\\), so we have confirmed the identity \\(J(\\eta) = -\\EE_\\eta[\\nabla^2 \\ell(\\eta;X)]\\).\nExample: Curved exponential family\nNext, consider a curved version of the previous family, parameterized by \\(\\theta \\in \\RR\\): \\[p_\\theta(x) = e^{\\eta(\\theta)'T(x) - B(\\theta)}h(x),\\quad \\text{ with } B(\\theta) = A(\\eta(\\theta))\\] Again, the log-likelihood is \\[\\ell(\\theta;X) = \\eta(\\theta)'T(x) - B(\\theta)  + \\log h(x),\\] and its first derivative is \\[\\begin{aligned}\n\\dot{\\ell}(\\theta;X) &= \\dot{\\eta}(\\theta)'T(X) - \\dot{\\eta}(\\theta)'\\nabla_\\eta A(\\eta(\\theta))\\\\\n&= \\dot{\\eta}(\\theta) '\\left(T(X) - \\nabla_\\eta A(\\eta(\\theta))\\right)\\\\\n&= \\dot{\\eta}(\\theta)'(T(X) - \\EE_\\theta T(X)).\\end{aligned}\\]\nAs a result, the Fisher information is \\[J(\\theta) = \\Var_\\theta(\\dot{\\eta}(\\theta)'T(X)) =  \\dot{\\eta}(\\theta)'\\Var_\\theta(T(X))\\dot{\\eta}(\\theta).\\] Note in this model \\(\\dot{\\eta}'T(X)\\) is a “local complete sufficient statistic” for the model near \\(\\theta\\)."
  },
  {
    "objectID": "reader/score-fisher.html#efficiency",
    "href": "reader/score-fisher.html#efficiency",
    "title": "Score Function and Fisher Information",
    "section": "7 Efficiency",
    "text": "7 Efficiency\nThe CRLB is not necessarily attainable.\nWe define the efficiency of an unbiased estimator as:\n\\[\\text{eff}_\\delta(\\theta) = \\frac{\\text{CRLB}(\\theta)}{\\Var_\\theta(\\delta)} \\leq 1,\\]\nWe say \\(\\delta(X)\\) is efficient if \\(\\text{eff}_\\delta(\\theta) = 1\\) for all \\(\\theta\\).\nFor \\(g(\\theta)=\\theta\\in \\RR\\), the efficiency depends on how correlated \\(\\delta(X)\\) is with the score: \\[\\begin{aligned}\\text{eff}_\\delta(\\theta) &= \\frac{\\Cov_\\theta(\\delta(X), \\dot{\\ell}(\\theta;X))^2}{\\Var_\\theta(\\delta(X)) \\cdot \\Var_\\theta(\\dot{\\ell}(\\theta;X))}\\\\\n&= \\Corr_\\theta(\\delta,\\dot{\\ell}(\\theta))^2\n\\end{aligned}\\]\nThus, an efficient estimator for \\(\\theta\\) is one that is perfectly correlated with the score. This is rarely achieved in finite samples, but we can often approach it asymptotically as \\(n \\to \\infty\\)."
  },
  {
    "objectID": "reader/measure-theory-basics.html",
    "href": "reader/measure-theory-basics.html",
    "title": "Measure Theory Basics",
    "section": "",
    "text": "\\[\n\\newcommand{\\cB}{\\mathcal{B}}\n\\newcommand{\\cF}{\\mathcal{F}}\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cP}{\\mathcal{P}}\n\\newcommand{\\cX}{\\mathcal{X}}\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\PP}{\\mathbb{P}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\ZZ}{\\mathbb{Z}}\n\\newcommand{\\td}{\\,\\textrm{d}}\n\\newcommand{\\simiid}{\\stackrel{\\textrm{i.i.d.}}{\\sim}}\n\\newcommand{\\eqas}{\\stackrel{\\textrm{a.s.}}{=}}\n\\newcommand{\\eqPas}{\\stackrel{\\cP\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqmuas}{\\stackrel{\\mu\\textrm{-a.s.}}{=}}\n\\newcommand{\\eqD}{\\stackrel{D}{=}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\!\\perp}\n\\DeclareMathOperator*{\\minz}{minimize\\;}\n\\newcommand{\\Var}{\\textnormal{Var}}\n\\newcommand{\\Cov}{\\textnormal{Cov}}\n\\newcommand{\\Corr}{\\textnormal{Corr}}\n\\]"
  },
  {
    "objectID": "reader/measure-theory-basics.html#measure-theory-a-rigorous-grounding-for-probability",
    "href": "reader/measure-theory-basics.html#measure-theory-a-rigorous-grounding-for-probability",
    "title": "Measure Theory Basics",
    "section": "Measure theory: a rigorous grounding for probability",
    "text": "Measure theory: a rigorous grounding for probability\nMeasure theory is an area of mathematics concerned with measuring the “size” of subsets of a certain set. Soon after it was developed in the the early twentieth century, the great Soviet mathematician Kolmogorov realized it could be applied to give a rigorous grounding to probability theory, it was a major advance in understanding and resolving certain paradoxes in probability theory. David Aldous gives a nice discussion of this history.\nThis is not a course on measure-theoretic probability and we will not rigorously develop the subject. However, it will be useful to draw on some of the basics of measure theory, to simplify our notation throughout the course and to clarify certain concepts around integration and conditioning. Homework 0 illustrates some of the interesting aspects of measure theory and why it is useful."
  },
  {
    "objectID": "reader/measure-theory-basics.html#measures",
    "href": "reader/measure-theory-basics.html#measures",
    "title": "Measure Theory Basics",
    "section": "Measures",
    "text": "Measures\nGiven a set \\(\\cX\\), a measure\\(\\mu\\) is a certain kind of function mapping “nice enough” subsets \\(A \\subseteq \\cX\\) to non-negative numbers \\(\\mu(A) \\in [0,\\infty]\\).\nExample 1 (Counting measure): If \\(\\cX\\) is countable, e.g. \\(\\cX = \\mathbb{Z}\\), then a natural measure is the counting measure \\(\\#(A)\\), which simply counts the number of points in a subset \\(A\\). That is, \\(\\#(\\{0,1\\}) = 2\\), and \\(\\#(\\{2,4,6,8,\\ldots\\}) = \\infty\\) .\nExample 2 (Lebesgue measure): If \\(\\cX = \\RR^n\\) for some integer \\(n\\), a natural measure is the Lebesgue measure \\(\\lambda(A)\\), which returns the volume of a subset \\(A\\). Roughly speaking, we can write\n\\[\n\\lambda(A) = \\int \\cdots \\int_A \\td x_1\\td x_2\\cdots \\td x_n.\n\\]\nExample 3 (Gaussian measure): Now taking \\(\\cX = \\RR\\), we might instead want to define the “size” of a set as the probability that a standard Gaussian random variable \\(Z \\sim \\cN(0,1)\\) is observed to be in the set \\(A\\). That is, we can define the measure:\n\\[\nP_Z(A) = \\PP(Z \\in A) = \\int_A \\phi(x)\\td x, \\quad \\text{ where } \\;\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\n\\]\nis the probability density function of \\(Z\\).\nAs it turns out, it is not so obvious how to define what exactly we mean by the right-hand side of the previous two equations; in fact, it is not even possible to define the volume of every subset \\(A \\in \\mathbb{R}\\). In Homework 0, Problem 3, you will use the axiom of choice to construct pathological subsets (so-called non-measurable sets) to which we cannot sensibly assign any volume.\nOne of the original motivations for measure theory was to provide a framework for excluding these pathological sets and rigorously defining integrals over the other, nicer sets. In general, the domain of a measure is not all subsets of \\(\\cX\\) (called the power set and notated \\(2^{\\cX}\\)), but rather a collection of nice subsets \\(\\cF \\subseteq 2^{\\cX}\\).\nFormally, the collection \\(\\cF\\) must be a \\(\\sigma\\)-field, meaning that it satisfies certain closure properties. We say \\(\\cF\\) is a \\(\\sigma\\)-field (or \\(\\sigma\\)-algebra) if\n\nThe full set \\(\\cX\\) is in \\(\\cF\\).\nIf \\(A\\) is in \\(\\cF\\) then its complement \\(\\cX \\setminus A\\) is also in \\(\\cF\\) (i.e., \\(\\cF\\) is closed under complementation)\nIf \\(A_1,A_2,\\ldots \\in \\cF\\) then \\(\\bigcup_{i=1}^\\infty A_i\\) is also in \\(\\cF\\) (i.e. \\(\\cF\\) is closed under countable unions)\n\nNote: The details of this definition are not important for purposes of this course.\nExample: If \\(\\cX\\) is countable we can take \\(\\cF\\) to be the entire power set.\nExample: If \\(\\cX = \\mathbb{R}^n\\) we will typically use the Borel \\(\\sigma\\)-field \\(\\cB\\), defined as the smallest \\(\\sigma\\)-field that includes all open rectangles \\((a_1,b_1)\\times (a_2,b_2) \\times \\cdots \\times (a_n, b_n)\\), where \\(a_i &lt; b_i\\) for all \\(i\\). That is, we start with the open rectangles and recursively apply the closure properties to obtain a very large collection of sets, which informally we can think of as containing all non-pathological subsets of \\(\\mathbb{R}^n\\).\nWe are now ready to define a measure. We call a pair of a set \\(\\cX\\) and an associated \\(\\sigma\\)-field \\(\\cF \\subseteq 2^{\\cX}\\) a measurable space. Given a measurable space \\((\\cX, \\cF)\\), a measure is a function \\(\\mu: \\cF \\to \\mathbb{R}\\) satisfying three properties:\n\nNon-negativity: \\(\\mu(A) \\geq 0\\) for all \\(A\\in \\cF\\).\nEmpty set maps to zero: \\(\\mu(\\emptyset) = 0\\)\nCountable additivity: If \\(A_1,A_2,\\ldots\\in \\cF\\) are all disjoint, then\n\n\\[\n\\mu\\left(\\bigcup_{i=1}^\\infty A_i\\right) = \\sum_{i=1}^\\infty \\mu(A_i)\n\\]\nIf \\(\\mu\\) is a measure on \\((\\cX, \\cF)\\) we call \\((\\cX, \\cF, \\mu)\\) a measure space.\nIn the special case \\(\\mu(\\cX) = 1\\), we call \\(\\mu\\) a probability measure and \\((\\cX, \\cF, \\mu)\\) is called a probability space."
  },
  {
    "objectID": "reader/measure-theory-basics.html#integrals",
    "href": "reader/measure-theory-basics.html#integrals",
    "title": "Measure Theory Basics",
    "section": "Integrals",
    "text": "Integrals\nOne very nice thing about measures is that they let us define integrals of (nice enough) real-valued functions on \\(\\cX\\) with respect to the measure \\(\\mu\\), meaning the integral is “weighted” in a way that assigns total weight \\(\\mu(A)\\) to each set \\(A\\). We will use the notation \\(\\int f(x)\\,\\td\\mu(x)\\), or just \\(\\int f \\td\\mu\\).\nTo construct this integral, we begin by defining it for indicator functions, and then extend to more general functions by linearity and limits, in a few steps:\nFirst, for an indicator function \\(1_A(x) = 1\\{x \\in A\\}\\) of a set \\(A \\in \\cF\\), it is straightforward to define the integral as \\(\\int 1_A \\td\\mu = \\mu(A)\\) (note if \\(A \\notin \\cF\\) this does not work, but we are only defining the integral for a class of “nice” functions determined by our \\(\\sigma\\)-field)\nNext, consider a simple function \\(f(x) = \\sum_{i=1}^\\infty c_i 1_{A_i}(x)\\), with all \\(c_i \\geq 0\\) and \\(A_i \\in \\cF\\). Because the integral should be linear, we should have\n\\[\n\\int f\\td\\mu = \\sum_{i=1}^\\infty c_i \\int 1_{A_i}\\td\\mu = \\sum_{i=1}^\\infty c_i \\mu(A_i)\n\\]\nThird, we can extend to all sufficiently nice non-negative functions by approximating them from below with a series of simple functions:\n\\[\n\\int f\\td\\mu = \\lim_{i=1}^\\infty \\int f_i\\td\\mu.\n\\]\nThis idea is illustrated in the picture below:\n\n\n\nApproximating a non-negative function from below by a series of simple (piecewise constant) functions. The red function is an indicator, the blue function is a simple function, and the dashed orange function is a simple function that approximates the orange curve.\n\n\nFinally, we can write any real-valued function as the sum of its positive and negative parts, \\(f(x) = f^+(x) - f^-(x)\\), where \\(f^+(x) = \\max\\{f(x), 0\\}\\) and \\(f^-(x) = \\max\\{-f(x), 0\\}\\). Then both \\(f^+\\) and \\(f^-\\) have non-negative (possibly infinite) integrals. Then we simply take\n\\[\n\\int f\\td\\mu = \\int f^+\\td\\mu - \\int f^-\\td\\mu \\in [-\\infty, \\infty],\n\\]\ncalling the difference undefined if the integrals of both \\(f^+\\) and \\(f^-\\) are infinite.\nAs a result, we have \\(\\int f\\td\\mu\\) for any function \\(f\\) whose positive and negative parts can both be approximated from below by simple functions. Note that we have left out some important details in this presentation (for example we have not characterized which functions \\(f\\) are nice enough to be approximated well by simple functions) but these details are unimportant for this class. The important thing to know is that to any measure \\(\\mu\\) there corresponds a well-defined integral \\(\\int \\cdot \\td\\mu\\), which behaves as we would expect it to.\nWe can now return to our previous examples of measures and ask what the corresponding integrals are:\nExample 1, continued (Counting measure): An integral with respect to \\(\\#\\) just adds up all the values of \\(f(x)\\):\n\\[\n\\int f\\td\\# = \\sum_{x\\in \\cX} f(x)\n\\]\nExample 2, continued (Lebesgue measure): An integral with respect to the Lebesgue measure is called a Lebesgue integral, which is essentially just the usual integral you are used to from calculus class:\n\\[\n\\int f\\td\\lambda = \\int\\cdots \\int f(x) \\td x_1 \\cdots \\td x_n.\n\\]\nThe Lebesgue integral extends the Riemann integral to a more general class of functions, in the sense that if the Riemann integral of \\(f\\) is defined then the Lebesgue integral is also well defined and the two integrals coincide. But the Lebesgue integral is also well-defined for functions like \\(f(x) = 1\\{x \\in \\mathbb{Q}\\}\\), for which the Riemann integral is not well-defined (Exercise: what is the Lebesgue integral of \\(1\\{x \\in \\mathbb{Q}\\}\\)?)\nExample 3, continued (Gaussian measure): Note that \\(P_Z(A)\\) is defined as the (Lebesgue) integral of \\(1_A(x)\\phi(x)\\). By extension, the integral of \\(f\\) with respect to \\(P_Z\\) is the Lebesgue integral of \\(f(x) \\phi(x)\\), which is nothing more than the expectation of \\(f(Z)\\):\n\\[\n\\int f\\td P_Z = \\int_{-\\infty}^\\infty f(x) \\phi(x) \\td x = \\EE[f(Z)].\n\\]"
  },
  {
    "objectID": "reader/measure-theory-basics.html#densities",
    "href": "reader/measure-theory-basics.html#densities",
    "title": "Measure Theory Basics",
    "section": "Densities",
    "text": "Densities\nWe have just seen in the last two examples that there is a special relationship between the Lebesgue measure \\(\\lambda\\) on \\(\\RR\\) and the Gaussian measure \\(P_Z\\), allowing us to evaluate integrals with respect to \\(P\\) by turning them into integrals with respect to \\(\\lambda\\), namely \\(\\int f(x)\\td P_Z(x) = \\int f(x)\\phi(x) \\td\\lambda(x)\\).\nThis is a happy fact, since mathematicians have gone to a lot of trouble figuring out how to calculate integrals with respect to the usual (Lebesgue) measure. Most of the expectations we want to calculate in statistics are integrals with respect to some joint probability measure over random variables, and we certainly wouldn’t want to have to reinvent the wheel of integration every time we want to do calculations with respect to a new random variable.\nNote that we can’t turn integrals for every random variable into Lebesgue integrals. If \\(Y\\) follows a binomial distribution, for example, we can just as well define \\(P_Y(A) = \\PP(Y \\in A)\\), but there is no counterpart to \\(\\phi\\) that would let us turn \\(P_Y\\) integrals into Lebesgue integrals in the same way.\nFormally, consider a measurable space \\((\\cX, \\cF)\\), with two measures \\(P\\) and \\(\\mu\\). We say \\(P\\) is absolutely continuous with respect to \\(\\mu\\) if \\(P(A) = 0\\) whenever \\(\\mu(A) = 0\\). In notation, we write \\(P \\ll \\mu\\).\nIf \\(P \\ll \\mu\\) then, under mild conditions, we can always define a density function \\(p:\\cX \\mapsto [0,\\infty)\\) such that\n\\[\nP(A) = \\int 1_A(x) p(x)\\td\\mu(x), \\quad \\text{ for all } A \\in \\cF,\n\\]\nand by extension \\(\\int f(x)\\td P(x) = \\int f(x) p(x) \\td\\mu(x)\\).\nThe function \\(p\\) is called the density function or Radon-Nikodym derivative of \\(P\\) with respect to \\(\\mu\\). It is sometimes written using the suggestive notation \\(\\frac{\\td P}{\\td\\mu}(x)\\). Whenever we have a density function we can turn integrals with respect to \\(P\\) into integrals with respect to \\(\\mu\\) simply by multiplying the integrand by \\(p\\).\nIf we do not specify what \\(\\mu\\) is, it is assumed to be the Lebesgue measure; that is, if we say \\(P\\) is absolutely continuous with no further elaboration, we mean \\(P \\ll \\lambda\\).\nIf \\(P\\) is a probability measure, we call \\(p(x)\\) its probability density function (with respect to \\(\\mu\\)). If \\(\\mu\\) is a counting measure, we call \\(p(x)\\) its probability mass function. These are abbreviated pdf and pmf, respectively."
  },
  {
    "objectID": "reader/measure-theory-basics.html#probability-spaces-and-random-variables",
    "href": "reader/measure-theory-basics.html#probability-spaces-and-random-variables",
    "title": "Measure Theory Basics",
    "section": "Probability spaces and random variables",
    "text": "Probability spaces and random variables\nA typical statistics problem involves many, random variables of various types (e.g. some discrete and some continuous random variables), some of which may be functions of others. The overall joint distribution is defined implicitly by specifying the variables’ relationships to one another, or giving some sequence of rules for how they are all generated. We will want to ask about probabilities of events that involve functions of multiple random variables, e.g. a natural question to ask in a simple variance estimation problem with i.i.d. random variables \\(X_1,\\ldots,X_n\\) might be “what is the probability that \\(\\left|\\frac{1}{n-1}\\sum_{i=1}^n \\left(X_i - \\overline X\\right)^2 - \\sigma^2\\right| &lt; \\delta\\) ?”\nThe notation in the previous section doesn’t allow us to ask questions like this without, e.g., massaging the above event into the set of \\((X_1,\\ldots,X_n)\\) vectors for which the event would hold. This would become even more difficult in more complicated setups.\nInstead of trying to work directly with the measure corresponding to the joint distribution of all of the random variables involved, it can be convenient to instead think of the variables as all being functions of some abstract “outcome” \\(\\omega\\) that encompasses all of the randomness in the problem. We introduce an abstract probability space \\((\\Omega, \\cF, \\PP)\\) where\n\n\\(\\omega \\in \\Omega\\) is called an outcome,\n\\(A \\in \\cF\\) is called an event,\n\\(\\PP(A)\\) is called the probability of \\(A\\).\n\nThen a random variable is any (nice enough) function \\(X:\\; \\Omega \\to \\cX\\). We say \\(X\\) has distribution \\(P\\), and write \\(X \\sim P\\), if\n\\[\n\\PP(X \\in B) = \\PP(\\{\\omega:\\; X(\\omega) \\in B\\}) = P(B).\n\\]We say the real-valued random variable \\(X\\) is continuous if its distribution is absolutely continuous (with respect to the Lebesgue meaure). If \\(X\\) is a random variable, then \\(f(X)\\) is also a random variable for any (nice enough) function \\(f\\).\nLikewise, the expectation of a random variable is defined as an integral with respect to \\(\\PP\\):\n\\[\n\\EE[X] = \\int X(\\omega) \\td\\PP(\\omega), \\quad \\text{ and } \\quad \\EE[f(X,Y)] = \\int f(X(\\omega), Y(\\omega)) \\td\\PP(\\omega).\n\\]\nUsually, to do real calculations we will eventually boil \\(\\PP\\) or \\(\\EE\\) into a composition of integrals and/or sums."
  },
  {
    "objectID": "reader/measure-theory-basics.html#conditional-probability",
    "href": "reader/measure-theory-basics.html#conditional-probability",
    "title": "Measure Theory Basics",
    "section": "Conditional probability",
    "text": "Conditional probability\nWhile it is beyond the scope of this course, measure theory also allows us to patch the definition of conditional probability and conditional expectation. Given two events \\(A\\) and \\(B\\), if \\(\\PP(B) &gt; 0\\), we can unproblematically define the conditional probability of \\(A\\) given \\(B\\) as \\(\\PP(A \\mid B) = \\PP(A \\cap B) / \\PP(B)\\), but this definition obviously fails when \\(\\PP(B) = 0\\).\nGenerally speaking, we cannot necessarily define \\(\\PP(A \\mid B)\\) for measure zero events \\(B\\) (Homework 0 includes a problem illustrating the inherent ambiguity of this definition). But, for example, if \\(X\\) and \\(Y\\) are both continuous random variables with some dependence between them we would like to be able to discuss, e.g., the distribution or expectation of \\(Y\\) given that \\(X\\) takes on some specific value \\(x\\). We can do this by defining the conditional expectation \\(\\EE(Y \\mid X)\\) as a random variable \\(g(X)\\), which has the property \\(\\EE[(Y - g(X)) 1_A(X)] = 0\\) for all (nice) subsets \\(A\\). By evaluating this function \\(g\\) at \\(x\\) we can answer the question we asked earlier. However, note this explanation is informal and brushes many important points under the rug; for a more complete explanation, take Stat 205A.\nHaving defined the conditional expectation, we can also ask about the conditional distribution of \\(Y\\) by evaluating the conditional expectation on new random variables defined with indicator functions: \\(\\PP(Y \\in A \\mid X) = \\EE[1_A(Y) \\mid X]\\) ."
  },
  {
    "objectID": "reader/measure-theory-basics.html#more-definitions",
    "href": "reader/measure-theory-basics.html#more-definitions",
    "title": "Measure Theory Basics",
    "section": "More definitions",
    "text": "More definitions\nFinally, this section includes some scattered definitions to remind you of a few more useful definitions which you likely have already seen in your previous probability courses.\nIf \\(\\PP(A) = 1\\), we say \\(A\\) occurs almost surely.\nThe variance of a random variable \\(X\\) is \\(\\textrm{Var}(X) = \\EE[X^2] - \\EE [X]\\)\nSee Keener Ch. 1 for more on probability."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 210a: Theoretical Statistics",
    "section": "",
    "text": "Under construction"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Statistics 210a: Theoretical Statistics",
    "section": "Schedule",
    "text": "Schedule\nHere we use Quarto’s EJS templating.\nFor a style similar to that of Stat 20, modify index.qmd to point to assets/{schedule-alt.ejs,buttons-alt.ejs} and to {schedule-alt.yml,buttons-alt.yml} and _quarto.yml to point to assets/styles-alt.scss.\n\n\n   Week 1\n   \n   \n   \n   \n           \n           Aug 29:\n           Lecture 1 Introduction\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 2\n   \n   \n   \n   \n           \n           Sep 3:\n           Lecture 2 Probability\n                \n                  Handwritten notes\n                \n           \n           \n           Sep 5:\n           Lecture 3 Decision theory\n                \n                  Handwritten notes\n                \n           \n           \n           Sep 6:\n           Recitation 1 Probability review\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 3\n   \n   \n   \n   \n           \n           Sep 10:\n           Lecture 4 Estimation\n                \n                  Handwritten notes\n                \n           \n           \n           Sep 11:\n           Homework 1 Homework 1 due 11:59pm\n                \n                  LaTeX template\n                \n           \n           \n           Sep 12:\n           Lecture 5 Sufficiency\n                \n                  Handwritten notes\n                \n           \n   \n   \n   Week 17\n   \n   \n   \n   \n           \n           Dec 18:\n           Exam  Final Exam, 8-11am\n                \n           \n   \n   \n\nNo matching items"
  },
  {
    "objectID": "units/unit2.html",
    "href": "units/unit2.html",
    "title": "Unit 2: Next",
    "section": "",
    "text": "This is an example of using an ipynb file as source rather than qmd. It follows instructions from https://github.com/DS-100/course-notes/README.md.\nLink to the data."
  },
  {
    "objectID": "units/unit2.html#latex",
    "href": "units/unit2.html#latex",
    "title": "Unit 2: Next",
    "section": "\\(\\LaTeX\\)",
    "text": "\\(\\LaTeX\\)\nHere is some \\(\\LaTeX\\). \\[\n\\theta = 7\n\\]"
  },
  {
    "objectID": "units/unit2.html#evaluated-python-code",
    "href": "units/unit2.html#evaluated-python-code",
    "title": "Unit 2: Next",
    "section": "Evaluated Python code",
    "text": "Evaluated Python code\nNote that to get code output shown, the underlying notebook must have executed the code.\n\n\nCode\na=7\nprint(a)\n\n\n7"
  },
  {
    "objectID": "units/unit2.html#callout",
    "href": "units/unit2.html#callout",
    "title": "Unit 2: Next",
    "section": "Callout",
    "text": "Callout\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title."
  },
  {
    "objectID": "units/unit1.html",
    "href": "units/unit1.html",
    "title": "Unit 1: Intro",
    "section": "",
    "text": "This is an example of using qmd as the source document."
  },
  {
    "objectID": "units/unit1.html#evaluated-python-code-chunk-with-a-plot",
    "href": "units/unit1.html#evaluated-python-code-chunk-with-a-plot",
    "title": "Unit 1: Intro",
    "section": "Evaluated Python code chunk, with a plot",
    "text": "Evaluated Python code chunk, with a plot\n\n\nCode\nimport numpy as np\nx = np.random.normal(size=100)\nimport matplotlib.pyplot as plt\nplt.hist(x)\nplt.show()\nnp.mean(x)\n\n\n\n\n\n-0.10729664182100629"
  },
  {
    "objectID": "units/unit1.html#latex",
    "href": "units/unit1.html#latex",
    "title": "Unit 1: Intro",
    "section": "\\(\\LaTeX\\)",
    "text": "\\(\\LaTeX\\)\n\\[\n\\theta = \\int_0^\\infty f(x,\\theta)d\\theta\n\\]"
  },
  {
    "objectID": "units/unit1.html#latex-macro",
    "href": "units/unit1.html#latex-macro",
    "title": "Unit 1: Intro",
    "section": "\\(\\LaTeX\\) macro",
    "text": "\\(\\LaTeX\\) macro\n\nWarning: need to look back at this as having include-before-body in the yaml causes extra space at top of page.\n\n\\[\nA = X \\trans Y\n\\]"
  },
  {
    "objectID": "units/unit1.html#styled-div-via-direct-html",
    "href": "units/unit1.html#styled-div-via-direct-html",
    "title": "Unit 1: Intro",
    "section": "Styled div via direct html",
    "text": "Styled div via direct html\n\nThis content can be styled via the border class."
  },
  {
    "objectID": "units/unit1.html#a-callout",
    "href": "units/unit1.html#a-callout",
    "title": "Unit 1: Intro",
    "section": "A callout",
    "text": "A callout\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title."
  },
  {
    "objectID": "units/unit1.html#tabset",
    "href": "units/unit1.html#tabset",
    "title": "Unit 1: Intro",
    "section": "Tabset",
    "text": "Tabset\n\nRPython\n\n\nThis code is not executed.\nfizz_buzz &lt;- function(fbnums = 1:50) {\n  output &lt;- dplyr::case_when(\n    fbnums %% 15 == 0 ~ \"FizzBuzz\",\n    fbnums %% 3 == 0 ~ \"Fizz\",\n    fbnums %% 5 == 0 ~ \"Buzz\",\n    TRUE ~ as.character(fbnums)\n  )\n  print(output)\n}\n\nfizz_buzz(3)\n\n\nThis code is executed.\n\n\nCode\ndef fizz_buzz(num):\n  if num % 15 == 0:\n    print(\"FizzBuzz\")\n  elif num % 5 == 0:\n    print(\"Buzz\")\n  elif num % 3 == 0:\n    print(\"Fizz\")\n  else:\n    print(num)\n    \nfizz_buzz(3)\n\n\nFizz"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "[UNDER CONSTRUCTION]\nStat 210A is an introductory Ph.D.-level course in theoretical statistics. It is a fast-paced and demanding course intended to prepare students for research careers in statistics."
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "Syllabus",
    "section": "Course information",
    "text": "Course information\n\nInstructors\n\nPrimary Instructor Prof. Will Fithian\n\nOffice Hours: Tuesday 3:30-4:30pm on Zoom, Thursday 9:30-10:30am in Evans 301\nEmail: wfithian@berkeley.edu\n\nGSI Dohyeong Ki\n\nOffice Hours: Wednesday 9-10am, Friday 4:30-5:30pm (location TBD)\nEmail: dohyeong_ki@berkeley.edu\n\n\n\n\nCourse schedule\n\nLectures: Tuesday and Thursday 11am-12:30pm, Evans 60\nRecitation sections: Every second F 3:30-4:30pm (location TBD), starting September 6\nElection week:\n\nBlanket 2-day extension on homework 9 (due Friday November 8, 11:59pm)\n\nThanksgiving week:\n\nTuesday November 26: lecture 11-12:30 on Zoom, 3:30-4:30 OH on Zoom\nNo lecture Thursday November 28\nNo in-person office hours\n\nFinal exam review: (a.k.a. last recitation section) Friday, December\nFinal exam: Wed December 18, 8-11am\n\n\n\nCourse communications\n\nLecture videos and homework solutions at [https://bcourses.berkeley.edu bCourses]\nEmail policy: You can email me or Dohyeong about administrative questions, with “[Stat 210A]” in the subject line. No math over email, please.\nEd page for announcements and technical discussion (no homework spoilers!)\nGradescope for turning in homework"
  },
  {
    "objectID": "syllabus.html#about-stat-210a",
    "href": "syllabus.html#about-stat-210a",
    "title": "Syllabus",
    "section": "About Stat 210A",
    "text": "About Stat 210A\n\nWhat is the theory of statistics?\nStatistics is the study of methods that use data to understand the world. Statistical methods are used throughout the natural and social sciences, in machine learning and artificial intelligence, and in engineering. Despite the ubiquitous use of statistics, its practitioners are perpetually accused of not actually understanding what they are doing. Statistics theory is, broadly speaking, the subject of what exactly we are doing when we apply statistical methods.\nWhile there are many possible ways to analyze data, most (but certainly not all) statistical methods are based on statistical modeling: treating the data as a realization of some random data-generating process with attributes, usually called parameters, that are a priori unknown. The goal of the analyst, then, is to use the data to draw accurate inferences about these parameters and/or to make accurate predictions about future data. If the modeling has been done well (a very big “if”) then these unknown parameters will correspond well to whatever real-world questions initially motivated the analysis. Applied statistics courses like Stat 215A and B delve deeply into questions about how to ensure that the statistical modeling exercise successfully captures something interesting about reality.\nIn this course we will instead focus on how the analyst can use the data most effectively within the context of a given mathematical setup. We will discuss the structure of statistical models, how to evaluate the quality of a statistical method, how to design good methods for new settings, and the philosophy of Bayesian vs frequentist modeling frameworks. We will cover estimation, confidence intervals, and hypothesis testing, in parametric and nonparametric methods, in finite samples and asymptotic regimes.\n\n\nTopics\nStatistical decision theory (frequentist and Bayesian), exponential families, point estimation, hypothesis testing, resampling methods, estimating equations and maximum likelihood, empirical Bayes, large-sample theory, high-dimensional testing, multiple testing and selective inference.\n\n\nPrerequisites\nThe course prerequisites are linear algebra, analysis, probability, and statistics.\n\n\nRelationship of Stat 210A to other Berkeley courses\nStat 210A focuses on classical statistical contexts: either inference in finite samples, or in fixed-dimensional asymptotic regimes. Stat 210B (for which 210A is a prerequisite) is more technical and covers topics like empirical process theory and high-dimensional statistics.\nBerkeley’s graduate course on Statistical Learning Theory (CS 281A / Stat 241A) is also very popular and has some overlap in its topics. Roughly speaking, it is more tilted toward “machine learning”: it spends more time on topics in predictive modeling (i.e. classification and regression, which are covered in Stat 215A), optimization, and signal processing, but spends less time on inferential questions and (I believe) does not cover topics like hypothesis testing, confidence intervals, and causal inference. Both courses cover estimation and exponential families."
  },
  {
    "objectID": "syllabus.html#references",
    "href": "syllabus.html#references",
    "title": "Syllabus",
    "section": "References",
    "text": "References\nThe online notes for this course are self-contained, however it can be helpful to see a different presentation in the following supplementary texts (all links are to public websites or Springer Link):\n\nKeener, Theoretical Statistics: Topics for a Core Course, Springer 2010.\nLehmann and Casella, Theory of Point Estimation, Springer 1998.\nLehmann and Romano, Testing Statistical Hypotheses, Springer 2005.\nCandes, Stats 300C Lecture notes, Stanford 2016.\n\nUndergrad-level review texts for prerequisites:\n\nAxler, Linear Algebra Done Right\nAbbott, Understanding Analysis\nAdhikari & Pitman, Probability for Data Science"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nYour final grade is based on:\n\nWeekly problem sets: 50%\nFinal exam: 50%\n\nLateness policy: Homework must be submitted to Gradescope at 11:59pm on Wednesday nights. Late problem sets will not be accepted, but we will drop your lowest two grades.\nCollaboration policy: For homework, you are welcome to work with each other or consult articles or textbooks online, with the following caveats:\n\nYou must write up your solution by yourself.\nYou may NOT consult any solutions from previous iterations of this course.\nNo generative AI allowed for problem sets.\nIf you collaborate or use any resources other than course texts, you must acknowledge your collaborators and the resources you used.\n\nAcademic integrity: You are expected to abide by the Berkeley honor code. Violating the collaboration policy, or cheating in any other way, will result in a failing grade for the semester and you will be reported to the University Office of Student Conduct.\nWhile the final exam is nominally one half of the grade, it is quite difficult and typically accounts for most of the variance in final course grades. If you take shortcuts on the homework, you will save yourself time in the short run, but you may do quite poorly on the final.\n\n\n\nHomework and final exam grades for a recent semester, with letter grade cutoffs"
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\nStudents with disabilities: Please see me as soon as possible if you need particular accommodations, and we will work out the necessary arrangements.\nScheduling conflicts: Please notify me in writing by the second week of the semester about any known or potential extracurricular conflicts (such as religious observances, graduate or medical school interviews, or team activities). I will try my best to help you with making accommodations, but cannot promise them in all cases. In the event there is no mutually-workable solution, you may be dropped from the class.\nExam accommodations: If you need accommodations on the final exam due to disability, or unavoidable travel or time conflict, please fill out the exam exam accommodation form by Friday, October 4 so that I can make arrangements. To ensure exam integrity I much prefer for all students to take the exam in Berkeley at the regularly scheduled time (8am December 18th), but will try to work with you if you have a conflict."
  }
]